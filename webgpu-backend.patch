diff --git a/setup.py b/setup.py
index 9a1cd901a..58f1fa0b2 100644
--- a/setup.py
+++ b/setup.py
@@ -650,7 +650,7 @@ def download_and_copy_dependencies():
     )
 
 
-backends = [*BackendInstaller.copy(["nvidia", "amd"]), *BackendInstaller.copy_externals()]
+backends = [*BackendInstaller.copy(["nvidia", "amd", "webgpu"]), *BackendInstaller.copy_externals()]
 
 
 def get_package_dirs():
diff --git a/third_party/webgpu/CMakeLists.txt b/third_party/webgpu/CMakeLists.txt
new file mode 100644
index 000000000..1fa42b28e
--- /dev/null
+++ b/third_party/webgpu/CMakeLists.txt
@@ -0,0 +1,16 @@
+include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)
+include_directories(${CMAKE_CURRENT_BINARY_DIR}/include)
+
+add_subdirectory(lib/TritonWebGPUToLLVM)
+
+if(TRITON_BUILD_PYTHON_MODULE)
+  add_triton_plugin(TritonWebGPU
+    ${CMAKE_CURRENT_SOURCE_DIR}/triton_webgpu.cc
+    LINK_LIBS TritonWebGPUToLLVM
+  )
+  target_link_libraries(TritonWebGPU PRIVATE Python3::Module pybind11::headers)
+  target_include_directories(TritonWebGPU PRIVATE
+    ${CMAKE_CURRENT_SOURCE_DIR}/include/webgpu-headers
+    ${CMAKE_CURRENT_SOURCE_DIR}/lib/TritonWebGPUToLLVM
+  )
+endif()
diff --git a/third_party/webgpu/backend/__init__.py b/third_party/webgpu/backend/__init__.py
new file mode 100644
index 000000000..e69de29bb
diff --git a/third_party/webgpu/backend/compiler.py b/third_party/webgpu/backend/compiler.py
new file mode 100644
index 000000000..60dd4def5
--- /dev/null
+++ b/third_party/webgpu/backend/compiler.py
@@ -0,0 +1,271 @@
+"""
+WebGPU Backend Compiler for Triton
+===================================
+
+Compilation pipeline: ttir -> ttgir -> llir -> spv
+
+Follows the Intel XPU approach of generating SPIR-V from LLVM IR.
+Dawn consumes the SPIR-V compute shaders directly.
+"""
+
+from triton.backends.compiler import BaseBackend, GPUTarget, Language
+from triton._C.libtriton import ir, passes, llvm, webgpu
+
+from dataclasses import dataclass
+import functools
+from typing import Any, Dict, Tuple, Optional
+from types import ModuleType
+import hashlib
+import re
+import os
+from pathlib import Path
+
+
+@dataclass(frozen=True)
+class WebGPUOptions:
+    num_warps: int = 4
+    num_ctas: int = 1
+    num_stages: int = 2
+    warp_size: int = 32
+    enable_fp_fusion: bool = True
+    supported_fp8_dtypes: Tuple[str] = ()
+    deprecated_fp8_dot_operand_dtypes: Tuple[str] = ()
+    default_dot_input_precision: str = "ieee"
+    allowed_dot_input_precisions: Tuple[str] = ("ieee",)
+    max_num_imprecise_acc_default: int = 0
+    extern_libs: dict = None
+    debug: bool = False
+    backend_name: str = 'webgpu'
+    sanitize_overflow: bool = True
+    arch: str = None
+
+    def __post_init__(self):
+        extern_libs = {} if self.extern_libs is None else dict(self.extern_libs)
+        object.__setattr__(self, 'extern_libs', tuple(extern_libs.items()))
+        assert self.num_warps > 0 and (self.num_warps & (self.num_warps - 1)) == 0, \
+               "num_warps must be a power of 2"
+
+    def hash(self):
+        hash_dict = dict(self.__dict__)
+        key = "_".join([f"{name}-{val}" for name, val in sorted(hash_dict.items())])
+        return hashlib.sha256(key.encode("utf-8")).hexdigest()
+
+
+class WebGPUBackend(BaseBackend):
+    """
+    WebGPU Backend for Triton.
+
+    Generates SPIR-V compute shaders that run on Dawn's WebGPU implementation.
+    Pipeline: Triton IR -> TritonGPU IR -> LLVM IR -> SPIR-V
+    """
+
+    @staticmethod
+    def supports_target(target: GPUTarget):
+        return target.backend == 'webgpu'
+
+    def __init__(self, target: GPUTarget) -> None:
+        super().__init__(target)
+        self.binary_ext = "wgsl"
+
+    def parse_options(self, opts) -> Any:
+        args = {'arch': f"webgpu{self.target.arch}"}
+        args.update({
+            k: opts[k]
+            for k in WebGPUOptions.__dataclass_fields__.keys()
+            if k in opts and opts[k] is not None
+        })
+        return WebGPUOptions(**args)
+
+    def pack_metadata(self, metadata):
+        return (
+            metadata.num_warps,
+            metadata.num_ctas,
+            metadata.shared,
+        )
+
+    def get_codegen_implementation(self, options):
+        codegen_fns = {
+            # FMA-based dot has no minimum shape constraints
+            "min_dot_size": lambda lhs_type, rhs_type: (1, 1, 1),
+        }
+        return codegen_fns
+
+    def get_module_map(self) -> Dict[str, ModuleType]:
+        return {}
+
+    def load_dialects(self, ctx):
+        webgpu.load_dialects(ctx)
+
+    @staticmethod
+    def make_ttir(mod, metadata, opt):
+        """Optimize Triton IR (same as NVIDIA/Intel pipeline)."""
+        pm = ir.pass_manager(mod.context)
+        pm.enable_debug()
+        passes.common.add_inliner(pm)
+        passes.ttir.add_rewrite_tensor_pointer(pm)
+        passes.ttir.add_rewrite_tensor_descriptor_to_pointer(pm)
+        passes.common.add_canonicalizer(pm)
+        passes.ttir.add_combine(pm)
+        passes.ttir.add_reorder_broadcast(pm)
+        passes.common.add_cse(pm)
+        passes.common.add_symbol_dce(pm)
+        passes.ttir.add_loop_unroll(pm)
+        pm.run(mod, 'make_ttir')
+        return mod
+
+    @staticmethod
+    def make_ttgir(mod, metadata, opt):
+        """Convert Triton IR to TritonGPU IR for WebGPU target."""
+        pm = ir.pass_manager(mod.context)
+        pm.enable_debug()
+
+        # Convert to TritonGPU IR targeting the WebGPU device
+        passes.ttir.add_convert_to_ttgpuir(
+            pm, "webgpu:0", opt.num_warps, opt.warp_size, opt.num_ctas
+        )
+
+        # Standard optimizations
+        passes.ttgpuir.add_coalesce(pm)
+        passes.ttgpuir.add_remove_layout_conversions(pm)
+        passes.ttgpuir.add_optimize_thread_locality(pm)
+        # Decompose mixed-precision dots (e.g. f16 inputs → f32 accumulator)
+        # and accelerate matmul patterns. The MMA acceleration patterns target
+        # NVIDIA-specific encodings (no-op for WebGPU), but the mixed-mode
+        # decomposition inserts arith.extf so convertFMADot sees uniform types.
+        passes.ttgpuir.add_accelerate_matmul(pm)
+        passes.ttgpuir.add_remove_layout_conversions(pm)
+        passes.ttgpuir.add_optimize_dot_operands(pm, False)
+        passes.ttgpuir.add_remove_layout_conversions(pm)
+        passes.ttgpuir.add_reduce_data_duplication(pm)
+        passes.ttgpuir.add_reorder_instructions(pm)
+        passes.common.add_cse(pm)
+        passes.common.add_symbol_dce(pm)
+        passes.common.add_canonicalizer(pm)
+
+        pm.run(mod, 'make_ttgir')
+        return mod
+
+    def make_llir(self, src, metadata, options):
+        """Convert TritonGPU IR to LLVM IR targeting SPIR-V."""
+        mod = src
+        pm = ir.pass_manager(mod.context)
+        pm.enable_debug()
+
+        passes.ttgpuir.add_combine_tensor_select_and_if(pm)
+
+        # Lower SCF (for/if/while) to CF (branch/cond_br) BEFORE the main
+        # conversion pass, matching NVIDIA's pipeline ordering.  The main
+        # conversion pass then handles CF ops together with TritonGPU ops
+        # using the same TritonGPU type-converter, so tensor-typed block
+        # args are consistently converted to LLVM struct types.
+        passes.convert.add_scf_to_cf(pm)
+
+        # Allocate shared memory
+        passes.ttgpuir.add_allocate_shared_memory(pm)
+
+        # WebGPU-specific TritonGPU → LLVM conversion pass
+        # This lowers all TritonGPU ops to LLVM dialect using
+        # generic conversion patterns + WebGPU-specific patterns.
+        # CF→LLVM lowering is handled at the end of this pass.
+        webgpu.passes.ttgpuir.add_to_llvmir(pm)
+
+        passes.convert.add_arith_to_llvmir(pm)
+        passes.convert.add_index_to_llvmir(pm)
+
+        passes.ttgpuir.add_canonicalize_llvm_ir(pm)
+
+        passes.common.add_canonicalizer(pm)
+        passes.common.add_cse(pm)
+        passes.common.add_symbol_dce(pm)
+
+        pm.run(mod, 'make_llir')
+
+        # LLVM-IR (MLIR) -> LLVM-IR (LLVM)
+        llvm.init_targets()
+        context = llvm.context()
+        llvm_mod = llvm.to_module(mod, context)
+
+        # Set SPIR-V target triple and data layout directly
+        # (we can't use llvm.attach_datalayout because the bundled LLVM
+        # doesn't include the SPIR-V target backend)
+        webgpu.set_spv_target_triple(llvm_mod)
+
+        # Link external libs if any
+        if options.extern_libs:
+            paths = [path for (name, path) in options.extern_libs]
+            llvm.link_extern_libs(llvm_mod, paths)
+
+        # Skip target-specific optimizations (no SPIR-V target machine available)
+        # llvm.optimize_module(llvm_mod, llvm.OPTIMIZE_O3)
+
+        # Extract metadata
+        metadata["shared"] = src.get_int_attr("ttg.shared") or 0
+        metadata["global_scratch_size"] = src.get_int_attr("ttg.global_scratch_memory_size") or 0
+        metadata["global_scratch_align"] = src.get_int_attr("ttg.global_scratch_memory_alignment") or 1
+
+        ret = str(llvm_mod)
+        del llvm_mod
+        del context
+        # Save LLVM IR for WGSL translation stage
+        metadata["llir_str"] = ret
+
+        # Extract kernel name from LLVM IR (first 'define' function)
+        import re as _re
+        m = _re.search(r'define\s+\S+\s+@(\w+)\s*\(', ret)
+        metadata["name"] = m.group(1) if m else "kernel"
+
+        return ret
+
+    @staticmethod
+    def make_spv(src, metadata, options):
+        """Translate LLVM IR to SPIR-V binary."""
+        spirv, name = webgpu.translate_to_spirv(src)
+        metadata["name"] = name
+        return spirv
+
+    @staticmethod
+    def make_wgsl(src, metadata, options):
+        """Translate LLVM IR to WGSL compute shader for GPU execution via wgpu.
+
+        This stage runs after SPV.  The LLVM IR was saved in metadata by
+        make_llir, so we read it from there (src is the SPV binary, which
+        we ignore).
+
+        A ``signature`` entry must be present in *metadata* (added
+        by WebGPUBackend.make_ttir or by caller) for full type
+        information; otherwise types are inferred from the LLVM IR.
+        """
+        from .llvm_to_wgsl import translate_llvm_to_wgsl
+
+        # Retrieve the LLVM IR saved by make_llir
+        llir = metadata.get('llir_str', '')
+        if not llir:
+            raise ValueError("No LLVM IR found in metadata for WGSL translation")
+
+        sig = metadata.get('signature', {})
+        num_warps = options.num_warps if hasattr(options, 'num_warps') else 4
+        warp_size = options.warp_size if hasattr(options, 'warp_size') else 32
+
+        try:
+            result = translate_llvm_to_wgsl(llir, sig, num_warps, warp_size)
+            metadata['wgsl_bindings'] = result.buffer_bindings
+            metadata['wgsl_params'] = result.param_fields
+            metadata['wgsl_workgroup_size'] = result.workgroup_size
+            return result.wgsl
+        except Exception as e:
+            # WGSL translation is best-effort; don't break compilation
+            metadata['wgsl_error'] = str(e)
+            return f"// WGSL translation failed: {e}\n"
+
+    def add_stages(self, stages, options, language):
+        if language == Language.TRITON:
+            stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options)
+            stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options)
+        stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options)
+        # Skip SPIR-V — make_wgsl reads LLVM IR directly from metadata,
+        # and Dawn compiles WGSL to the backend's native format (DXIL/SPIR-V).
+        stages["wgsl"] = lambda src, metadata: self.make_wgsl(src, metadata, options)
+
+    @functools.lru_cache()
+    def hash(self):
+        return f'SPIRV-webgpu-{self.target.arch}'
diff --git a/third_party/webgpu/backend/dawn_runner.py b/third_party/webgpu/backend/dawn_runner.py
new file mode 100644
index 000000000..1d64b97b6
--- /dev/null
+++ b/third_party/webgpu/backend/dawn_runner.py
@@ -0,0 +1,2328 @@
+"""
+Dawn WebGPU Runtime Kernel Runner
+===================================
+
+Executes WGSL compute shaders on the GPU via Dawn's native WebGPU C API.
+
+Dawn natively supports D3D12 (Windows), Vulkan (Linux), and Metal (macOS),
+consuming WGSL shaders directly through its Tint compiler.
+
+Usage:
+    runner = DawnRunner()
+    result = runner.run_kernel(
+        wgsl_code=wgsl,
+        buffer_bindings=[...],
+        param_fields=[...],
+        workgroup_size=128,
+        grid=(num_workgroups,),
+        buffers={'x_ptr': x_np, 'y_ptr': y_np, 'output_ptr': output_np},
+        scalars={'n_elements': n},
+    )
+    output = result['output_ptr']  # numpy array
+"""
+
+import ctypes
+import ctypes.util
+import struct
+import os
+import sys
+import numpy as np
+
+from .llvm_to_wgsl import BufferBinding, ParamField
+
+# ============================================================================
+# Dawn library discovery
+# ============================================================================
+
+_dawn_lib = None
+_dawn_loaded = False
+
+def _find_dawn_dll():
+    """Locate the Dawn webgpu_dawn shared library."""
+    # 1. Check DAWN_PATH environment variable
+    dawn_path = os.environ.get("DAWN_PATH")
+    if dawn_path and os.path.exists(dawn_path):
+        return dawn_path
+
+    # 2. Check relative to this file (build output locations)
+    here = os.path.dirname(os.path.abspath(__file__))
+    candidates = []
+
+    # In-tree build location
+    triton_root = os.path.normpath(os.path.join(here, "..", "..", "..", ".."))
+    candidates.extend([
+        os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "webgpu_dawn.dll"),
+        os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "libwebgpu_dawn.so"),
+        os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "libwebgpu_dawn.dylib"),
+    ])
+
+    # Backend lib directory
+    candidates.extend([
+        os.path.join(here, "lib", "webgpu_dawn.dll"),
+        os.path.join(here, "lib", "libwebgpu_dawn.so"),
+        os.path.join(here, "lib", "libwebgpu_dawn.dylib"),
+    ])
+
+    for path in candidates:
+        if os.path.exists(path):
+            return path
+
+    # 3. System search
+    lib = ctypes.util.find_library("webgpu_dawn")
+    if lib:
+        return lib
+
+    return None
+
+
+def _load_dawn():
+    """Load the Dawn library and set up function prototypes."""
+    global _dawn_lib, _dawn_loaded
+    if _dawn_loaded:
+        return _dawn_lib
+
+    path = _find_dawn_dll()
+    if path is None:
+        _dawn_loaded = True
+        return None
+
+    try:
+        # Dawn uses LoadLibraryExA with LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR which
+        # requires absolute paths. Pre-load system DLLs that Dawn will need
+        # (d3dcompiler_47.dll, vulkan-1.dll) so Dawn finds them already loaded.
+        if sys.platform == 'win32':
+            _preload_system_dlls()
+
+        _dawn_lib = ctypes.CDLL(path)
+    except OSError:
+        _dawn_loaded = True
+        return None
+
+    _setup_prototypes(_dawn_lib)
+    _dawn_loaded = True
+    return _dawn_lib
+
+
+def _preload_system_dlls():
+    """Pre-load system DLLs that Dawn depends on (Windows only).
+
+    Dawn's DynamicLib::Open uses LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR which
+    fails with ERROR_INVALID_PARAMETER for relative filenames. Pre-loading
+    the DLLs with standard LoadLibrary makes them available via
+    GetModuleHandle when Dawn calls OpenLoaded.
+    """
+    kernel32 = ctypes.windll.kernel32
+
+    # d3dcompiler_47.dll — needed for D3D11/D3D12 shader compilation
+    try:
+        kernel32.LoadLibraryW("d3dcompiler_47.dll")
+    except Exception:
+        pass
+
+    # dxgi.dll — needed for D3D adapter enumeration
+    try:
+        kernel32.LoadLibraryW("dxgi.dll")
+    except Exception:
+        pass
+
+    # vulkan-1.dll — needed for Vulkan backend
+    vulkan_paths = []
+    vulkan_sdk = os.environ.get("VULKAN_SDK")
+    if vulkan_sdk:
+        vulkan_paths.append(os.path.join(vulkan_sdk, "Bin", "vulkan-1.dll"))
+    vulkan_paths.append("vulkan-1.dll")  # system default
+
+    for vp in vulkan_paths:
+        try:
+            kernel32.LoadLibraryW(vp)
+            break
+        except Exception:
+            pass
+
+
+def HAS_DAWN():
+    """Check if Dawn is available."""
+    return _load_dawn() is not None
+
+
+# ============================================================================
+# ctypes struct definitions matching Dawn's webgpu.h
+# ============================================================================
+
+# Opaque handle types — all are pointers to opaque structs
+WGPUInstance = ctypes.c_void_p
+WGPUAdapter = ctypes.c_void_p
+WGPUDevice = ctypes.c_void_p
+WGPUQueue = ctypes.c_void_p
+WGPUShaderModule = ctypes.c_void_p
+WGPUComputePipeline = ctypes.c_void_p
+WGPUBuffer = ctypes.c_void_p
+WGPUBindGroup = ctypes.c_void_p
+WGPUBindGroupLayout = ctypes.c_void_p
+WGPUPipelineLayout = ctypes.c_void_p
+WGPUCommandEncoder = ctypes.c_void_p
+WGPUCommandBuffer = ctypes.c_void_p
+WGPUComputePassEncoder = ctypes.c_void_p
+WGPUSurface = ctypes.c_void_p
+WGPUSampler = ctypes.c_void_p
+WGPUTextureView = ctypes.c_void_p
+
+# Basic types
+WGPUBool = ctypes.c_uint32
+WGPUFlags = ctypes.c_uint64
+WGPUBufferUsageFlags = WGPUFlags
+WGPUMapModeFlags = WGPUFlags
+WGPUShaderStageFlags = WGPUFlags
+
+# size_t — platform dependent
+SIZE_T = ctypes.c_size_t
+WGPU_STRLEN = SIZE_T(-1).value  # SIZE_MAX sentinel for null-terminated strings
+
+
+# ── Enums ──
+
+class WGPUSType:
+    ShaderSourceSPIRV = 0x00000001
+    ShaderSourceWGSL = 0x00000002
+    DawnTogglesDescriptor = 0x0005000A
+
+
+class WGPUCallbackMode:
+    WaitAnyOnly = 0x00000001
+    AllowProcessEvents = 0x00000002
+    AllowSpontaneous = 0x00000003
+
+
+class WGPUWaitStatus:
+    Success = 0x00000001
+    TimedOut = 0x00000002
+    Error = 0x00000003
+
+
+class WGPUMapAsyncStatus:
+    Success = 0x00000001
+    CallbackCancelled = 0x00000002
+    Error = 0x00000003
+    Aborted = 0x00000004
+
+
+class WGPURequestAdapterStatus:
+    Success = 0x00000001
+
+
+class WGPURequestDeviceStatus:
+    Success = 0x00000001
+
+
+class WGPUBufferBindingType:
+    BindingNotUsed = 0x00000000
+    Undefined = 0x00000001
+    Uniform = 0x00000002
+    Storage = 0x00000003
+    ReadOnlyStorage = 0x00000004
+
+
+class WGPUBackendType:
+    Undefined = 0x00000000
+    Null = 0x00000001
+    WebGPU = 0x00000002
+    D3D11 = 0x00000003
+    D3D12 = 0x00000004
+    Metal = 0x00000005
+    Vulkan = 0x00000006
+
+
+class WGPUFeatureLevel:
+    Undefined = 0x00000000
+    Compatibility = 0x00000001
+    Core = 0x00000002
+
+
+class WGPUPowerPreference:
+    Undefined = 0x00000000
+    LowPower = 0x00000001
+    HighPerformance = 0x00000002
+
+
+# Buffer usage flags
+BUFFER_USAGE_MAP_READ = 0x0001
+BUFFER_USAGE_MAP_WRITE = 0x0002
+BUFFER_USAGE_COPY_SRC = 0x0004
+BUFFER_USAGE_COPY_DST = 0x0008
+BUFFER_USAGE_STORAGE = 0x0080
+
+# Map mode flags
+MAP_MODE_READ = 0x0001
+
+# Shader stage flags
+SHADER_STAGE_COMPUTE = 0x0004
+
+
+# ── Structs ──
+
+class WGPUStringView(ctypes.Structure):
+    _fields_ = [
+        ("data", ctypes.c_char_p),
+        ("length", SIZE_T),
+    ]
+
+    @staticmethod
+    def from_str(s):
+        """Create a WGPUStringView from a Python string."""
+        if s is None:
+            return WGPUStringView(None, 0)
+        encoded = s.encode("utf-8")
+        return WGPUStringView(encoded, len(encoded))
+
+    @staticmethod
+    def null_terminated(s):
+        """Create a WGPUStringView with WGPU_STRLEN sentinel."""
+        encoded = s.encode("utf-8")
+        return WGPUStringView(encoded, WGPU_STRLEN)
+
+
+class WGPUChainedStruct(ctypes.Structure):
+    pass
+
+WGPUChainedStruct._fields_ = [
+    ("next", ctypes.POINTER(WGPUChainedStruct)),
+    ("sType", ctypes.c_uint32),
+]
+
+
+class WGPUDawnTogglesDescriptor(ctypes.Structure):
+    """Dawn-specific toggle descriptor, chainable to adapter/device descriptors."""
+    _fields_ = [
+        ("chain", WGPUChainedStruct),
+        ("enabledToggleCount", SIZE_T),
+        ("enabledToggles", ctypes.POINTER(ctypes.c_char_p)),
+        ("disabledToggleCount", SIZE_T),
+        ("disabledToggles", ctypes.POINTER(ctypes.c_char_p)),
+    ]
+
+
+class WGPUFuture(ctypes.Structure):
+    _fields_ = [("id", ctypes.c_uint64)]
+
+
+class WGPUFutureWaitInfo(ctypes.Structure):
+    _fields_ = [
+        ("future", WGPUFuture),
+        ("completed", WGPUBool),
+    ]
+
+
+# ── Callback types ──
+# void (*)(WGPURequestAdapterStatus status, WGPUAdapter adapter, WGPUStringView message, void* ud1, void* ud2)
+RequestAdapterCallback = ctypes.CFUNCTYPE(
+    None, ctypes.c_uint32, WGPUAdapter, WGPUStringView, ctypes.c_void_p, ctypes.c_void_p
+)
+
+# void (*)(WGPURequestDeviceStatus status, WGPUDevice device, WGPUStringView message, void* ud1, void* ud2)
+RequestDeviceCallback = ctypes.CFUNCTYPE(
+    None, ctypes.c_uint32, WGPUDevice, WGPUStringView, ctypes.c_void_p, ctypes.c_void_p
+)
+
+# void (*)(WGPUMapAsyncStatus status, WGPUStringView message, void* ud1, void* ud2)
+BufferMapCallback = ctypes.CFUNCTYPE(
+    None, ctypes.c_uint32, WGPUStringView, ctypes.c_void_p, ctypes.c_void_p
+)
+
+# void (*)(const WGPUDevice*, WGPUErrorType type, WGPUStringView message, void* ud1, void* ud2)
+UncapturedErrorCallback = ctypes.CFUNCTYPE(
+    None, ctypes.c_void_p, ctypes.c_uint32, WGPUStringView, ctypes.c_void_p, ctypes.c_void_p
+)
+
+
+# ── Callback info structs ──
+
+class WGPURequestAdapterCallbackInfo(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("mode", ctypes.c_uint32),
+        ("callback", RequestAdapterCallback),
+        ("userdata1", ctypes.c_void_p),
+        ("userdata2", ctypes.c_void_p),
+    ]
+
+
+class WGPURequestDeviceCallbackInfo(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("mode", ctypes.c_uint32),
+        ("callback", RequestDeviceCallback),
+        ("userdata1", ctypes.c_void_p),
+        ("userdata2", ctypes.c_void_p),
+    ]
+
+
+class WGPUBufferMapCallbackInfo(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("mode", ctypes.c_uint32),
+        ("callback", BufferMapCallback),
+        ("userdata1", ctypes.c_void_p),
+        ("userdata2", ctypes.c_void_p),
+    ]
+
+
+# ── Descriptor structs ──
+
+class WGPUInstanceDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("requiredFeatureCount", SIZE_T),
+        ("requiredFeatures", ctypes.c_void_p),
+        ("requiredLimits", ctypes.c_void_p),
+    ]
+
+
+class WGPURequestAdapterOptions(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("featureLevel", ctypes.c_uint32),
+        ("powerPreference", ctypes.c_uint32),
+        ("forceFallbackAdapter", WGPUBool),
+        ("backendType", ctypes.c_uint32),
+        ("compatibleSurface", WGPUSurface),
+    ]
+
+
+class WGPUAdapterInfo(ctypes.Structure):
+    """Adapter information: vendor, architecture, device, description, backend."""
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("vendor", WGPUStringView),
+        ("architecture", WGPUStringView),
+        ("device", WGPUStringView),
+        ("description", WGPUStringView),
+        ("backendType", ctypes.c_uint32),
+        ("adapterType", ctypes.c_uint32),
+        ("vendorID", ctypes.c_uint32),
+        ("deviceID", ctypes.c_uint32),
+    ]
+
+
+class WGPUQueueDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+    ]
+
+
+class WGPUDeviceLostCallbackInfo(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("mode", ctypes.c_uint32),
+        ("callback", ctypes.c_void_p),
+        ("userdata1", ctypes.c_void_p),
+        ("userdata2", ctypes.c_void_p),
+    ]
+
+
+class WGPUUncapturedErrorCallbackInfo(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("callback", ctypes.c_void_p),
+        ("userdata1", ctypes.c_void_p),
+        ("userdata2", ctypes.c_void_p),
+    ]
+
+
+class WGPULimits(ctypes.Structure):
+    """WebGPU device/adapter limits (matches dawn/webgpu.h WGPULimits)."""
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("maxTextureDimension1D", ctypes.c_uint32),
+        ("maxTextureDimension2D", ctypes.c_uint32),
+        ("maxTextureDimension3D", ctypes.c_uint32),
+        ("maxTextureArrayLayers", ctypes.c_uint32),
+        ("maxBindGroups", ctypes.c_uint32),
+        ("maxBindGroupsPlusVertexBuffers", ctypes.c_uint32),
+        ("maxBindingsPerBindGroup", ctypes.c_uint32),
+        ("maxDynamicUniformBuffersPerPipelineLayout", ctypes.c_uint32),
+        ("maxDynamicStorageBuffersPerPipelineLayout", ctypes.c_uint32),
+        ("maxSampledTexturesPerShaderStage", ctypes.c_uint32),
+        ("maxSamplersPerShaderStage", ctypes.c_uint32),
+        ("maxStorageBuffersPerShaderStage", ctypes.c_uint32),
+        ("maxStorageTexturesPerShaderStage", ctypes.c_uint32),
+        ("maxUniformBuffersPerShaderStage", ctypes.c_uint32),
+        ("maxUniformBufferBindingSize", ctypes.c_uint64),
+        ("maxStorageBufferBindingSize", ctypes.c_uint64),
+        ("minUniformBufferOffsetAlignment", ctypes.c_uint32),
+        ("minStorageBufferOffsetAlignment", ctypes.c_uint32),
+        ("maxVertexBuffers", ctypes.c_uint32),
+        ("maxBufferSize", ctypes.c_uint64),
+        ("maxVertexAttributes", ctypes.c_uint32),
+        ("maxVertexBufferArrayStride", ctypes.c_uint32),
+        ("maxInterStageShaderVariables", ctypes.c_uint32),
+        ("maxColorAttachments", ctypes.c_uint32),
+        ("maxColorAttachmentBytesPerSample", ctypes.c_uint32),
+        ("maxComputeWorkgroupStorageSize", ctypes.c_uint32),
+        ("maxComputeInvocationsPerWorkgroup", ctypes.c_uint32),
+        ("maxComputeWorkgroupSizeX", ctypes.c_uint32),
+        ("maxComputeWorkgroupSizeY", ctypes.c_uint32),
+        ("maxComputeWorkgroupSizeZ", ctypes.c_uint32),
+        ("maxComputeWorkgroupsPerDimension", ctypes.c_uint32),
+        ("maxImmediateSize", ctypes.c_uint32),
+    ]
+
+
+class WGPUDeviceDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("requiredFeatureCount", SIZE_T),
+        ("requiredFeatures", ctypes.c_void_p),
+        ("requiredLimits", ctypes.POINTER(WGPULimits)),
+        ("defaultQueue", WGPUQueueDescriptor),
+        ("deviceLostCallbackInfo", WGPUDeviceLostCallbackInfo),
+        ("uncapturedErrorCallbackInfo", WGPUUncapturedErrorCallbackInfo),
+    ]
+
+
+class WGPUShaderModuleDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+    ]
+
+
+class WGPUShaderSourceWGSL(ctypes.Structure):
+    _fields_ = [
+        ("chain", WGPUChainedStruct),  # embedded by value, not pointer
+        ("code", WGPUStringView),
+    ]
+
+
+class WGPUComputeState(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("module", WGPUShaderModule),
+        ("entryPoint", WGPUStringView),
+        ("constantCount", SIZE_T),
+        ("constants", ctypes.c_void_p),
+    ]
+
+
+class WGPUComputePipelineDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("layout", WGPUPipelineLayout),
+        ("compute", WGPUComputeState),
+    ]
+
+
+class WGPUBufferDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("usage", WGPUBufferUsageFlags),
+        ("size", ctypes.c_uint64),
+        ("mappedAtCreation", WGPUBool),
+    ]
+
+
+class WGPUBindGroupEntry(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("binding", ctypes.c_uint32),
+        ("buffer", WGPUBuffer),
+        ("offset", ctypes.c_uint64),
+        ("size", ctypes.c_uint64),
+        ("sampler", WGPUSampler),
+        ("textureView", WGPUTextureView),
+    ]
+
+
+class WGPUBindGroupDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("layout", WGPUBindGroupLayout),
+        ("entryCount", SIZE_T),
+        ("entries", ctypes.POINTER(WGPUBindGroupEntry)),
+    ]
+
+
+class WGPUBufferBindingLayout(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("type", ctypes.c_uint32),
+        ("hasDynamicOffset", WGPUBool),
+        ("minBindingSize", ctypes.c_uint64),
+    ]
+
+
+class WGPUSamplerBindingLayout(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("type", ctypes.c_uint32),
+    ]
+
+
+class WGPUTextureBindingLayout(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("sampleType", ctypes.c_uint32),
+        ("viewDimension", ctypes.c_uint32),
+        ("multisampled", WGPUBool),
+    ]
+
+
+class WGPUStorageTextureBindingLayout(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("access", ctypes.c_uint32),
+        ("format", ctypes.c_uint32),
+        ("viewDimension", ctypes.c_uint32),
+    ]
+
+
+class WGPUBindGroupLayoutEntry(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("binding", ctypes.c_uint32),
+        ("visibility", WGPUShaderStageFlags),
+        ("bindingArraySize", ctypes.c_uint32),
+        ("buffer", WGPUBufferBindingLayout),
+        ("sampler", WGPUSamplerBindingLayout),
+        ("texture", WGPUTextureBindingLayout),
+        ("storageTexture", WGPUStorageTextureBindingLayout),
+    ]
+
+
+class WGPUBindGroupLayoutDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("entryCount", SIZE_T),
+        ("entries", ctypes.POINTER(WGPUBindGroupLayoutEntry)),
+    ]
+
+
+class WGPUPipelineLayoutDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("bindGroupLayoutCount", SIZE_T),
+        ("bindGroupLayouts", ctypes.POINTER(WGPUBindGroupLayout)),
+        ("immediateSize", ctypes.c_uint32),
+    ]
+
+
+class WGPUCommandEncoderDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+    ]
+
+
+class WGPUCommandBufferDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+    ]
+
+
+class WGPUComputePassDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.POINTER(WGPUChainedStruct)),
+        ("label", WGPUStringView),
+        ("timestampWrites", ctypes.c_void_p),
+    ]
+
+
+class WGPUPassTimestampWrites(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.c_void_p),
+        ("querySet", ctypes.c_void_p),  # WGPUQuerySet
+        ("beginningOfPassWriteIndex", ctypes.c_uint32),
+        ("endOfPassWriteIndex", ctypes.c_uint32),
+    ]
+
+
+class WGPUQuerySetDescriptor(ctypes.Structure):
+    _fields_ = [
+        ("nextInChain", ctypes.c_void_p),
+        ("label", WGPUStringView),
+        ("type", ctypes.c_uint32),  # WGPUQueryType
+        ("count", ctypes.c_uint32),
+    ]
+
+
+# ============================================================================
+# Function prototype setup
+# ============================================================================
+
+def _setup_prototypes(lib):
+    """Set argument and return types for all used Dawn functions."""
+
+    # wgpuCreateInstance
+    lib.wgpuCreateInstance.argtypes = [ctypes.POINTER(WGPUInstanceDescriptor)]
+    lib.wgpuCreateInstance.restype = WGPUInstance
+
+    # wgpuInstanceRequestAdapter
+    lib.wgpuInstanceRequestAdapter.argtypes = [
+        WGPUInstance,
+        ctypes.POINTER(WGPURequestAdapterOptions),
+        WGPURequestAdapterCallbackInfo,
+    ]
+    lib.wgpuInstanceRequestAdapter.restype = WGPUFuture
+
+    # wgpuInstanceWaitAny
+    lib.wgpuInstanceWaitAny.argtypes = [
+        WGPUInstance, SIZE_T,
+        ctypes.POINTER(WGPUFutureWaitInfo),
+        ctypes.c_uint64,
+    ]
+    lib.wgpuInstanceWaitAny.restype = ctypes.c_uint32  # WGPUWaitStatus
+
+    # wgpuInstanceProcessEvents
+    lib.wgpuInstanceProcessEvents.argtypes = [WGPUInstance]
+    lib.wgpuInstanceProcessEvents.restype = None
+
+    # wgpuAdapterCreateDevice (Dawn synchronous extension)
+    lib.wgpuAdapterCreateDevice.argtypes = [
+        WGPUAdapter, ctypes.POINTER(WGPUDeviceDescriptor)
+    ]
+    lib.wgpuAdapterCreateDevice.restype = WGPUDevice
+
+    # wgpuAdapterGetLimits
+    lib.wgpuAdapterGetLimits.argtypes = [
+        WGPUAdapter, ctypes.POINTER(WGPULimits)
+    ]
+    lib.wgpuAdapterGetLimits.restype = ctypes.c_uint32  # WGPUStatus
+
+    # wgpuAdapterRelease / wgpuAdapterAddRef
+    lib.wgpuAdapterRelease.argtypes = [WGPUAdapter]
+    lib.wgpuAdapterRelease.restype = None
+
+    # wgpuAdapterHasFeature
+    lib.wgpuAdapterHasFeature.argtypes = [WGPUAdapter, ctypes.c_uint32]
+    lib.wgpuAdapterHasFeature.restype = ctypes.c_uint32  # WGPUBool
+
+    # wgpuAdapterGetInfo
+    lib.wgpuAdapterGetInfo.argtypes = [WGPUAdapter, ctypes.POINTER(WGPUAdapterInfo)]
+    lib.wgpuAdapterGetInfo.restype = ctypes.c_uint32  # WGPUStatus
+
+    # wgpuDeviceGetQueue
+    lib.wgpuDeviceGetQueue.argtypes = [WGPUDevice]
+    lib.wgpuDeviceGetQueue.restype = WGPUQueue
+
+    # wgpuDeviceCreateShaderModule
+    lib.wgpuDeviceCreateShaderModule.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUShaderModuleDescriptor)
+    ]
+    lib.wgpuDeviceCreateShaderModule.restype = WGPUShaderModule
+
+    # wgpuDeviceCreateComputePipeline
+    lib.wgpuDeviceCreateComputePipeline.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUComputePipelineDescriptor)
+    ]
+    lib.wgpuDeviceCreateComputePipeline.restype = WGPUComputePipeline
+
+    # wgpuDeviceCreateBuffer
+    lib.wgpuDeviceCreateBuffer.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUBufferDescriptor)
+    ]
+    lib.wgpuDeviceCreateBuffer.restype = WGPUBuffer
+
+    # wgpuDeviceCreateBindGroupLayout
+    lib.wgpuDeviceCreateBindGroupLayout.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUBindGroupLayoutDescriptor)
+    ]
+    lib.wgpuDeviceCreateBindGroupLayout.restype = WGPUBindGroupLayout
+
+    # wgpuDeviceCreatePipelineLayout
+    lib.wgpuDeviceCreatePipelineLayout.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUPipelineLayoutDescriptor)
+    ]
+    lib.wgpuDeviceCreatePipelineLayout.restype = WGPUPipelineLayout
+
+    # wgpuDeviceCreateBindGroup
+    lib.wgpuDeviceCreateBindGroup.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUBindGroupDescriptor)
+    ]
+    lib.wgpuDeviceCreateBindGroup.restype = WGPUBindGroup
+
+    # wgpuDeviceCreateCommandEncoder
+    lib.wgpuDeviceCreateCommandEncoder.argtypes = [
+        WGPUDevice, ctypes.POINTER(WGPUCommandEncoderDescriptor)
+    ]
+    lib.wgpuDeviceCreateCommandEncoder.restype = WGPUCommandEncoder
+
+    # wgpuCommandEncoderBeginComputePass
+    lib.wgpuCommandEncoderBeginComputePass.argtypes = [
+        WGPUCommandEncoder, ctypes.POINTER(WGPUComputePassDescriptor)
+    ]
+    lib.wgpuCommandEncoderBeginComputePass.restype = WGPUComputePassEncoder
+
+    # wgpuComputePassEncoderSetPipeline
+    lib.wgpuComputePassEncoderSetPipeline.argtypes = [
+        WGPUComputePassEncoder, WGPUComputePipeline
+    ]
+    lib.wgpuComputePassEncoderSetPipeline.restype = None
+
+    # wgpuComputePassEncoderSetBindGroup
+    lib.wgpuComputePassEncoderSetBindGroup.argtypes = [
+        WGPUComputePassEncoder, ctypes.c_uint32,
+        WGPUBindGroup, SIZE_T, ctypes.POINTER(ctypes.c_uint32)
+    ]
+    lib.wgpuComputePassEncoderSetBindGroup.restype = None
+
+    # wgpuComputePassEncoderDispatchWorkgroups
+    lib.wgpuComputePassEncoderDispatchWorkgroups.argtypes = [
+        WGPUComputePassEncoder, ctypes.c_uint32, ctypes.c_uint32, ctypes.c_uint32
+    ]
+    lib.wgpuComputePassEncoderDispatchWorkgroups.restype = None
+
+    # wgpuComputePassEncoderEnd
+    lib.wgpuComputePassEncoderEnd.argtypes = [WGPUComputePassEncoder]
+    lib.wgpuComputePassEncoderEnd.restype = None
+
+    # wgpuCommandEncoderCopyBufferToBuffer
+    lib.wgpuCommandEncoderCopyBufferToBuffer.argtypes = [
+        WGPUCommandEncoder, WGPUBuffer, ctypes.c_uint64,
+        WGPUBuffer, ctypes.c_uint64, ctypes.c_uint64
+    ]
+    lib.wgpuCommandEncoderCopyBufferToBuffer.restype = None
+
+    # wgpuCommandEncoderFinish
+    lib.wgpuCommandEncoderFinish.argtypes = [
+        WGPUCommandEncoder, ctypes.POINTER(WGPUCommandBufferDescriptor)
+    ]
+    lib.wgpuCommandEncoderFinish.restype = WGPUCommandBuffer
+
+    # wgpuQueueSubmit
+    lib.wgpuQueueSubmit.argtypes = [
+        WGPUQueue, SIZE_T, ctypes.POINTER(WGPUCommandBuffer)
+    ]
+    lib.wgpuQueueSubmit.restype = None
+
+    # wgpuQueueWriteBuffer
+    lib.wgpuQueueWriteBuffer.argtypes = [
+        WGPUQueue, WGPUBuffer, ctypes.c_uint64,
+        ctypes.c_void_p, SIZE_T
+    ]
+    lib.wgpuQueueWriteBuffer.restype = None
+
+    # wgpuBufferMapAsync
+    lib.wgpuBufferMapAsync.argtypes = [
+        WGPUBuffer, WGPUMapModeFlags, SIZE_T, SIZE_T,
+        WGPUBufferMapCallbackInfo,
+    ]
+    lib.wgpuBufferMapAsync.restype = WGPUFuture
+
+    # wgpuBufferGetConstMappedRange
+    lib.wgpuBufferGetConstMappedRange.argtypes = [
+        WGPUBuffer, SIZE_T, SIZE_T
+    ]
+    lib.wgpuBufferGetConstMappedRange.restype = ctypes.c_void_p
+
+    # wgpuBufferUnmap
+    lib.wgpuBufferUnmap.argtypes = [WGPUBuffer]
+    lib.wgpuBufferUnmap.restype = None
+
+    # wgpuBufferDestroy
+    lib.wgpuBufferDestroy.argtypes = [WGPUBuffer]
+    lib.wgpuBufferDestroy.restype = None
+
+    # wgpuBufferRelease
+    lib.wgpuBufferRelease.argtypes = [WGPUBuffer]
+    lib.wgpuBufferRelease.restype = None
+
+    # wgpuShaderModuleRelease
+    lib.wgpuShaderModuleRelease.argtypes = [WGPUShaderModule]
+    lib.wgpuShaderModuleRelease.restype = None
+
+    # wgpuComputePipelineRelease
+    lib.wgpuComputePipelineRelease.argtypes = [WGPUComputePipeline]
+    lib.wgpuComputePipelineRelease.restype = None
+
+    # wgpuBindGroupRelease
+    lib.wgpuBindGroupRelease.argtypes = [WGPUBindGroup]
+    lib.wgpuBindGroupRelease.restype = None
+
+    # wgpuBindGroupLayoutRelease
+    lib.wgpuBindGroupLayoutRelease.argtypes = [WGPUBindGroupLayout]
+    lib.wgpuBindGroupLayoutRelease.restype = None
+
+    # wgpuPipelineLayoutRelease
+    lib.wgpuPipelineLayoutRelease.argtypes = [WGPUPipelineLayout]
+    lib.wgpuPipelineLayoutRelease.restype = None
+
+    # wgpuCommandBufferRelease
+    lib.wgpuCommandBufferRelease.argtypes = [WGPUCommandBuffer]
+    lib.wgpuCommandBufferRelease.restype = None
+
+    # wgpuComputePassEncoderRelease
+    lib.wgpuComputePassEncoderRelease.argtypes = [WGPUComputePassEncoder]
+    lib.wgpuComputePassEncoderRelease.restype = None
+
+    # wgpuCommandEncoderRelease
+    lib.wgpuCommandEncoderRelease.argtypes = [WGPUCommandEncoder]
+    lib.wgpuCommandEncoderRelease.restype = None
+
+    # wgpuDeviceRelease
+    lib.wgpuDeviceRelease.argtypes = [WGPUDevice]
+    lib.wgpuDeviceRelease.restype = None
+
+    # wgpuQueueRelease
+    lib.wgpuQueueRelease.argtypes = [WGPUQueue]
+    lib.wgpuQueueRelease.restype = None
+
+    # wgpuInstanceRelease
+    lib.wgpuInstanceRelease.argtypes = [WGPUInstance]
+    lib.wgpuInstanceRelease.restype = None
+
+    # wgpuDeviceTick
+    lib.wgpuDeviceTick.argtypes = [WGPUDevice]
+    lib.wgpuDeviceTick.restype = WGPUBool
+
+    # --- Timestamp query APIs ---
+    WGPUQuerySet = ctypes.c_void_p
+
+    # wgpuDeviceCreateQuerySet
+    lib.wgpuDeviceCreateQuerySet.argtypes = [WGPUDevice, ctypes.c_void_p]
+    lib.wgpuDeviceCreateQuerySet.restype = WGPUQuerySet
+
+    # wgpuCommandEncoderResolveQuerySet
+    lib.wgpuCommandEncoderResolveQuerySet.argtypes = [
+        WGPUCommandEncoder, WGPUQuerySet,
+        ctypes.c_uint32, ctypes.c_uint32,  # firstQuery, queryCount
+        WGPUBuffer, ctypes.c_uint64,  # destination, destinationOffset
+    ]
+    lib.wgpuCommandEncoderResolveQuerySet.restype = None
+
+    # wgpuQuerySetDestroy
+    lib.wgpuQuerySetDestroy.argtypes = [WGPUQuerySet]
+    lib.wgpuQuerySetDestroy.restype = None
+
+    # wgpuQuerySetRelease
+    lib.wgpuQuerySetRelease.argtypes = [WGPUQuerySet]
+    lib.wgpuQuerySetRelease.restype = None
+
+
+# ============================================================================
+# Numpy / WGSL type mappings
+# ============================================================================
+
+WGSL_TYPE_TO_NUMPY = {
+    'f32': np.float32,
+    'f16': np.float16,
+    'i32': np.int32,
+    'u32': np.uint32,
+}
+
+WGSL_TYPE_TO_STRUCT_FMT = {
+    'f32': '<f',
+    'f16': '<e',
+    'i32': '<i',
+    'u32': '<I',
+}
+
+
+# ============================================================================
+# GPUBuffer — handle to GPU-resident data
+# ============================================================================
+
+class GPUBuffer:
+    """Handle to a GPU-resident buffer for zero-copy kernel inputs/outputs.
+
+    When passed as a value in the ``buffers`` dict of ``run_kernel()``, the
+    runner skips the CPU→GPU upload and uses the GPU buffer directly.  When
+    returned by ``run_kernel()`` via ``gpu_outputs``, the result stays on
+    GPU and can be fed into subsequent kernel calls without readback.
+    """
+    __slots__ = ('_runner', 'handle', 'size', 'dtype', 'shape', '_owned')
+
+    def __init__(self, runner, handle, size, dtype=np.float32,
+                 shape=None, owned=True):
+        self._runner = runner
+        self.handle = handle       # WGPUBuffer
+        self.size = size           # bytes
+        self.dtype = dtype
+        self.shape = shape
+        self._owned = owned        # if True, __del__ destroys & releases
+
+    @property
+    def nbytes(self):
+        return self.size
+
+    def __del__(self):
+        if self._owned and self.handle:
+            runner = self._runner
+            if runner and runner._lib:
+                try:
+                    runner._lib.wgpuBufferDestroy(self.handle)
+                    runner._lib.wgpuBufferRelease(self.handle)
+                except Exception:
+                    pass
+            self.handle = None
+
+
+# ============================================================================
+# DawnRunner — main runtime class
+# ============================================================================
+
+class DawnRunner:
+    """Execute WGSL compute shaders on the GPU via Dawn's native WebGPU API."""
+
+    def __init__(self):
+        self._lib = _load_dawn()
+        if self._lib is None:
+            raise RuntimeError(
+                "Dawn WebGPU library not found. "
+                "Build Dawn or set DAWN_PATH environment variable. "
+                "Expected: webgpu_dawn.dll / libwebgpu_dawn.so"
+            )
+
+        # Create instance with TimedWaitAny feature enabled
+        features = (ctypes.c_uint32 * 1)(0x00000001)  # WGPUInstanceFeatureName_TimedWaitAny
+        desc = WGPUInstanceDescriptor()
+        desc.nextInChain = None
+        desc.requiredFeatureCount = 1
+        desc.requiredFeatures = ctypes.cast(features, ctypes.c_void_p)
+        desc.requiredLimits = None
+        self._instance = self._lib.wgpuCreateInstance(ctypes.byref(desc))
+        if not self._instance:
+            raise RuntimeError("Failed to create Dawn WebGPU instance")
+
+        # Request adapter (async with WaitAny)
+        self._adapter = None
+        self._adapter_error = None
+
+        @RequestAdapterCallback
+        def on_adapter(status, adapter, message, ud1, ud2):
+            if status == WGPURequestAdapterStatus.Success:
+                self._adapter = adapter
+            else:
+                msg = ""
+                if message.data:
+                    msg = message.data.decode("utf-8", errors="replace")
+                self._adapter_error = msg
+
+        # Keep callback alive
+        self._adapter_cb = on_adapter
+
+        # -- Adapter toggles (matching onnxruntime WebGPU EP) --
+        # use_dxc: Shader Model 6+ (required for native f16 on D3D12)
+        # allow_unsafe_apis: enable Chrome experimental features
+        adapter_toggle_names = [b"use_dxc", b"allow_unsafe_apis"]
+        self._adapter_toggle_ptrs = (ctypes.c_char_p * len(adapter_toggle_names))(
+            *adapter_toggle_names)
+        adapter_toggles = WGPUDawnTogglesDescriptor()
+        adapter_toggles.chain.next = None
+        adapter_toggles.chain.sType = WGPUSType.DawnTogglesDescriptor
+        adapter_toggles.enabledToggleCount = len(adapter_toggle_names)
+        adapter_toggles.enabledToggles = self._adapter_toggle_ptrs
+        adapter_toggles.disabledToggleCount = 0
+        adapter_toggles.disabledToggles = None
+        # prevent GC
+        self._adapter_toggles = adapter_toggles
+
+        opts = WGPURequestAdapterOptions()
+        opts.nextInChain = ctypes.cast(ctypes.pointer(adapter_toggles), ctypes.POINTER(WGPUChainedStruct))
+        opts.featureLevel = WGPUFeatureLevel.Core
+        opts.powerPreference = WGPUPowerPreference.HighPerformance
+        opts.forceFallbackAdapter = 0
+        opts.backendType = WGPUBackendType.D3D12  # Use D3D12 on Windows
+        opts.compatibleSurface = None
+
+        cb_info = WGPURequestAdapterCallbackInfo()
+        cb_info.nextInChain = None
+        cb_info.mode = WGPUCallbackMode.WaitAnyOnly
+        cb_info.callback = on_adapter
+        cb_info.userdata1 = None
+        cb_info.userdata2 = None
+
+        future = self._lib.wgpuInstanceRequestAdapter(
+            self._instance, ctypes.byref(opts), cb_info
+        )
+
+        # Wait for adapter
+        wait_info = WGPUFutureWaitInfo()
+        wait_info.future = future
+        wait_info.completed = 0
+
+        status = self._lib.wgpuInstanceWaitAny(
+            self._instance, 1, ctypes.byref(wait_info), ctypes.c_uint64(-1)  # UINT64_MAX = infinite timeout
+        )
+
+        if not self._adapter:
+            err = self._adapter_error or "unknown error"
+            raise RuntimeError(f"Failed to obtain WebGPU adapter from Dawn: {err}")
+
+        # Query adapter limits
+        adapter_limits = WGPULimits()
+        ctypes.memset(ctypes.byref(adapter_limits), 0, ctypes.sizeof(WGPULimits))
+        adapter_limits.nextInChain = None
+        self._lib.wgpuAdapterGetLimits(self._adapter, ctypes.byref(adapter_limits))
+        self._adapter_limits = adapter_limits
+
+        # Create device (synchronous Dawn extension)
+        # Detect ShaderF16 and Subgroups features.
+        # Dawn's enum values shifted when CoreFeaturesAndLimits (0x01) was added,
+        # so we probe both old and new IDs for compatibility with different
+        # Dawn builds.
+        WGPUFeatureName = ctypes.c_uint32
+        # New API: ShaderF16=0x0B, Subgroups=0x12
+        # Old API: ShaderF16=0x0A, Subgroups=0x11
+        FEATURE_SHADER_F16_IDS = [0x0000000B, 0x0000000A]
+        FEATURE_SUBGROUPS_IDS = [0x00000012, 0x00000011]
+        requested_features = []
+        self._shader_f16_id = None
+        self._subgroups_id = None
+        self._timestamp_query_id = None
+        for fid in FEATURE_SHADER_F16_IDS:
+            if self._lib.wgpuAdapterHasFeature(self._adapter, fid):
+                requested_features.append(fid)
+                self._shader_f16_id = fid
+                break
+        for fid in FEATURE_SUBGROUPS_IDS:
+            if self._lib.wgpuAdapterHasFeature(self._adapter, fid):
+                requested_features.append(fid)
+                self._subgroups_id = fid
+                break
+        # TimestampQuery: new=0x09, old=0x03
+        FEATURE_TIMESTAMP_IDS = [0x00000009, 0x00000003]
+        for fid in FEATURE_TIMESTAMP_IDS:
+            if self._lib.wgpuAdapterHasFeature(self._adapter, fid):
+                requested_features.append(fid)
+                self._timestamp_query_id = fid
+                break
+
+        # Request adapter's actual limits instead of conservative defaults.
+        # Copy the entire adapter limits struct — this handles both "max"
+        # fields (higher = better) and "min" alignment fields (must not
+        # request a value *lower* than supported).
+        required_limits = WGPULimits()
+        ctypes.memmove(ctypes.byref(required_limits),
+                       ctypes.byref(adapter_limits),
+                       ctypes.sizeof(WGPULimits))
+        required_limits.nextInChain = None
+        # Keep reference alive
+        self._required_limits = required_limits
+
+        # -- Device toggles (matching onnxruntime WebGPU EP) --
+        dev_enabled_names = [b"skip_validation", b"disable_robustness",
+                             b"d3d_disable_ieee_strictness"]
+        dev_disabled_names = [b"lazy_clear_resource_on_first_use",
+                              b"timestamp_quantization"]
+        self._dev_enabled_ptrs = (ctypes.c_char_p * len(dev_enabled_names))(
+            *dev_enabled_names)
+        self._dev_disabled_ptrs = (ctypes.c_char_p * len(dev_disabled_names))(
+            *dev_disabled_names)
+        dev_toggles = WGPUDawnTogglesDescriptor()
+        dev_toggles.chain.next = None
+        dev_toggles.chain.sType = WGPUSType.DawnTogglesDescriptor
+        dev_toggles.enabledToggleCount = len(dev_enabled_names)
+        dev_toggles.enabledToggles = self._dev_enabled_ptrs
+        dev_toggles.disabledToggleCount = len(dev_disabled_names)
+        dev_toggles.disabledToggles = self._dev_disabled_ptrs
+        self._dev_toggles = dev_toggles
+
+        dev_desc = WGPUDeviceDescriptor()
+        ctypes.memset(ctypes.byref(dev_desc), 0, ctypes.sizeof(WGPUDeviceDescriptor))
+        dev_desc.nextInChain = ctypes.cast(ctypes.pointer(dev_toggles), ctypes.POINTER(WGPUChainedStruct))
+        dev_desc.label = WGPUStringView.from_str("triton")
+
+        # Set uncaptured error callback to catch validation errors
+        @UncapturedErrorCallback
+        def on_uncaptured_error(device, error_type, message, ud1, ud2):
+            try:
+                msg_str = message.data[:message.length].decode('utf-8', errors='replace') if message.data and message.length > 0 else "<no message>"
+            except Exception:
+                msg_str = "<failed to decode>"
+            error_names = {0: "NoError", 1: "Validation", 2: "OutOfMemory", 3: "Internal", 4: "Unknown", 5: "DeviceLost"}
+            print(f"[DAWN ERROR] type={error_names.get(error_type, error_type)} msg={msg_str[:500]}", flush=True)
+        self._uncaptured_error_cb = on_uncaptured_error
+        dev_desc.uncapturedErrorCallbackInfo.callback = ctypes.cast(
+            on_uncaptured_error, ctypes.c_void_p)
+        dev_desc.uncapturedErrorCallbackInfo.userdata1 = None
+        dev_desc.uncapturedErrorCallbackInfo.userdata2 = None
+        if requested_features:
+            features_array = (WGPUFeatureName * len(requested_features))(
+                *requested_features)
+            dev_desc.requiredFeatureCount = len(requested_features)
+            dev_desc.requiredFeatures = ctypes.cast(features_array, ctypes.c_void_p)
+        else:
+            dev_desc.requiredFeatureCount = 0
+            dev_desc.requiredFeatures = None
+        dev_desc.requiredLimits = ctypes.pointer(required_limits)
+        self._has_subgroups = self._subgroups_id is not None
+        self._has_f16 = self._shader_f16_id is not None
+        self._has_timestamp_query = self._timestamp_query_id is not None
+
+        self._device = self._lib.wgpuAdapterCreateDevice(
+            self._adapter, ctypes.byref(dev_desc)
+        )
+        if not self._device:
+            raise RuntimeError("Failed to create Dawn WebGPU device")
+
+        self._queue = self._lib.wgpuDeviceGetQueue(self._device)
+
+        # Cache for pipelines and shader modules
+        self._pipeline_cache = {}  # wgsl_hash -> (shader_module, pipeline, bg_layout, pipeline_layout)
+        # Cache for GPU buffers to avoid per-call alloc/dealloc
+        self._buffer_cache = {}  # (name, size, usage) -> WGPUBuffer
+        # Toggle pool for gpu_outputs — avoids fresh allocation each call
+        self._gpu_out_toggles = {}  # pool_key -> 0 or 1
+        # Track total GPU memory allocated via this runner
+        self._total_gpu_bytes = 0
+        self._gpu_alloc_count = 0
+
+    def gpu_memory_stats(self) -> dict:
+        """Return GPU memory usage statistics.
+
+        Tracks all buffers allocated via this runner:
+        - buffer_cache: internal kernel I/O buffers (reused per dispatch)
+        - upload_to_gpu: weight and data buffers (permanent, owned)
+        """
+        cache_bytes = sum(size for (_, size, _) in self._buffer_cache.keys())
+        return {
+            'total_allocated_mb': self._total_gpu_bytes / 1024 / 1024,
+            'buffer_cache_entries': len(self._buffer_cache),
+            'buffer_cache_mb': cache_bytes / 1024 / 1024,
+            'pipeline_cache_entries': len(self._pipeline_cache),
+            'alloc_count': self._gpu_alloc_count,
+        }
+
+    @property
+    def adapter_info(self) -> dict:
+        """Query adapter info: vendor, device name, backend type."""
+        if hasattr(self, '_adapter_info_cache'):
+            return self._adapter_info_cache
+        info = WGPUAdapterInfo()
+        ctypes.memset(ctypes.byref(info), 0, ctypes.sizeof(WGPUAdapterInfo))
+        self._lib.wgpuAdapterGetInfo(self._adapter, ctypes.byref(info))
+
+        def _sv(sv):
+            if sv.data and sv.length > 0:
+                return sv.data[:sv.length].decode('utf-8', errors='replace')
+            return ''
+
+        backend_names = {
+            WGPUBackendType.D3D11: 'D3D11', WGPUBackendType.D3D12: 'D3D12',
+            WGPUBackendType.Metal: 'Metal', WGPUBackendType.Vulkan: 'Vulkan',
+            WGPUBackendType.WebGPU: 'WebGPU', WGPUBackendType.Null: 'Null',
+        }
+        self._adapter_info_cache = {
+            'vendor': _sv(info.vendor),
+            'architecture': _sv(info.architecture),
+            'device': _sv(info.device),
+            'description': _sv(info.description),
+            'backend': backend_names.get(info.backendType, f'Unknown({info.backendType})'),
+            'vendorID': info.vendorID,
+            'deviceID': info.deviceID,
+        }
+        return self._adapter_info_cache
+
+    @property
+    def adapter_info_str(self) -> str:
+        """Human-readable GPU adapter string."""
+        info = self.adapter_info
+        return f"{info['description']} ({info['backend']})"
+
+    # -- Batched dispatch ------------------------------------------------------
+
+    def begin_batch(self):
+        """Start a batched dispatch session.
+
+        While batching, run_kernel() with gpu_outputs accumulates
+        compute passes in a shared command encoder instead of submitting
+        individually. Call end_batch() to submit all at once.
+
+        This eliminates per-dispatch wgpuQueueSubmit overhead (~0.1ms each)
+        and can significantly speed up sequences of GPU operations.
+        """
+        lib = self._lib
+        enc_desc = WGPUCommandEncoderDescriptor()
+        enc_desc.nextInChain = None
+        enc_desc.label = WGPUStringView.from_str("batch")
+        self._batch_encoder = lib.wgpuDeviceCreateCommandEncoder(
+            self._device, ctypes.byref(enc_desc))
+        self._batch_readbacks = []  # list of (gpu_buf, size, bindings) to readback
+
+    def end_batch(self, readback_buffers=None):
+        """Submit all batched dispatches and optionally readback results.
+
+        Args:
+            readback_buffers: optional list of GPUBuffer objects to read back
+
+        Returns:
+            dict mapping GPUBuffer → numpy array for requested readbacks
+        """
+        if not hasattr(self, '_batch_encoder') or self._batch_encoder is None:
+            return {}
+
+        lib = self._lib
+        encoder = self._batch_encoder
+
+        # Add readback copies for requested buffers
+        results = {}
+        readback_mapping = {}  # rb_buf -> (gpu_buf, size)
+        if readback_buffers:
+            readback_usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+            for i, gpu_buf in enumerate(readback_buffers):
+                size = gpu_buf.size
+                rb_name = f"__batch_rb_{i}__"
+                rb_buf = self._get_or_create_buffer(
+                    rb_name, size, readback_usage)
+                lib.wgpuCommandEncoderCopyBufferToBuffer(
+                    encoder, gpu_buf.handle, 0, rb_buf, 0, size)
+                readback_mapping[rb_name] = (rb_buf, size, gpu_buf)
+
+        # Finish and submit
+        cb_desc = WGPUCommandBufferDescriptor()
+        cb_desc.nextInChain = None
+        cb_desc.label = WGPUStringView.from_str("")
+        cmd_buf = lib.wgpuCommandEncoderFinish(encoder, ctypes.byref(cb_desc))
+        cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+        lib.wgpuQueueSubmit(
+            self._queue, 1,
+            ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer)))
+
+        # Release encoder and command buffer
+        lib.wgpuCommandEncoderRelease(encoder)
+        lib.wgpuCommandBufferRelease(cmd_buf)
+
+        # Release accumulated batch compute passes and bind groups
+        if hasattr(self, '_batch_cleanup'):
+            for compute_pass, bind_group in self._batch_cleanup:
+                lib.wgpuComputePassEncoderRelease(compute_pass)
+                lib.wgpuBindGroupRelease(bind_group)
+            self._batch_cleanup = []
+
+        # Read back requested buffers
+        for rb_name, (rb_buf, size, gpu_buf) in readback_mapping.items():
+            data = self._map_and_read(rb_buf, size, gpu_buf.dtype)
+            results[id(gpu_buf)] = data
+
+        self._batch_encoder = None
+        self._batch_readbacks = []
+        return results
+
+    def _map_and_read(self, rb_buf, size, dtype):
+        """Map a readback buffer and return numpy array."""
+        lib = self._lib
+        map_done = [False]
+        map_status = [0]
+
+        @BufferMapCallback
+        def on_map(status, message, ud1, ud2, _md=map_done, _ms=map_status):
+            _md[0] = True
+            _ms[0] = status
+
+        self._map_cb = on_map
+        cb_info = WGPUBufferMapCallbackInfo()
+        cb_info.nextInChain = None
+        cb_info.mode = WGPUCallbackMode.WaitAnyOnly
+        cb_info.callback = on_map
+        cb_info.userdata1 = None
+        cb_info.userdata2 = None
+
+        future = lib.wgpuBufferMapAsync(
+            rb_buf, MAP_MODE_READ, 0, size, cb_info)
+        wait_info = WGPUFutureWaitInfo()
+        wait_info.future = future
+        wait_info.completed = 0
+        lib.wgpuInstanceWaitAny(
+            self._instance, 1, ctypes.byref(wait_info),
+            ctypes.c_uint64(-1))
+
+        if map_status[0] != WGPUMapAsyncStatus.Success:
+            raise RuntimeError(f"Batch readback map failed: {map_status[0]}")
+
+        data_ptr = lib.wgpuBufferGetConstMappedRange(rb_buf, 0, size)
+        np_dtype = dtype if dtype else np.float32
+        result = np.ctypeslib.as_array(
+            (ctypes.c_uint8 * size).from_address(data_ptr),
+            shape=(size,)
+        ).view(np_dtype).copy()
+
+        lib.wgpuBufferUnmap(rb_buf)
+        return result
+
+    @property
+    def is_batching(self):
+        """Whether we're currently in a batch dispatch session."""
+        return hasattr(self, '_batch_encoder') and self._batch_encoder is not None
+
+    @property
+    def max_storage_buffer_binding_size(self) -> int:
+        """Adapter's maxStorageBufferBindingSize in bytes."""
+        return int(self._adapter_limits.maxStorageBufferBindingSize)
+
+    @property
+    def max_compute_invocations_per_workgroup(self) -> int:
+        """Adapter's maxComputeInvocationsPerWorkgroup."""
+        return int(self._adapter_limits.maxComputeInvocationsPerWorkgroup)
+
+    @property
+    def has_subgroups(self) -> bool:
+        """Whether the adapter supports native WebGPU Subgroups."""
+        return self._has_subgroups
+
+    @property
+    def has_f16(self) -> bool:
+        """Whether the adapter supports ShaderF16."""
+        return self._has_f16
+
+    @property
+    def has_timestamp_query(self) -> bool:
+        """Whether the adapter supports TimestampQuery."""
+        return self._has_timestamp_query
+
+    # -- Fast dispatch API ----------------------------------------------------
+    #
+    # Pre-create bind groups and submit multiple dispatches with minimal
+    # per-dispatch Python/ctypes overhead.  Used by the fast decode path
+    # to avoid repeated buffer allocation, bind group creation, and
+    # compute pass begin/end calls.
+
+    def get_pipeline_info(self, wgsl_code, buffer_bindings, param_fields):
+        """Get cached pipeline and bind group layout for compiled WGSL.
+
+        Returns (pipeline, bg_layout) tuple.
+        """
+        _, pipeline, bg_layout, _ = self._get_or_create_pipeline(
+            wgsl_code, buffer_bindings, param_fields)
+        return pipeline, bg_layout
+
+    def create_gpu_buffer(self, name, size_bytes):
+        """Create a named GPU storage buffer.
+
+        Returns the raw WGPUBuffer handle (ctypes.c_void_p).
+        Uses the existing buffer cache — safe to call multiple times
+        with the same name/size.
+        """
+        usage = (BUFFER_USAGE_STORAGE | BUFFER_USAGE_COPY_SRC
+                 | BUFFER_USAGE_COPY_DST)
+        return self._get_or_create_buffer(name, size_bytes, usage)
+
+    def create_bind_group(self, bg_layout, entries):
+        """Create a bind group from raw buffer handles.
+
+        Args:
+            bg_layout: WGPUBindGroupLayout handle
+            entries: list of (binding_index, buffer_handle, size_bytes) tuples
+
+        Returns:
+            WGPUBindGroup handle (caller must release when done)
+        """
+        lib = self._lib
+        n = len(entries)
+        BindEntryArray = WGPUBindGroupEntry * n
+        bind_entries = BindEntryArray()
+        for i, (binding, buf_handle, size) in enumerate(entries):
+            ctypes.memset(ctypes.byref(bind_entries[i]), 0,
+                         ctypes.sizeof(WGPUBindGroupEntry))
+            bind_entries[i].binding = binding
+            bind_entries[i].buffer = buf_handle
+            bind_entries[i].offset = 0
+            bind_entries[i].size = size
+
+        bg_desc = WGPUBindGroupDescriptor()
+        bg_desc.nextInChain = None
+        bg_desc.label = WGPUStringView.from_str("")
+        bg_desc.layout = bg_layout
+        bg_desc.entryCount = n
+        bg_desc.entries = bind_entries
+
+        return lib.wgpuDeviceCreateBindGroup(
+            self._device, ctypes.byref(bg_desc))
+
+    def write_buffer(self, buf_handle, data_bytes):
+        """Write raw bytes to a GPU buffer."""
+        size = len(data_bytes)
+        self._lib.wgpuQueueWriteBuffer(
+            self._queue, buf_handle, 0,
+            ctypes.c_char_p(data_bytes), size)
+
+    def submit_dispatches(self, dispatches, readback=None):
+        """Record and submit dispatches in a single compute pass.
+
+        Args:
+            dispatches: list of (pipeline, bind_group, (gx, gy, gz)) tuples
+            readback: optional (gpu_buf_handle, size_bytes, numpy_dtype)
+                      to copy-back after GPU finishes
+
+        Returns:
+            numpy array if readback requested, else None
+        """
+        lib = self._lib
+
+        enc_desc = WGPUCommandEncoderDescriptor()
+        enc_desc.nextInChain = None
+        enc_desc.label = WGPUStringView.from_str("fast")
+        encoder = lib.wgpuDeviceCreateCommandEncoder(
+            self._device, ctypes.byref(enc_desc))
+
+        pass_desc = WGPUComputePassDescriptor()
+        pass_desc.nextInChain = None
+        pass_desc.label = WGPUStringView.from_str("")
+        pass_desc.timestampWrites = None
+
+        compute_pass = lib.wgpuCommandEncoderBeginComputePass(
+            encoder, ctypes.byref(pass_desc))
+
+        for pipeline, bind_group, grid in dispatches:
+            lib.wgpuComputePassEncoderSetPipeline(compute_pass, pipeline)
+            lib.wgpuComputePassEncoderSetBindGroup(
+                compute_pass, 0, bind_group, 0, None)
+            gx = grid[0] if len(grid) > 0 else 1
+            gy = grid[1] if len(grid) > 1 else 1
+            gz = grid[2] if len(grid) > 2 else 1
+            lib.wgpuComputePassEncoderDispatchWorkgroups(
+                compute_pass, gx, gy, gz)
+
+        lib.wgpuComputePassEncoderEnd(compute_pass)
+
+        # Readback copy
+        rb_buf = None
+        if readback:
+            gpu_handle, size, dtype = readback
+            readback_usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+            rb_buf = self._get_or_create_buffer(
+                "__fast_rb__", size, readback_usage)
+            lib.wgpuCommandEncoderCopyBufferToBuffer(
+                encoder, gpu_handle, 0, rb_buf, 0, size)
+
+        # Finish + submit
+        cb_desc = WGPUCommandBufferDescriptor()
+        cb_desc.nextInChain = None
+        cb_desc.label = WGPUStringView.from_str("")
+        cmd_buf = lib.wgpuCommandEncoderFinish(
+            encoder, ctypes.byref(cb_desc))
+        cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+        lib.wgpuQueueSubmit(
+            self._queue, 1,
+            ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer)))
+
+        lib.wgpuComputePassEncoderRelease(compute_pass)
+        lib.wgpuCommandEncoderRelease(encoder)
+        lib.wgpuCommandBufferRelease(cmd_buf)
+
+        if rb_buf:
+            return self._map_and_read(rb_buf, size, dtype)
+        return None
+
+    def submit_dispatches_pipelined(self, layer_batches, readback=None):
+        """Submit dispatch batches with CPU/GPU pipelining.
+
+        Groups consecutive batches into larger encoder submissions to reduce
+        per-submit overhead while maintaining CPU/GPU overlap.
+
+        Args:
+            layer_batches: list of lists, each inner list is
+                [(pipeline, bind_group, grid), ...] for one layer
+            readback: optional (gpu_buf_handle, size_bytes, numpy_dtype)
+
+        Returns:
+            numpy array if readback requested, else None
+        """
+        lib = self._lib
+        n_batches = len(layer_batches)
+        GROUP_SIZE = 4  # layers per submit
+
+        pass_desc = WGPUComputePassDescriptor()
+        pass_desc.nextInChain = None
+        pass_desc.label = WGPUStringView.from_str("")
+        pass_desc.timestampWrites = None
+
+        group_start = 0
+        while group_start < n_batches:
+            group_end = min(group_start + GROUP_SIZE, n_batches)
+            is_last_group = (group_end == n_batches)
+
+            enc_desc = WGPUCommandEncoderDescriptor()
+            enc_desc.nextInChain = None
+            enc_desc.label = WGPUStringView.from_str("pipe")
+            encoder = lib.wgpuDeviceCreateCommandEncoder(
+                self._device, ctypes.byref(enc_desc))
+
+            compute_pass = lib.wgpuCommandEncoderBeginComputePass(
+                encoder, ctypes.byref(pass_desc))
+
+            for batch_idx in range(group_start, group_end):
+                dispatches = layer_batches[batch_idx]
+
+                for pipeline, bind_group, grid in dispatches:
+                    lib.wgpuComputePassEncoderSetPipeline(compute_pass, pipeline)
+                    lib.wgpuComputePassEncoderSetBindGroup(
+                        compute_pass, 0, bind_group, 0, None)
+                    gx = grid[0] if len(grid) > 0 else 1
+                    gy = grid[1] if len(grid) > 1 else 1
+                    gz = grid[2] if len(grid) > 2 else 1
+                    lib.wgpuComputePassEncoderDispatchWorkgroups(
+                        compute_pass, gx, gy, gz)
+
+            lib.wgpuComputePassEncoderEnd(compute_pass)
+            lib.wgpuComputePassEncoderRelease(compute_pass)
+
+            # Readback on last group
+            if is_last_group and readback:
+                gpu_handle, size, dtype = readback
+                readback_usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+                rb_buf = self._get_or_create_buffer(
+                    "__fast_rb__", size, readback_usage)
+                lib.wgpuCommandEncoderCopyBufferToBuffer(
+                    encoder, gpu_handle, 0, rb_buf, 0, size)
+
+            cb_desc = WGPUCommandBufferDescriptor()
+            cb_desc.nextInChain = None
+            cb_desc.label = WGPUStringView.from_str("")
+            cmd_buf = lib.wgpuCommandEncoderFinish(
+                encoder, ctypes.byref(cb_desc))
+            cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+            lib.wgpuQueueSubmit(
+                self._queue, 1,
+                ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer)))
+
+            lib.wgpuCommandEncoderRelease(encoder)
+            lib.wgpuCommandBufferRelease(cmd_buf)
+
+            group_start = group_end
+
+        if readback:
+            gpu_handle, size, dtype = readback
+            readback_usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+            rb_buf = self._get_or_create_buffer(
+                "__fast_rb__", size, readback_usage)
+            return self._map_and_read(rb_buf, size, dtype)
+        return None
+
+    # -- GPU memory management ------------------------------------------------
+
+    # Maximum bytes per wgpuQueueWriteBuffer call.
+    # D3D12 staging heap is limited to 2GB; writes above this threshold
+    # silently fail, producing a zero-filled GPU buffer.
+    _MAX_WRITE_SIZE = 2 * 1024 * 1024 * 1024  # 2 GiB
+
+    def upload_to_gpu(self, data, name="tensor"):
+        """Upload a numpy array to GPU memory and return a GPUBuffer handle.
+
+        The returned GPUBuffer can be passed in the ``buffers`` dict of
+        ``run_kernel()`` in place of a numpy array.  The runner will use the
+        GPU buffer directly, skipping the per-call CPU→GPU upload.  This is
+        ideal for model weights that are constant across inference steps.
+
+        For buffers larger than 2 GiB, the upload is split into chunks
+        because wgpuQueueWriteBuffer silently fails above the D3D12
+        staging heap limit.
+        """
+        np_arr = np.ascontiguousarray(data)
+        usage = (BUFFER_USAGE_STORAGE | BUFFER_USAGE_COPY_SRC
+                 | BUFFER_USAGE_COPY_DST)
+
+        buf_desc = WGPUBufferDescriptor()
+        buf_desc.nextInChain = None
+        buf_desc.label = WGPUStringView.from_str(name)
+        buf_desc.usage = usage
+        buf_desc.size = np_arr.nbytes
+        buf_desc.mappedAtCreation = 0
+
+        buf = self._lib.wgpuDeviceCreateBuffer(
+            self._device, ctypes.byref(buf_desc))
+        if not buf:
+            raise RuntimeError(f"Failed to create GPU buffer '{name}'")
+        self._total_gpu_bytes += np_arr.nbytes
+        self._gpu_alloc_count += 1
+
+        total = np_arr.nbytes
+        if total <= self._MAX_WRITE_SIZE:
+            # Single write
+            data_ptr = np_arr.ctypes.data_as(ctypes.c_void_p)
+            self._lib.wgpuQueueWriteBuffer(
+                self._queue, buf, 0, data_ptr, total)
+        else:
+            # Chunked write to stay under D3D12 staging limit
+            chunk = self._MAX_WRITE_SIZE
+            base = np_arr.ctypes.data
+            offset = 0
+            while offset < total:
+                sz = min(chunk, total - offset)
+                ptr = ctypes.c_void_p(base + offset)
+                self._lib.wgpuQueueWriteBuffer(
+                    self._queue, buf, offset, ptr, sz)
+                offset += sz
+
+        return GPUBuffer(self, buf, np_arr.nbytes,
+                         np_arr.dtype, np_arr.shape)
+
+    def readback(self, gpu_buffer, dtype=None):
+        """Read a GPUBuffer back to a numpy array."""
+        if dtype is None:
+            dtype = gpu_buffer.dtype
+        lib = self._lib
+        size = gpu_buffer.size
+
+        # Create temporary readback buffer
+        rb_desc = WGPUBufferDescriptor()
+        rb_desc.nextInChain = None
+        rb_desc.label = WGPUStringView.from_str("readback")
+        rb_desc.usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+        rb_desc.size = size
+        rb_desc.mappedAtCreation = 0
+        rb_buf = lib.wgpuDeviceCreateBuffer(
+            self._device, ctypes.byref(rb_desc))
+
+        # Encode copy + submit
+        enc_desc = WGPUCommandEncoderDescriptor()
+        enc_desc.nextInChain = None
+        enc_desc.label = WGPUStringView.from_str("")
+        encoder = lib.wgpuDeviceCreateCommandEncoder(
+            self._device, ctypes.byref(enc_desc))
+        lib.wgpuCommandEncoderCopyBufferToBuffer(
+            encoder, gpu_buffer.handle, 0, rb_buf, 0, size)
+        cb_desc = WGPUCommandBufferDescriptor()
+        cb_desc.nextInChain = None
+        cb_desc.label = WGPUStringView.from_str("")
+        cmd_buf = lib.wgpuCommandEncoderFinish(
+            encoder, ctypes.byref(cb_desc))
+        cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+        lib.wgpuQueueSubmit(
+            self._queue, 1,
+            ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer)))
+
+        # Map and read
+        map_done = [False]
+        map_status = [0]
+
+        @BufferMapCallback
+        def on_map(status, message, ud1, ud2,
+                   _md=map_done, _ms=map_status):
+            _md[0] = True
+            _ms[0] = status
+        self._map_cb = on_map
+
+        cb_info = WGPUBufferMapCallbackInfo()
+        cb_info.nextInChain = None
+        cb_info.mode = WGPUCallbackMode.WaitAnyOnly
+        cb_info.callback = on_map
+        cb_info.userdata1 = None
+        cb_info.userdata2 = None
+        future = lib.wgpuBufferMapAsync(
+            rb_buf, MAP_MODE_READ, 0, size, cb_info)
+        wait_info = WGPUFutureWaitInfo()
+        wait_info.future = future
+        wait_info.completed = 0
+        lib.wgpuInstanceWaitAny(
+            self._instance, 1, ctypes.byref(wait_info),
+            ctypes.c_uint64(-1))
+
+        if map_status[0] != WGPUMapAsyncStatus.Success:
+            raise RuntimeError(
+                f"Readback map failed: status={map_status[0]}")
+        data_ptr = lib.wgpuBufferGetConstMappedRange(rb_buf, 0, size)
+        result = np.ctypeslib.as_array(
+            (ctypes.c_uint8 * size).from_address(data_ptr),
+            shape=(size,)).copy()
+        lib.wgpuBufferUnmap(rb_buf)
+        lib.wgpuBufferDestroy(rb_buf)
+        lib.wgpuBufferRelease(rb_buf)
+        lib.wgpuCommandBufferRelease(cmd_buf)
+        lib.wgpuCommandEncoderRelease(encoder)
+
+        return np.frombuffer(result, dtype=dtype)
+
+    def gpu_slice(self, gpu_buffer, offset_bytes, size_bytes, name="slice"):
+        """Create a new GPU buffer containing a slice of an existing buffer.
+
+        Uses GPU-to-GPU copy (CopyBufferToBuffer) — no CPU readback needed.
+        This is essential for slicing large buffers (>2GB) that exceed the
+        mapping limit for readback.
+
+        Args:
+            gpu_buffer: source GPUBuffer
+            offset_bytes: byte offset into the source buffer
+            size_bytes: number of bytes to copy
+            name: label for the new buffer
+
+        Returns:
+            GPUBuffer containing the copied data
+        """
+        lib = self._lib
+        usage = (BUFFER_USAGE_STORAGE | BUFFER_USAGE_COPY_SRC
+                 | BUFFER_USAGE_COPY_DST)
+
+        buf_desc = WGPUBufferDescriptor()
+        buf_desc.nextInChain = None
+        buf_desc.label = WGPUStringView.from_str(name)
+        buf_desc.usage = usage
+        buf_desc.size = size_bytes
+        buf_desc.mappedAtCreation = 0
+
+        dst_buf = lib.wgpuDeviceCreateBuffer(
+            self._device, ctypes.byref(buf_desc))
+        if not dst_buf:
+            raise RuntimeError(f"Failed to create GPU buffer '{name}'")
+
+        # Encode GPU-to-GPU copy
+        enc_desc = WGPUCommandEncoderDescriptor()
+        enc_desc.nextInChain = None
+        enc_desc.label = WGPUStringView.from_str("")
+        encoder = lib.wgpuDeviceCreateCommandEncoder(
+            self._device, ctypes.byref(enc_desc))
+        lib.wgpuCommandEncoderCopyBufferToBuffer(
+            encoder, gpu_buffer.handle, offset_bytes, dst_buf, 0, size_bytes)
+        cb_desc = WGPUCommandBufferDescriptor()
+        cb_desc.nextInChain = None
+        cb_desc.label = WGPUStringView.from_str("")
+        cmd_buf = lib.wgpuCommandEncoderFinish(
+            encoder, ctypes.byref(cb_desc))
+        cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+        lib.wgpuQueueSubmit(
+            self._queue, 1,
+            ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer)))
+
+        lib.wgpuCommandBufferRelease(cmd_buf)
+        lib.wgpuCommandEncoderRelease(encoder)
+
+        return GPUBuffer(self, dst_buf, size_bytes,
+                         gpu_buffer.dtype, None)
+
+    def _get_or_create_pipeline(self, wgsl_code, buffer_bindings, param_fields):
+        """Get cached pipeline or create a new one.
+
+        Returns (shader_module, pipeline, bg_layout, pipeline_layout).
+        Pipelines are cached by WGSL code hash — the shader, layout, and
+        pipeline are created once and reused across calls.
+        """
+        import hashlib
+        key = hashlib.sha256(wgsl_code.encode()).hexdigest()
+        if key in self._pipeline_cache:
+            return self._pipeline_cache[key]
+
+        lib = self._lib
+
+        # Create shader module
+        wgsl_source = WGPUShaderSourceWGSL()
+        wgsl_source.chain.next = None
+        wgsl_source.chain.sType = WGPUSType.ShaderSourceWGSL
+        wgsl_bytes = wgsl_code.encode("utf-8")
+        wgsl_source.code = WGPUStringView(wgsl_bytes, len(wgsl_bytes))
+
+        shader_desc = WGPUShaderModuleDescriptor()
+        shader_desc.nextInChain = ctypes.cast(
+            ctypes.pointer(wgsl_source.chain), ctypes.POINTER(WGPUChainedStruct)
+        )
+        shader_desc.label = WGPUStringView.from_str("triton_kernel")
+
+        shader_module = lib.wgpuDeviceCreateShaderModule(
+            self._device, ctypes.byref(shader_desc)
+        )
+        if not shader_module:
+            raise RuntimeError("Failed to create WGSL shader module")
+
+        # Create bind group layout
+        n_bindings = len(buffer_bindings) + (1 if param_fields else 0)
+        LayoutEntryArray = WGPUBindGroupLayoutEntry * n_bindings
+        layout_entries = LayoutEntryArray()
+
+        for i, bb in enumerate(buffer_bindings):
+            ctypes.memset(ctypes.byref(layout_entries[i]), 0,
+                         ctypes.sizeof(WGPUBindGroupLayoutEntry))
+            layout_entries[i].binding = bb.binding
+            layout_entries[i].visibility = SHADER_STAGE_COMPUTE
+            if bb.access == 'read':
+                layout_entries[i].buffer.type = WGPUBufferBindingType.ReadOnlyStorage
+            else:
+                layout_entries[i].buffer.type = WGPUBufferBindingType.Storage
+
+        if param_fields:
+            idx = len(buffer_bindings)
+            ctypes.memset(ctypes.byref(layout_entries[idx]), 0,
+                         ctypes.sizeof(WGPUBindGroupLayoutEntry))
+            layout_entries[idx].binding = len(buffer_bindings)
+            layout_entries[idx].visibility = SHADER_STAGE_COMPUTE
+            layout_entries[idx].buffer.type = WGPUBufferBindingType.ReadOnlyStorage
+
+        bg_layout_desc = WGPUBindGroupLayoutDescriptor()
+        bg_layout_desc.nextInChain = None
+        bg_layout_desc.label = WGPUStringView.from_str("")
+        bg_layout_desc.entryCount = n_bindings
+        bg_layout_desc.entries = layout_entries
+
+        bg_layout = lib.wgpuDeviceCreateBindGroupLayout(
+            self._device, ctypes.byref(bg_layout_desc)
+        )
+
+        # Create pipeline layout
+        bg_layouts = (ctypes.c_void_p * 1)(bg_layout)
+        pl_desc = WGPUPipelineLayoutDescriptor()
+        pl_desc.nextInChain = None
+        pl_desc.label = WGPUStringView.from_str("")
+        pl_desc.bindGroupLayoutCount = 1
+        pl_desc.bindGroupLayouts = ctypes.cast(bg_layouts, ctypes.POINTER(WGPUBindGroupLayout))
+        pl_desc.immediateSize = 0
+
+        pipeline_layout = lib.wgpuDeviceCreatePipelineLayout(
+            self._device, ctypes.byref(pl_desc)
+        )
+
+        # Create compute pipeline
+        cp_desc = WGPUComputePipelineDescriptor()
+        cp_desc.nextInChain = None
+        cp_desc.label = WGPUStringView.from_str("triton_pipeline")
+        cp_desc.layout = pipeline_layout
+        cp_desc.compute.nextInChain = None
+        cp_desc.compute.module = shader_module
+        cp_desc.compute.entryPoint = WGPUStringView.from_str("main")
+        cp_desc.compute.constantCount = 0
+        cp_desc.compute.constants = None
+
+        pipeline = lib.wgpuDeviceCreateComputePipeline(
+            self._device, ctypes.byref(cp_desc)
+        )
+        if not pipeline:
+            raise RuntimeError("Failed to create compute pipeline")
+
+        result = (shader_module, pipeline, bg_layout, pipeline_layout)
+        self._pipeline_cache[key] = result
+        return result
+
+    def _get_or_create_buffer(self, name, size, usage):
+        """Get a cached GPU buffer or create a new one.
+
+        Buffers are cached by (name, size, usage). If a buffer with the same
+        key exists, it is reused; otherwise a new one is created.
+        """
+        key = (name, size, usage)
+        if key in self._buffer_cache:
+            return self._buffer_cache[key]
+
+        lib = self._lib
+        buf_desc = WGPUBufferDescriptor()
+        buf_desc.nextInChain = None
+        buf_desc.label = WGPUStringView.from_str(name)
+        buf_desc.usage = usage
+        buf_desc.size = size
+        buf_desc.mappedAtCreation = 0
+
+        buf = lib.wgpuDeviceCreateBuffer(self._device, ctypes.byref(buf_desc))
+        if not buf:
+            raise RuntimeError(f"Failed to create buffer '{name}'")
+        self._buffer_cache[key] = buf
+        self._total_gpu_bytes += size
+        self._gpu_alloc_count += 1
+        return buf
+
+    def run_kernel(
+        self,
+        wgsl_code: str,
+        buffer_bindings: list,
+        param_fields: list,
+        workgroup_size: int,
+        grid: tuple,
+        buffers: dict,
+        scalars: dict = None,
+        gpu_outputs: set = None,
+        timestamp_writes_ptr=None,
+    ) -> dict:
+        """
+        Execute a WGSL compute shader on the GPU.
+
+        Pipelines and GPU buffers are cached across calls for efficiency.
+        Only bind groups and command buffers are created per dispatch.
+
+        Args:
+            buffers: dict mapping binding name → numpy array *or* GPUBuffer.
+                     GPUBuffer values bypass the CPU→GPU upload entirely.
+            gpu_outputs: optional set of read_write buffer names that should
+                         stay on GPU.  Those entries in the result dict will
+                         be GPUBuffer objects instead of numpy arrays.
+            timestamp_writes_ptr: optional ctypes pointer to
+                         WGPUPassTimestampWrites for GPU profiling.
+        """
+        scalars = scalars or {}
+        lib = self._lib
+
+        # Get or create cached pipeline
+        _, pipeline, bg_layout, _ = self._get_or_create_pipeline(
+            wgsl_code, buffer_bindings, param_fields)
+
+        return self._run_with_pipeline(
+            pipeline, bg_layout, buffer_bindings, param_fields,
+            workgroup_size, grid, buffers, scalars, gpu_outputs,
+            timestamp_writes_ptr=timestamp_writes_ptr,
+        )
+
+    def _run_with_pipeline(self, pipeline, bg_layout, buffer_bindings,
+                          param_fields, workgroup_size, grid, buffers,
+                          scalars, gpu_outputs=None,
+                          timestamp_writes_ptr=None):
+        """Execute a compute dispatch using cached pipeline.
+
+        GPU storage buffers are cached by (name, size) — only re-uploaded
+        when contents change.  Readback buffers are also cached.
+        Only bind groups, command encoders and command buffers are
+        created per dispatch (these are lightweight).
+
+        If *gpu_outputs* is given, the named read_write buffers are returned
+        as GPUBuffer objects instead of being read back to numpy.
+        """
+        lib = self._lib
+        batching = self.is_batching and gpu_outputs
+
+        # 1. Allocate / reuse GPU storage buffers and upload data
+        gpu_buffers = {}       # name -> WGPUBuffer
+        gpu_buf_sizes = {}     # name -> size in bytes
+        gpu_owned = {}         # name -> True if buffer is a fresh allocation for gpu_outputs
+
+        storage_usage = (BUFFER_USAGE_STORAGE | BUFFER_USAGE_COPY_SRC
+                         | BUFFER_USAGE_COPY_DST)
+
+        for bb in buffer_bindings:
+            if bb.name in buffers:
+                val = buffers[bb.name]
+                if isinstance(val, GPUBuffer):
+                    # Pre-uploaded GPU buffer — use directly, skip upload
+                    gpu_buf = val.handle
+                    buf_size = val.size
+                    gpu_owned[bb.name] = False
+                else:
+                    np_arr = np.ascontiguousarray(val)
+                    buf_size = np_arr.nbytes
+
+                    if gpu_outputs and bb.name in gpu_outputs:
+                        # Output stays on GPU — use toggle-cached buffer pool
+                        # to avoid fresh allocation each call. Two buffers per
+                        # (binding_name, size) alternate to prevent read-write
+                        # aliasing between consecutive operations.
+                        pool_key = f"__gpu_out_{bb.name}_{buf_size}"
+                        toggle = self._gpu_out_toggles.get(pool_key, 0)
+                        cache_name = f"{pool_key}_{toggle}"
+                        self._gpu_out_toggles[pool_key] = 1 - toggle
+                        gpu_buf = self._get_or_create_buffer(
+                            cache_name, buf_size, storage_usage)
+                        gpu_owned[bb.name] = False
+                        # Skip uploading zeros — kernel overwrites entirely.
+                    else:
+                        gpu_buf = self._get_or_create_buffer(
+                            bb.name, buf_size, storage_usage)
+                        gpu_owned[bb.name] = False
+
+                        # Upload data
+                        data_ptr = np_arr.ctypes.data_as(ctypes.c_void_p)
+                        lib.wgpuQueueWriteBuffer(
+                            self._queue, gpu_buf, 0, data_ptr, np_arr.nbytes)
+            else:
+                buf_size = 16  # dummy
+                gpu_buf = self._get_or_create_buffer(
+                    bb.name, buf_size, storage_usage)
+                gpu_owned[bb.name] = False
+
+            gpu_buffers[bb.name] = gpu_buf
+            gpu_buf_sizes[bb.name] = buf_size
+
+        # 2. Params buffer (scalar arguments)
+        params_buf = None
+        params_size = 0
+        if param_fields:
+            params_data = bytearray()
+            for pf in param_fields:
+                val = scalars.get(pf.name, 0)
+                fmt = WGSL_TYPE_TO_STRUCT_FMT.get(pf.wgsl_type, '<i')
+                params_data.extend(struct.pack(fmt, val))
+            while len(params_data) < 16:
+                params_data.extend(b'\x00')
+            params_size = len(params_data)
+
+            # In batch mode each dispatch needs its own params buffer
+            # because wgpuQueueWriteBuffer calls all complete before the
+            # batch command buffer executes on the GPU.
+            if batching:
+                batch_idx = len(self._batch_cleanup) if hasattr(self, '_batch_cleanup') else 0
+                params_name = f"__params_batch_{batch_idx}__"
+            else:
+                params_name = "__params__"
+
+            params_buf = self._get_or_create_buffer(
+                params_name, params_size,
+                BUFFER_USAGE_STORAGE | BUFFER_USAGE_COPY_DST)
+
+            params_bytes = bytes(params_data)
+            lib.wgpuQueueWriteBuffer(
+                self._queue, params_buf, 0,
+                ctypes.c_char_p(params_bytes), params_size
+            )
+
+        # 3. Create bind group (lightweight — not cached)
+        n_bindings = len(buffer_bindings) + (1 if param_fields else 0)
+        BindEntryArray = WGPUBindGroupEntry * n_bindings
+        bind_entries = BindEntryArray()
+
+        for i, bb in enumerate(buffer_bindings):
+            ctypes.memset(ctypes.byref(bind_entries[i]), 0,
+                         ctypes.sizeof(WGPUBindGroupEntry))
+            bind_entries[i].binding = bb.binding
+            bind_entries[i].buffer = gpu_buffers[bb.name]
+            bind_entries[i].offset = 0
+            bind_entries[i].size = gpu_buf_sizes[bb.name]
+
+        if param_fields:
+            idx = len(buffer_bindings)
+            ctypes.memset(ctypes.byref(bind_entries[idx]), 0,
+                         ctypes.sizeof(WGPUBindGroupEntry))
+            bind_entries[idx].binding = len(buffer_bindings)
+            bind_entries[idx].buffer = params_buf
+            bind_entries[idx].offset = 0
+            bind_entries[idx].size = params_size
+
+        bg_desc = WGPUBindGroupDescriptor()
+        bg_desc.nextInChain = None
+        bg_desc.label = WGPUStringView.from_str("")
+        bg_desc.layout = bg_layout
+        bg_desc.entryCount = n_bindings
+        bg_desc.entries = bind_entries
+
+        bind_group = lib.wgpuDeviceCreateBindGroup(
+            self._device, ctypes.byref(bg_desc)
+        )
+
+        # 4. Encode and submit compute pass
+        #    In batch mode: use shared encoder, skip submit.
+        #    In normal mode: create own encoder, submit + readback.
+
+        if batching:
+            encoder = self._batch_encoder
+        else:
+            enc_desc = WGPUCommandEncoderDescriptor()
+            enc_desc.nextInChain = None
+            enc_desc.label = WGPUStringView.from_str("")
+
+            encoder = lib.wgpuDeviceCreateCommandEncoder(
+                self._device, ctypes.byref(enc_desc)
+            )
+
+        pass_desc = WGPUComputePassDescriptor()
+        pass_desc.nextInChain = None
+        pass_desc.label = WGPUStringView.from_str("")
+        if timestamp_writes_ptr is not None:
+            # timestamp_writes_ptr is a ctypes.byref() to a WGPUPassTimestampWrites
+            # We need the raw address as a c_void_p for the struct field
+            pass_desc.timestampWrites = ctypes.cast(timestamp_writes_ptr,
+                                                     ctypes.c_void_p)
+        else:
+            pass_desc.timestampWrites = None
+
+        compute_pass = lib.wgpuCommandEncoderBeginComputePass(
+            encoder, ctypes.byref(pass_desc)
+        )
+        lib.wgpuComputePassEncoderSetPipeline(compute_pass, pipeline)
+        lib.wgpuComputePassEncoderSetBindGroup(compute_pass, 0, bind_group, 0, None)
+
+        gx = grid[0] if len(grid) > 0 else 1
+        gy = grid[1] if len(grid) > 1 else 1
+        gz = grid[2] if len(grid) > 2 else 1
+
+        lib.wgpuComputePassEncoderDispatchWorkgroups(compute_pass, gx, gy, gz)
+        lib.wgpuComputePassEncoderEnd(compute_pass)
+
+        # 5. Copy output buffers to readback buffers
+        #    Buffers in gpu_outputs stay on GPU — no copy or readback.
+        #    In batch mode: skip submit entirely, return GPUBuffer objects.
+        if batching:
+            # In batch mode: return GPUBuffer objects immediately
+            # Accumulate pass/bind_group for later cleanup (after submit)
+            if not hasattr(self, '_batch_cleanup'):
+                self._batch_cleanup = []
+            self._batch_cleanup.append((compute_pass, bind_group))
+            results = {}
+            for bb in buffer_bindings:
+                if bb.name in buffers and bb.access == 'read_write':
+                    if gpu_outputs and bb.name in gpu_outputs:
+                        buf_size = gpu_buf_sizes[bb.name]
+                        elem_type = bb.elem_type
+                        np_dtype = WGSL_TYPE_TO_NUMPY.get(elem_type, np.float32)
+                        results[bb.name] = GPUBuffer(
+                            self, gpu_buffers[bb.name], buf_size, np_dtype,
+                            owned=False)
+            return results
+
+        readback_buffers = {}
+        readback_usage = BUFFER_USAGE_MAP_READ | BUFFER_USAGE_COPY_DST
+        for bb in buffer_bindings:
+            if bb.name in buffers and bb.access == 'read_write':
+                if gpu_outputs and bb.name in gpu_outputs:
+                    continue  # skip — will return GPUBuffer
+                size = gpu_buf_sizes[bb.name]
+                rb_buf = self._get_or_create_buffer(
+                    f"__rb_{bb.name}__", size, readback_usage)
+                readback_buffers[bb.name] = rb_buf
+                lib.wgpuCommandEncoderCopyBufferToBuffer(
+                    encoder, gpu_buffers[bb.name], 0, rb_buf, 0, size
+                )
+
+        # Finish and submit
+        cb_desc = WGPUCommandBufferDescriptor()
+        cb_desc.nextInChain = None
+        cb_desc.label = WGPUStringView.from_str("")
+
+        cmd_buf = lib.wgpuCommandEncoderFinish(encoder, ctypes.byref(cb_desc))
+        cmd_bufs = (ctypes.c_void_p * 1)(cmd_buf)
+
+        lib.wgpuQueueSubmit(
+            self._queue, 1,
+            ctypes.cast(cmd_bufs, ctypes.POINTER(WGPUCommandBuffer))
+        )
+
+        # 6. Read back results
+        results = {}
+        for bb_name, rb_buf in readback_buffers.items():
+            size = gpu_buf_sizes[bb_name]
+
+            map_done = [False]
+            map_status = [0]
+
+            @BufferMapCallback
+            def on_map(status, message, ud1, ud2, _md=map_done, _ms=map_status):
+                _md[0] = True
+                _ms[0] = status
+
+            self._map_cb = on_map
+
+            cb_info = WGPUBufferMapCallbackInfo()
+            cb_info.nextInChain = None
+            cb_info.mode = WGPUCallbackMode.WaitAnyOnly
+            cb_info.callback = on_map
+            cb_info.userdata1 = None
+            cb_info.userdata2 = None
+
+            future = lib.wgpuBufferMapAsync(
+                rb_buf, MAP_MODE_READ, 0, size, cb_info
+            )
+
+            wait_info = WGPUFutureWaitInfo()
+            wait_info.future = future
+            wait_info.completed = 0
+
+            lib.wgpuInstanceWaitAny(
+                self._instance, 1, ctypes.byref(wait_info),
+                ctypes.c_uint64(-1)
+            )
+
+            if map_status[0] != WGPUMapAsyncStatus.Success:
+                raise RuntimeError(f"Buffer map failed for '{bb_name}': status={map_status[0]}")
+
+            data_ptr = lib.wgpuBufferGetConstMappedRange(rb_buf, 0, size)
+            if not data_ptr:
+                raise RuntimeError(f"Failed to get mapped range for '{bb_name}'")
+
+            # Find elem type for this buffer
+            elem_type = 'f32'
+            for bb in buffer_bindings:
+                if bb.name == bb_name:
+                    elem_type = bb.elem_type
+                    break
+            np_dtype = WGSL_TYPE_TO_NUMPY.get(elem_type, np.float32)
+            result_arr = np.ctypeslib.as_array(
+                (ctypes.c_uint8 * size).from_address(data_ptr),
+                shape=(size,)
+            ).copy()  # copy before unmap
+            results[bb_name] = np.frombuffer(result_arr, dtype=np_dtype)
+
+            lib.wgpuBufferUnmap(rb_buf)
+
+        # 7. Return GPU-resident outputs as GPUBuffer objects
+        if gpu_outputs:
+            for bb in buffer_bindings:
+                if bb.name in gpu_outputs and bb.access == 'read_write':
+                    elem_type = bb.elem_type
+                    np_dtype = WGSL_TYPE_TO_NUMPY.get(elem_type, np.float32)
+                    results[bb.name] = GPUBuffer(
+                        self, gpu_buffers[bb.name],
+                        gpu_buf_sizes[bb.name], np_dtype, owned=False)
+
+        # 8. Cleanup (only per-call objects, not cached ones)
+        lib.wgpuComputePassEncoderRelease(compute_pass)
+        lib.wgpuCommandEncoderRelease(encoder)
+        lib.wgpuCommandBufferRelease(cmd_buf)
+        lib.wgpuBindGroupRelease(bind_group)
+
+        return results
+
+    def __del__(self):
+        """Release Dawn resources."""
+        lib = self._lib
+        if lib is None:
+            return
+        try:
+            # Release cached buffers
+            for buf in getattr(self, '_buffer_cache', {}).values():
+                lib.wgpuBufferDestroy(buf)
+                lib.wgpuBufferRelease(buf)
+            # Release cached pipelines
+            for sm, pipe, bgl, pl in getattr(self, '_pipeline_cache', {}).values():
+                lib.wgpuComputePipelineRelease(pipe)
+                lib.wgpuShaderModuleRelease(sm)
+                lib.wgpuBindGroupLayoutRelease(bgl)
+                lib.wgpuPipelineLayoutRelease(pl)
+            if hasattr(self, '_queue') and self._queue:
+                lib.wgpuQueueRelease(self._queue)
+            if hasattr(self, '_device') and self._device:
+                lib.wgpuDeviceRelease(self._device)
+            if hasattr(self, '_adapter') and self._adapter:
+                lib.wgpuAdapterRelease(self._adapter)
+            if hasattr(self, '_instance') and self._instance:
+                lib.wgpuInstanceRelease(self._instance)
+        except Exception:
+            pass
+
+
+def run_triton_kernel_on_webgpu(
+    compiled_kernel,
+    grid: tuple,
+    **kwargs,
+) -> dict:
+    """
+    High-level API: Execute a compiled Triton WebGPU kernel on the GPU via Dawn.
+
+    Args:
+        compiled_kernel: Result of triton.compile() for WebGPU target
+        grid: Tuple of workgroup counts (x,) or (x, y) or (x, y, z)
+        **kwargs: Named arguments matching the kernel signature.
+                  Pointer args should be numpy arrays.
+                  Scalar args should be int/float.
+
+    Returns:
+        Dict mapping output buffer names → numpy arrays with GPU results
+    """
+    from .llvm_to_wgsl import translate_llvm_to_wgsl
+
+    llir = compiled_kernel.asm.get('llir', '')
+    if not llir:
+        raise ValueError("Compiled kernel has no LLVM IR. Was it compiled for WebGPU?")
+
+    metadata = compiled_kernel.metadata
+    sig = {}
+    if hasattr(compiled_kernel, 'signature'):
+        sig = compiled_kernel.signature
+
+    num_warps = metadata.get('num_warps', 4)
+    warp_size = 32
+
+    result = translate_llvm_to_wgsl(llir, sig, num_warps, warp_size)
+
+    buffers_dict = {}
+    scalars_dict = {}
+    for name, val in kwargs.items():
+        if isinstance(val, np.ndarray):
+            buffers_dict[name] = val
+        else:
+            scalars_dict[name] = val
+
+    runner = DawnRunner()
+    return runner.run_kernel(
+        wgsl_code=result.wgsl,
+        buffer_bindings=result.buffer_bindings,
+        param_fields=result.param_fields,
+        workgroup_size=result.workgroup_size,
+        grid=grid,
+        buffers=buffers_dict,
+        scalars=scalars_dict,
+    )
diff --git a/third_party/webgpu/backend/driver.c b/third_party/webgpu/backend/driver.c
new file mode 100644
index 000000000..8a4147404
--- /dev/null
+++ b/third_party/webgpu/backend/driver.c
@@ -0,0 +1,141 @@
+/*
+ * WebGPU Backend Driver - CPython Extension Module
+ *
+ * Provides native functions for WebGPU compute operations via Dawn:
+ *   - load_binary(): Load SPIR-V binary as a WebGPU compute pipeline
+ *   - launch(): Dispatch a compute shader
+ *   - get_device_properties(): Query WebGPU device capabilities
+ *
+ * This module interfaces with Dawn's C API (webgpu.h) for:
+ *   - WGPUDevice management
+ *   - WGPUBuffer allocation and data transfer
+ *   - WGPUComputePipeline creation from SPIR-V
+ *   - WGPUCommandEncoder based dispatch
+ */
+
+#include <Python.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/* ============================================================
+ * Stub implementations
+ *
+ * These are placeholder implementations that will be replaced
+ * with real Dawn WebGPU calls once Dawn is built and linked.
+ * ============================================================ */
+
+static PyObject *webgpu_load_binary(PyObject *self, PyObject *args) {
+    const char *name;
+    const char *binary;
+    Py_ssize_t binary_len;
+    int shared_mem;
+    int device;
+
+    if (!PyArg_ParseTuple(args, "ss#ii", &name, &binary, &binary_len,
+                          &shared_mem, &device)) {
+        return NULL;
+    }
+
+    /* TODO: Create WGPUComputePipeline from SPIR-V binary
+     *
+     * Steps:
+     *   1. Create WGPUShaderModule from SPIR-V bytes
+     *      - WGPUShaderModuleSPIRVDescriptor with code + codeSize
+     *   2. Create WGPUComputePipeline with the shader module
+     *      - Set entry point to kernel name
+     *   3. Return an opaque handle (capsule) to the pipeline
+     */
+
+    PyErr_SetString(PyExc_RuntimeError,
+                    "WebGPU load_binary: Dawn not yet linked. "
+                    "Build Dawn and set DAWN_PATH.");
+    return NULL;
+}
+
+static PyObject *webgpu_launch(PyObject *self, PyObject *args) {
+    /* TODO: Dispatch compute shader
+     *
+     * Parameters:
+     *   gridX, gridY, gridZ - workgroup dispatch dimensions
+     *   stream - command queue (WGPUQueue)
+     *   function - compute pipeline handle
+     *   kernel_args - tuple of kernel arguments
+     *
+     * Steps:
+     *   1. Create WGPUCommandEncoder
+     *   2. Begin compute pass
+     *   3. Set pipeline
+     *   4. Create bind group with argument buffers
+     *   5. Set bind group
+     *   6. DispatchWorkgroups(gridX, gridY, gridZ)
+     *   7. End compute pass
+     *   8. Submit command buffer
+     *   9. Wait for completion
+     */
+
+    PyErr_SetString(PyExc_RuntimeError,
+                    "WebGPU launch: Dawn not yet linked.");
+    return NULL;
+}
+
+static PyObject *webgpu_get_device_properties(PyObject *self, PyObject *args) {
+    int device_id;
+    if (!PyArg_ParseTuple(args, "i", &device_id)) {
+        return NULL;
+    }
+
+    /* TODO: Query actual device limits from Dawn
+     *
+     * Use wgpuDeviceGetLimits() to get:
+     *   - maxComputeWorkgroupSizeX/Y/Z
+     *   - maxComputeWorkgroupsPerDimension
+     *   - maxStorageBufferBindingSize
+     *   - maxBufferSize
+     *   - maxComputeInvocationsPerWorkgroup
+     *   - minSubgroupSize / maxSubgroupSize
+     */
+
+    PyObject *props = PyDict_New();
+    if (!props)
+        return NULL;
+
+    PyDict_SetItemString(props, "max_shared_mem",
+                         PyLong_FromLong(16384));
+    PyDict_SetItemString(props, "max_work_group_size",
+                         PyLong_FromLong(256));
+    PyDict_SetItemString(props, "subgroup_size",
+                         PyLong_FromLong(32));
+    PyDict_SetItemString(props, "max_compute_work_group_count_x",
+                         PyLong_FromLong(65535));
+    PyDict_SetItemString(props, "max_compute_work_group_count_y",
+                         PyLong_FromLong(65535));
+    PyDict_SetItemString(props, "max_compute_work_group_count_z",
+                         PyLong_FromLong(65535));
+
+    return props;
+}
+
+/* Module method table */
+static PyMethodDef WebGPUMethods[] = {
+    {"load_binary", webgpu_load_binary, METH_VARARGS,
+     "Load a SPIR-V binary as a WebGPU compute pipeline."},
+    {"launch", webgpu_launch, METH_VARARGS,
+     "Dispatch a WebGPU compute shader."},
+    {"get_device_properties", webgpu_get_device_properties, METH_VARARGS,
+     "Get WebGPU device properties."},
+    {NULL, NULL, 0, NULL}
+};
+
+/* Module definition */
+static struct PyModuleDef webgpu_utils_module = {
+    PyModuleDef_HEAD_INIT,
+    "webgpu_utils",
+    "WebGPU driver utilities for Triton (Dawn backend)",
+    -1,
+    WebGPUMethods
+};
+
+PyMODINIT_FUNC PyInit_webgpu_utils(void) {
+    return PyModule_Create(&webgpu_utils_module);
+}
diff --git a/third_party/webgpu/backend/driver.py b/third_party/webgpu/backend/driver.py
new file mode 100644
index 000000000..40f6b38b9
--- /dev/null
+++ b/third_party/webgpu/backend/driver.py
@@ -0,0 +1,215 @@
+"""
+WebGPU Backend Driver for Triton
+=================================
+
+Runtime driver that interfaces with Dawn's WebGPU native implementation.
+Handles device management, buffer operations, and kernel dispatch.
+
+Dawn provides a native C API (webgpu.h) for WebGPU compute operations
+and supports D3D12 (Windows), Vulkan (Linux), and Metal (macOS) natively,
+consuming WGSL shaders directly through its Tint compiler.
+"""
+
+import os
+import functools
+import ctypes
+from pathlib import Path
+
+from triton.backends.compiler import GPUTarget
+from triton.backends.driver import DriverBase
+
+dirname = os.path.dirname(os.path.realpath(__file__))
+
+
+def ty_to_cpp(ty):
+    """Map Triton type strings to C++ types for the WebGPU backend."""
+    if ty[0] == '*':
+        return "uint64_t"  # Device pointer (WGPUBuffer offset)
+    return {
+        "i1": "int8_t",
+        "i8": "int8_t",
+        "i16": "int16_t",
+        "i32": "int32_t",
+        "i64": "int64_t",
+        "u1": "uint8_t",
+        "u8": "uint8_t",
+        "u16": "uint16_t",
+        "u32": "uint32_t",
+        "u64": "uint64_t",
+        "fp16": "float",
+        "bf16": "float",
+        "fp32": "float",
+        "f32": "float",
+        "fp64": "double",
+    }[ty]
+
+
+class WebGPUUtils:
+    """
+    Utility class that loads the Dawn WebGPU shared library and
+    compiles the driver.c CPython extension for kernel launch.
+    """
+
+    def __new__(cls):
+        if not hasattr(cls, "instance"):
+            cls.instance = super(WebGPUUtils, cls).__new__(cls)
+        return cls.instance
+
+    def __init__(self):
+        self._initialized = getattr(self, '_initialized', False)
+        if self._initialized:
+            return
+        self._initialized = True
+
+        # Find Dawn library
+        self.dawn_path = self._find_dawn()
+        if self.dawn_path:
+            self._init_from_dawn()
+        else:
+            self._init_stub()
+
+    def _find_dawn(self):
+        """Locate the Dawn WebGPU shared library."""
+        # Check environment variable first
+        dawn_path = os.environ.get("DAWN_PATH")
+        if dawn_path and os.path.exists(dawn_path):
+            return dawn_path
+
+        # Check common locations
+        candidates = [
+            os.path.join(dirname, "lib", "dawn.dll"),
+            os.path.join(dirname, "lib", "libdawn.so"),
+            os.path.join(dirname, "lib", "libdawn.dylib"),
+        ]
+        for path in candidates:
+            if os.path.exists(path):
+                return path
+
+        return None
+
+    def _init_from_dawn(self):
+        """Initialize WebGPU functions from Dawn library."""
+        # TODO: Load Dawn shared library and set up function pointers
+        self._init_stub()
+
+    def _init_stub(self):
+        """Initialize with stub functions for development/testing."""
+        self.load_binary = self._stub_load_binary
+        self.launch = self._stub_launch
+        self.get_device_properties = self._stub_get_device_properties
+
+    @staticmethod
+    def _stub_load_binary(name, binary, shared_mem, device):
+        """Stub: Load a SPIR-V binary as a WebGPU compute pipeline."""
+        raise RuntimeError(
+            "WebGPU backend: Dawn library not found. "
+            "Set DAWN_PATH environment variable to the Dawn shared library path, "
+            "or build Dawn from source at third_party/webgpu/dawn/"
+        )
+
+    @staticmethod
+    def _stub_launch(*args, **kwargs):
+        """Stub: Launch a WebGPU compute shader."""
+        raise RuntimeError(
+            "WebGPU backend: Dawn library not available for kernel launch."
+        )
+
+    @staticmethod
+    def _stub_get_device_properties(device_id):
+        """Return basic device properties for the WebGPU device."""
+        return {
+            "max_shared_mem": 16384,
+            "max_work_group_size": 256,
+            "max_compute_work_group_count_x": 65535,
+            "max_compute_work_group_count_y": 65535,
+            "max_compute_work_group_count_z": 65535,
+            "subgroup_size": 32,
+        }
+
+
+class WebGPULauncher:
+    """Launcher for WebGPU compute kernels."""
+
+    def __init__(self, src, metadata):
+        self.metadata = metadata
+        self.launch_fn = None
+
+    def __call__(self, gridX, gridY, gridZ, stream, function,
+                 kernel_metadata, launch_metadata,
+                 launch_enter_hook, launch_exit_hook, *args):
+        # TODO: Implement WebGPU kernel dispatch
+        #  1. Create compute pass encoder
+        #  2. Set pipeline (function)
+        #  3. Set bind groups (args)
+        #  4. Dispatch(gridX, gridY, gridZ)
+        #  5. Submit command buffer
+        raise RuntimeError(
+            "WebGPU kernel launch not yet implemented. "
+            "Dawn runtime integration is in progress."
+        )
+
+
+class WebGPUDriver(DriverBase):
+    """
+    WebGPU Driver for Triton.
+
+    Uses Dawn's native WebGPU implementation for GPU compute.
+    Supports Vulkan, D3D12, and Metal backends through Dawn.
+    """
+
+    def __init__(self):
+        self.utils = WebGPUUtils()
+        self.launcher_cls = WebGPULauncher
+        super().__init__()
+
+    @staticmethod
+    def is_active():
+        """Check if WebGPU/Dawn is available."""
+        # Check if Dawn library is findable
+        dawn_path = os.environ.get("DAWN_PATH")
+        if dawn_path and os.path.exists(dawn_path):
+            return True
+
+        # Check in-tree build location
+        here = os.path.dirname(os.path.realpath(__file__))
+        triton_root = os.path.normpath(os.path.join(here, "..", "..", "..", ".."))
+        candidates = [
+            os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "webgpu_dawn.dll"),
+            os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "libwebgpu_dawn.so"),
+            os.path.join(triton_root, "third_party", "webgpu", "dawn", "build", "libwebgpu_dawn.dylib"),
+        ]
+
+        # Check for Dawn in the backend directory
+        backend_dir = os.path.dirname(os.path.realpath(__file__))
+        candidates.extend([
+            os.path.join(backend_dir, "lib", "webgpu_dawn.dll"),
+            os.path.join(backend_dir, "lib", "libwebgpu_dawn.so"),
+        ])
+
+        return any(os.path.exists(p) for p in candidates)
+
+    def get_current_target(self):
+        """Return the GPUTarget for WebGPU."""
+        # Default WebGPU target:
+        #   backend="webgpu", arch=0 (generic), warp_size=32
+        return GPUTarget("webgpu", 0, 32)
+
+    def get_active_torch_device(self):
+        """
+        Return the active torch device for data transfer.
+        WebGPU doesn't have native torch device support,
+        so we use CPU as the host device for data staging.
+        """
+        import torch
+        return torch.device("cpu")
+
+    def get_device_interface(self):
+        """Return None since there's no torch.webgpu module."""
+        return None
+
+    def map_python_to_cpp_type(self, ty: str) -> str:
+        return ty_to_cpp(ty)
+
+    def get_benchmarker(self):
+        from triton.testing import do_bench
+        return do_bench
diff --git a/third_party/webgpu/backend/llvm_to_wgsl.py b/third_party/webgpu/backend/llvm_to_wgsl.py
new file mode 100644
index 000000000..58f579eb0
--- /dev/null
+++ b/third_party/webgpu/backend/llvm_to_wgsl.py
@@ -0,0 +1,2565 @@
+"""
+LLVM IR → WGSL Translator for Triton WebGPU Backend
+=====================================================
+
+Translates the LLVM IR produced by Triton's WebGPU compilation pipeline
+into WGSL (WebGPU Shading Language) compute shaders that can be executed
+via wgpu-py on any GPU supporting Vulkan, Metal, or DX12.
+
+The LLVM IR from Triton follows predictable patterns:
+  - Single function with SPIR-V builtins for thread/block IDs
+  - addrspace(1) pointers for global memory (storage buffers)
+  - addrspace(3) for shared memory (workgroup storage)
+  - Standard arithmetic, bitwise, comparison, and memory ops
+
+This translator handles straight-line code (single basic block) and
+simple control flow (loops via back-edges, if/else via branches).
+"""
+
+import re
+import struct
+from dataclasses import dataclass, field
+from typing import List, Dict, Tuple, Optional
+
+
+# ---------------------------------------------------------------------------
+# LLVM intrinsic → WGSL function mapping
+# ---------------------------------------------------------------------------
+
+LLVM_UNARY_INTRINSICS = {
+    'llvm.fabs': 'abs',
+    'llvm.exp': 'exp',
+    'llvm.exp2': 'exp2',
+    'llvm.log': 'log',
+    'llvm.log2': 'log2',
+    'llvm.sqrt': 'sqrt',
+    'llvm.sin': 'sin',
+    'llvm.cos': 'cos',
+    'llvm.ceil': 'ceil',
+    'llvm.floor': 'floor',
+    'llvm.round': 'round',
+    'llvm.roundeven': 'round',
+    'llvm.trunc': 'trunc',
+    'llvm.rint': 'round',
+    'llvm.nearbyint': 'round',
+    'llvm.fabs': 'abs',
+}
+
+LLVM_BINARY_INTRINSICS = {
+    'llvm.maxnum': 'max',
+    'llvm.minnum': 'min',
+    'llvm.smin': 'min',
+    'llvm.smax': 'max',
+    'llvm.umin': 'min',
+    'llvm.umax': 'max',
+    'llvm.pow': 'pow',
+    'llvm.copysign': 'sign',
+}
+
+LLVM_TERNARY_INTRINSICS = {
+    'llvm.fma': 'fma',
+    'llvm.fmuladd': 'fma',
+}
+
+
+# ---------------------------------------------------------------------------
+# Type mapping
+# ---------------------------------------------------------------------------
+
+TRITON_TYPE_TO_WGSL = {
+    'fp16': 'f16',
+    'fp32': 'f32',
+    'f32': 'f32',
+    'fp64': 'f64',
+    'i8': 'i32',  # WGSL has no i8; promote
+    'i16': 'i32',  # WGSL has no i16; promote
+    'i32': 'i32',
+    'i64': 'i32',  # WGSL has no i64; truncate (sufficient for indices)
+    'u8': 'u32',
+    'u16': 'u32',
+    'u32': 'u32',
+    'u64': 'u32',
+}
+
+LLVM_TYPE_TO_WGSL = {
+    'float': 'f32',
+    'double': 'f64',
+    'half': 'f16',
+    'i32': 'i32',
+    'i64': 'i32',
+    'i16': 'i32',
+    'i8': 'i32',
+    'i1': 'bool',
+}
+
+LLVM_TYPE_TO_BYTES = {
+    'float': 4,
+    'double': 8,
+    'half': 2,
+    'i32': 4,
+    'i64': 8,
+    'i16': 2,
+    'i8': 1,
+    'i1': 1,
+}
+
+
+def triton_ptr_elem_type(sig_type: str) -> str:
+    """Extract WGSL element type from Triton pointer type like '*fp32'."""
+    assert sig_type.startswith('*'), f"Not a pointer type: {sig_type}"
+    elem = sig_type[1:]
+    return TRITON_TYPE_TO_WGSL.get(elem, 'f32')
+
+
+def triton_scalar_wgsl_type(sig_type: str) -> str:
+    """Convert Triton scalar type like 'i32' to WGSL type."""
+    return TRITON_TYPE_TO_WGSL.get(sig_type, 'i32')
+
+
+# ---------------------------------------------------------------------------
+# Data structures
+# ---------------------------------------------------------------------------
+
+@dataclass
+class BufferBinding:
+    """A storage buffer binding in the WGSL shader."""
+    binding: int
+    name: str
+    elem_type: str  # WGSL element type: f32, i32, etc.
+    access: str  # 'read' or 'read_write'
+
+
+@dataclass
+class ParamField:
+    """A scalar parameter field in the uniform/params struct."""
+    name: str
+    wgsl_type: str
+    byte_size: int
+
+
+@dataclass
+class GepInfo:
+    """Tracking info for a GEP (pointer) value."""
+    buffer_arg_idx: int  # Which function arg (buffer binding index)
+    offset_expr: str  # WGSL expression for the element offset
+    is_uniform: bool = True  # Whether offset is the same across all threads
+
+
+@dataclass
+class IRBasicBlock:
+    """A parsed LLVM IR basic block."""
+    label: str
+    phis: List[str]         # phi instructions
+    body: List[str]         # non-phi, non-terminator instructions
+    terminator: str         # branch or ret instruction
+
+
+@dataclass
+class TranslationResult:
+    """Result of LLVM IR → WGSL translation."""
+    wgsl: str
+    workgroup_size: int
+    buffer_bindings: List[BufferBinding]
+    param_fields: List[ParamField]
+    kernel_name: str
+
+
+# ---------------------------------------------------------------------------
+# LLVM IR Parser & WGSL Generator
+# ---------------------------------------------------------------------------
+
+class LLVMToWGSL:
+    """
+    Translates LLVM IR text from Triton's WebGPU backend into WGSL.
+
+    Handles single-BB (straight-line) kernels and simple multi-BB kernels
+    with structured control flow.
+    """
+
+    def __init__(self, llir: str, signature: dict,
+                 num_warps: int = 4, warp_size: int = 32,
+                 use_native_subgroups: bool = False):
+        """
+        Args:
+            llir: LLVM IR string from Triton compilation
+            signature: Triton parameter signature (excluding constexprs).
+                       e.g. {'x_ptr': '*fp32', 'n_elements': 'i32'}
+            num_warps: Warps per workgroup
+            warp_size: Threads per warp
+            use_native_subgroups: Use native WGSL subgroupShuffleXor instead
+                                  of shared-memory emulation (requires the
+                                  WebGPU Subgroups feature on the adapter)
+        """
+        self.llir = llir
+        self.signature = signature
+        self.num_warps = num_warps
+        self.warp_size = warp_size
+        self.workgroup_size = num_warps * warp_size
+        self.use_native_subgroups = use_native_subgroups
+
+        # State
+        self.kernel_name = ""
+        self.func_args: List[Tuple[str, str]] = []  # (llvm_type, arg_name)
+        self.values: Dict[str, Tuple[str, str]] = {}  # ssa_name → (wgsl_expr, wgsl_type)
+        self.gep_info: Dict[str, GepInfo] = {}  # ssa_name → GEP provenance
+        self.stored_buffers: set = set()  # Set of buffer arg indices that are written to
+        self.atomic_buffers: set = set()  # Set of buffer arg indices with atomic operations
+        self.needs_subgroups: bool = False  # Whether WGSL needs 'enable subgroups;'
+        self.needs_f16: bool = False  # Whether WGSL needs 'enable f16;'
+        self.needs_shuffle_scratch: bool = False  # Whether we need shared mem for shuffle emulation
+        self.needs_smem: bool = False  # Whether we need workgroup shared memory (addrspace(3))
+        self.smem_bytes: int = 0  # Total shared memory size in bytes (auto-detected)
+        self.smem_gep_info: Dict[str, str] = {}  # SSA → byte offset expression for shared mem GEPs
+        self.masked_values: Dict[str, str] = {}  # SSA → condition for select w/ undef
+        self.struct_fields: Dict[str, Dict[int, str]] = {}  # SSA → {field_idx: value_expr} for struct agg
+        self.vector_elements: Dict[str, Dict[int, Tuple[str, str]]] = {}  # SSA → {idx: (expr, type)} for vectors
+        self.uniform_values: set = set()  # SSA names that are thread-uniform (same across all threads in WG)
+
+        # Binding info
+        self.buffer_bindings: List[BufferBinding] = []
+        self.param_fields: List[ParamField] = []
+        self.sig_names: List[str] = []  # Ordered param names from signature
+        self.sig_types: List[str] = []  # Ordered param types from signature
+
+        # Separate pointer vs scalar args
+        self.ptr_arg_indices: List[int] = []  # func_arg indices that are pointers
+        self.scalar_arg_indices: List[int] = []  # func_arg indices that are scalars
+        # Map: func_arg_index → binding_index (for pointers)
+        self.arg_to_binding: Dict[int, int] = {}
+        # Map: func_arg_index → param field name (for scalars)
+        self.arg_to_param: Dict[int, str] = {}
+
+    def translate(self) -> TranslationResult:
+        """Run the full translation pipeline."""
+        self._parse_signature()
+        self._parse_function_header()
+        self._classify_args()
+        self._prescan_stores()
+        self._prescan_smem_size()
+        self._build_bindings()
+        self._detect_f16()
+
+        body_lines = self._extract_body()
+        wgsl_stmts = self._translate_instructions(body_lines)
+        wgsl_code = self._emit_wgsl(wgsl_stmts)
+
+        return TranslationResult(
+            wgsl=wgsl_code,
+            workgroup_size=self.workgroup_size,
+            buffer_bindings=self.buffer_bindings,
+            param_fields=self.param_fields,
+            kernel_name=self.kernel_name,
+        )
+
+    # -------------------------------------------------------------------
+    # Step 1: Parse Triton signature
+    # -------------------------------------------------------------------
+
+    def _parse_signature(self):
+        """Extract ordered parameter names and types from Triton signature.
+
+        If no signature was provided, auto-infer from the LLVM IR function
+        header: ``ptr addrspace(1)`` → ``*fp32`` (default), scalars keep
+        their LLVM type.
+        """
+        if self.signature:
+            for name, ty in self.signature.items():
+                if ty == 'constexpr':
+                    continue
+                self.sig_names.append(name)
+                self.sig_types.append(ty)
+        # If signature is empty, we defer to _classify_args which
+        # handles the no-signature case.
+
+    # -------------------------------------------------------------------
+    # Step 2: Parse LLVM function header
+    # -------------------------------------------------------------------
+
+    def _parse_function_header(self):
+        """Extract kernel name and function arguments from LLVM IR."""
+        # Match: define void @kernel_name(type %0, type %1, ...)
+        # Use .*? instead of [^)]* to handle nested parens in addrspace(1)
+        pattern = r'define\s+void\s+@(\w+)\((.+?)\)\s*\{'
+        m = re.search(pattern, self.llir, re.DOTALL)
+        if not m:
+            raise ValueError("Could not find function definition in LLVM IR")
+
+        self.kernel_name = m.group(1)
+        args_str = m.group(2)
+
+        # Parse each argument
+        # Examples: "ptr addrspace(1) %0", "i32 %3"
+        arg_pattern = r'((?:ptr\s+addrspace\(\d+\))|(?:i\d+)|(?:float|double|half))\s+(%\d+)'
+        for am in re.finditer(arg_pattern, args_str):
+            llvm_type = am.group(1).strip()
+            arg_name = am.group(2)
+            self.func_args.append((llvm_type, arg_name))
+
+    # -------------------------------------------------------------------
+    # Step 3: Classify args as pointers or scalars
+    # -------------------------------------------------------------------
+
+    def _classify_args(self):
+        """Map LLVM function args to pointer (buffer) or scalar (param) args."""
+        sig_idx = 0
+        for i, (llvm_type, _arg_name) in enumerate(self.func_args):
+            is_ptr = 'addrspace' in llvm_type
+            if sig_idx < len(self.sig_types):
+                sig_type = self.sig_types[sig_idx]
+                if is_ptr and sig_type.startswith('*'):
+                    self.ptr_arg_indices.append(i)
+                    sig_idx += 1
+                elif not is_ptr and not sig_type.startswith('*'):
+                    self.scalar_arg_indices.append(i)
+                    sig_idx += 1
+                else:
+                    # Extra internal arg (doesn't match signature)
+                    if is_ptr:
+                        self.ptr_arg_indices.append(i)
+                    else:
+                        self.scalar_arg_indices.append(i)
+            else:
+                # Extra args beyond the signature (internal Triton args)
+                if is_ptr:
+                    self.ptr_arg_indices.append(i)
+                elif not is_ptr:
+                    self.scalar_arg_indices.append(i)
+
+        # Auto-populate sig_names/sig_types when signature was empty
+        if not self.sig_names:
+            for i, (llvm_type, arg_name) in enumerate(self.func_args):
+                is_ptr = 'addrspace' in llvm_type
+                name = f'arg{i}'
+                if is_ptr:
+                    self.sig_names.append(name)
+                    self.sig_types.append('*fp32')  # default
+                else:
+                    wgsl_t = LLVM_TYPE_TO_WGSL.get(llvm_type, 'i32')
+                    triton_t = 'i32' if wgsl_t in ('i32', 'u32') else 'fp32'
+                    self.sig_names.append(name)
+                    self.sig_types.append(triton_t)
+
+        # Mark scalar function arguments as uniform (same for all threads)
+        for i in self.scalar_arg_indices:
+            self.uniform_values.add(f'%{i}')
+
+    # -------------------------------------------------------------------
+    # Step 4: Pre-scan for stores to determine buffer access modes
+    # -------------------------------------------------------------------
+
+    def _prescan_stores(self):
+        """Scan the LLVM IR body to find which buffers are stored to."""
+        # Find all store instructions and trace which buffer arg they write to
+        # Pattern: store T val, ptr addrspace(1) %ptr_ssa
+        # val can be %ssa, literal, hex, or undef
+        store_pattern = r'store\s+\S+\s+(?:%\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|true|false).*ptr\s+addrspace\(1\)\s+(%\d+)'
+        gep_pattern = r'(%\d+)\s*=\s*getelementptr\s+(?:inbounds\s+)?\S+,\s*ptr\s+addrspace\(1\)\s+(%\d+|\d+)'
+
+        # First, build a map of GEP destination → base argument
+        gep_to_base = {}
+        for m in re.finditer(gep_pattern, self.llir):
+            gep_dst = m.group(1)
+            base = m.group(2)
+            # If base is a function arg like %0, %1, etc.
+            if base.startswith('%'):
+                try:
+                    arg_idx = int(base[1:])
+                    if arg_idx < len(self.func_args):
+                        gep_to_base[gep_dst] = arg_idx
+                except ValueError:
+                    pass
+
+        # Propagate chained GEPs: if base is a known GEP, inherit its arg
+        changed = True
+        while changed:
+            changed = False
+            for m in re.finditer(gep_pattern, self.llir):
+                gep_dst = m.group(1)
+                base = m.group(2)
+                if gep_dst not in gep_to_base and base in gep_to_base:
+                    gep_to_base[gep_dst] = gep_to_base[base]
+                    changed = True
+
+        # Then check which GEP destinations are used in stores
+        for m in re.finditer(store_pattern, self.llir):
+            ptr_ssa = m.group(1)
+            if ptr_ssa in gep_to_base:
+                self.stored_buffers.add(gep_to_base[ptr_ssa])
+            else:
+                # Check if ptr_ssa is a direct function arg (scalar store to buffer)
+                if ptr_ssa.startswith('%'):
+                    try:
+                        arg_idx = int(ptr_ssa[1:])
+                        if arg_idx < len(self.func_args):
+                            llvm_type = self.func_args[arg_idx][0]
+                            if 'addrspace' in llvm_type:
+                                self.stored_buffers.add(arg_idx)
+                    except ValueError:
+                        pass
+
+        # Also scan for atomicrmw instructions and mark those buffers
+        # Pattern: atomicrmw OP ptr addrspace(1) %ptr_ssa, TYPE %val ...
+        atomic_pattern = r'atomicrmw\s+\w+\s+ptr\s+addrspace\(1\)\s+(%\d+)'
+        for m in re.finditer(atomic_pattern, self.llir):
+            ptr_ssa = m.group(1)
+            if ptr_ssa in gep_to_base:
+                arg_idx = gep_to_base[ptr_ssa]
+                self.atomic_buffers.add(arg_idx)
+                self.stored_buffers.add(arg_idx)  # atomics are read_write
+
+    def _prescan_smem_size(self):
+        """Scan LLVM IR to determine the required shared memory size in bytes.
+
+        Detects patterns like:
+          - getelementptr i8, ptr addrspace(3) @global_smem, i32 <offset>
+          - getelementptr i8, ptr addrspace(3) getelementptr (i8, ptr addrspace(3) @global_smem, i64 <base>), i32 <offset>
+          - store ... ptr addrspace(3) <ptr>, with byte offset tracking
+        """
+        max_static_offset = 0
+        max_dynamic_slots = 0
+
+        # Find all constant offsets into @global_smem (including nested GEP bases)
+        # Pattern 1: direct GEP with constant offset
+        for m in re.finditer(
+            r'getelementptr\s+(?:inbounds\s+)?i8,\s*ptr\s+addrspace\(3\)\s+@global_smem,\s*i(?:32|64)\s+(\d+)',
+            self.llir
+        ):
+            off = int(m.group(1))
+            if off > max_static_offset:
+                max_static_offset = off
+
+        # Pattern 2: nested GEP base offset (i64 constant) in constant expression
+        for m in re.finditer(
+            r'getelementptr\s*\(i8,\s*ptr\s+addrspace\(3\)\s+@global_smem,\s*i64\s+(\d+)\)',
+            self.llir
+        ):
+            base_off = int(m.group(1))
+            if base_off > max_static_offset:
+                max_static_offset = base_off
+
+        # The max offset tells us how far into smem code accesses.
+        # Add a margin of 1024 bytes for the data stored at the highest offset.
+        if max_static_offset > 0:
+            self.smem_bytes = max_static_offset + 1024
+        elif self.needs_smem:
+            # Fallback: num_warps * 4 bytes (for simple reductions)
+            self.smem_bytes = self.num_warps * 4
+        else:
+            self.smem_bytes = 0
+
+    # -------------------------------------------------------------------
+    # Step 5: Build buffer bindings and param struct
+    # -------------------------------------------------------------------
+
+    def _build_bindings(self):
+        """Create WGSL binding declarations."""
+        binding_idx = 0
+
+        # Detect which LLVM args are actually used in the IR body.
+        # Internal (extra) pointer args that are never referenced can be
+        # safely omitted; binding them with dummy buffers can cause D3D12
+        # issues on some drivers.
+        used_args: set = set()
+        # Extract just the function body (everything after the opening '{')
+        body_start = self.llir.find('{')
+        ir_body = self.llir[body_start:] if body_start >= 0 else self.llir
+        for arg_idx, (_llvm_type, _arg_name) in enumerate(self.func_args):
+            pattern = rf'(?<!\d)%{arg_idx}(?!\d)'
+            if re.search(pattern, ir_body):
+                used_args.add(arg_idx)
+
+        # Create buffer bindings for pointer args
+        for i, arg_idx in enumerate(self.ptr_arg_indices):
+            # Determine element type and name
+            if i < len(self.sig_names) and self.sig_types[i].startswith('*'):
+                name = self.sig_names[i]
+                elem_type = triton_ptr_elem_type(self.sig_types[i])
+            else:
+                name = f"_internal_{arg_idx}"
+                elem_type = 'f32'
+                # Skip unused internal args
+                if arg_idx not in used_args:
+                    continue
+
+            access = 'read_write' if arg_idx in self.stored_buffers else 'read'
+
+            self.buffer_bindings.append(BufferBinding(
+                binding=binding_idx,
+                name=name,
+                elem_type=elem_type,
+                access=access,
+            ))
+            self.arg_to_binding[arg_idx] = binding_idx
+            binding_idx += 1
+
+        # Create param fields for scalar args
+        param_binding_idx = binding_idx
+        for i, arg_idx in enumerate(self.scalar_arg_indices):
+            # Find the matching signature entry
+            scalar_count = 0
+            for si, st in enumerate(self.sig_types):
+                if not st.startswith('*'):
+                    if scalar_count == i:
+                        name = self.sig_names[si]
+                        wgsl_type = triton_scalar_wgsl_type(st)
+                        self.param_fields.append(ParamField(
+                            name=name, wgsl_type=wgsl_type,
+                            byte_size=4,  # i32 or f32
+                        ))
+                        self.arg_to_param[arg_idx] = name
+                        break
+                    scalar_count += 1
+
+    # -------------------------------------------------------------------
+    # Step 5b: Detect f16 usage
+    # -------------------------------------------------------------------
+
+    def _detect_f16(self):
+        """Set needs_f16 if any buffer uses f16 or LLVM IR contains half ops."""
+        for bb in self.buffer_bindings:
+            if bb.elem_type == 'f16':
+                self.needs_f16 = True
+                return
+        # Also scan LLVM IR for half type usage
+        if re.search(r'\bhalf\b', self.llir):
+            self.needs_f16 = True
+
+    # -------------------------------------------------------------------
+    # Step 6: Extract function body
+    # -------------------------------------------------------------------
+
+    def _extract_body(self) -> List[str]:
+        """Extract instruction lines from the function body."""
+        # Find the function body between { and }
+        # Use .*? for args to handle nested parens like addrspace(1)
+        func_match = re.search(
+            r'define\s+void\s+@\w+\(.*?\)\s*\{(.*?)\n\}',
+            self.llir, re.DOTALL
+        )
+        if not func_match:
+            raise ValueError("Could not extract function body")
+
+        body = func_match.group(1)
+        lines = []
+        for line in body.strip().split('\n'):
+            line = line.strip()
+            if not line or line.startswith(';'):
+                continue
+            # Remove inline comments
+            if ';' in line:
+                line = line[:line.index(';')].strip()
+            if line:
+                lines.append(line)
+        return lines
+
+    # -------------------------------------------------------------------
+    # Step 7: Parse basic blocks and translate instructions
+    # -------------------------------------------------------------------
+
+    def _parse_basic_blocks(self, lines: List[str]) -> List[IRBasicBlock]:
+        """Parse LLVM IR lines into basic blocks."""
+        blocks = []
+        current_label = 'entry'
+        current_phis = []
+        current_body = []
+        current_terminator = None
+
+        for line in lines:
+            # Label definition: "9:" or "14:  ..." (already stripped of comments)
+            label_match = re.match(r'^(\d+):$', line)
+            if label_match:
+                # Save previous block
+                if current_body or current_phis or current_terminator:
+                    blocks.append(IRBasicBlock(
+                        label=current_label,
+                        phis=current_phis,
+                        body=current_body,
+                        terminator=current_terminator or '',
+                    ))
+                current_label = label_match.group(1)
+                current_phis = []
+                current_body = []
+                current_terminator = None
+                continue
+
+            # Terminator instructions
+            if line == 'ret void' or line.startswith('br '):
+                current_terminator = line
+                continue
+
+            # Phi instruction
+            if '= phi ' in line:
+                current_phis.append(line)
+                continue
+
+            current_body.append(line)
+
+        # Save last block
+        if current_body or current_phis or current_terminator:
+            blocks.append(IRBasicBlock(
+                label=current_label,
+                phis=current_phis,
+                body=current_body,
+                terminator=current_terminator or 'ret void',
+            ))
+
+        return blocks
+
+    def _translate_instructions(self, lines: List[str]) -> List[str]:
+        """Translate LLVM IR instructions to WGSL statements."""
+        blocks = self._parse_basic_blocks(lines)
+
+        if len(blocks) <= 1:
+            # Single basic block — existing fast path
+            stmts = []
+            for line in lines:
+                result = self._translate_one(line)
+                if result:
+                    stmts.append(result)
+            return stmts
+
+        # Multi-basic-block: reconstruct structured control flow
+        return self._translate_multi_block(blocks)
+
+    def _translate_multi_block(self, blocks: List['IRBasicBlock']) -> List[str]:
+        """Translate multi-basic-block LLVM IR into structured WGSL."""
+        stmts = []
+        block_map = {b.label: b for b in blocks}
+        visited = set()
+
+        def find_successors(block):
+            """Return the list of successor labels from the terminator."""
+            t = block.terminator
+            if t == 'ret void' or not t:
+                return []
+            m = re.match(r'br i1 %\d+, label %(\w+), label %(\w+)', t)
+            if m:
+                return [m.group(1), m.group(2)]
+            m = re.match(r'br label %(\w+)', t)
+            if m:
+                return [m.group(1)]
+            return []
+
+        def find_merge_point(true_label, false_label):
+            """Find the merge point of a diamond pattern via BFS."""
+            # Follow successors from each branch until paths converge
+            reachable_from_true = set()
+            queue = [true_label]
+            while queue:
+                lbl = queue.pop(0)
+                if lbl in reachable_from_true:
+                    continue
+                reachable_from_true.add(lbl)
+                b = block_map.get(lbl)
+                if b:
+                    for s in find_successors(b):
+                        queue.append(s)
+
+            # Now search from false_label for first block also in true's reachable set
+            queue = [false_label]
+            seen = set()
+            while queue:
+                lbl = queue.pop(0)
+                if lbl in seen:
+                    continue
+                seen.add(lbl)
+                if lbl in reachable_from_true and lbl != true_label and lbl != false_label:
+                    return lbl
+                # Also check if true_label IS the merge point (else-then pattern)
+                b = block_map.get(lbl)
+                if b:
+                    for s in find_successors(b):
+                        if s in reachable_from_true:
+                            return s
+                        queue.append(s)
+
+            # If false block jumps directly to true_label, true_label is the merge
+            if true_label in reachable_from_true:
+                return true_label
+            return None
+
+        def resolve_phi_value(phi_line, from_label):
+            """Extract the value from a phi instruction for a given predecessor label."""
+            # phi type [val1, %label1], [val2, %label2], ...
+            pairs = re.findall(r'\[\s*(%\d+|\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|zeroinitializer),\s*%(\w+)\s*\]', phi_line)
+            for val, lbl in pairs:
+                if lbl == from_label:
+                    return val
+            return None
+
+        def phi_dst_and_type(phi_line):
+            """Extract destination SSA and type from a phi instruction."""
+            m = re.match(r'(%\d+)\s*=\s*phi\s+(\S+)', phi_line)
+            if m:
+                return m.group(1), m.group(2)
+            return None, None
+
+        def process_block_body(block, into_stmts):
+            """Translate body instructions of a block, appending to stmts."""
+            for line in block.body:
+                result = self._translate_one(line)
+                if result:
+                    into_stmts.append(result)
+
+        def collect_predecessors(label):
+            """Return labels of blocks that branch to the given label."""
+            preds = []
+            for b in blocks:
+                succs = find_successors(b)
+                if label in succs:
+                    preds.append(b.label)
+            return preds
+
+        def is_loop_header(target_block, entry_label):
+            """Check if target_block is a loop header (has a back-edge from a body block)."""
+            t = target_block.terminator
+            if not t:
+                return False, None, None, None
+            cond = re.match(r'br i1 (%\d+), label %(\w+), label %(\w+)', t)
+            if not cond:
+                return False, None, None, None
+
+            cond_ssa = cond.group(1)
+            true_lbl = cond.group(2)
+            false_lbl = cond.group(3)
+
+            # Check if the true branch has a back-edge to this header
+            true_blk = block_map.get(true_lbl)
+            if true_blk:
+                true_term = true_blk.terminator or ''
+                back = re.match(r'br label %(\w+)', true_term)
+                if back and back.group(1) == target_block.label:
+                    # True branch is the loop body, false is exit
+                    return True, cond_ssa, true_lbl, false_lbl
+
+            # Check if there's a chain: true_blk contains if/else that eventually
+            # branches back to header
+            if true_blk:
+                # Check all reachable blocks from true_lbl (within 3 hops)
+                check_queue = [true_lbl]
+                check_seen = set()
+                for _ in range(5):
+                    if not check_queue:
+                        break
+                    lbl = check_queue.pop(0)
+                    if lbl in check_seen or lbl == false_lbl:
+                        continue
+                    check_seen.add(lbl)
+                    blk = block_map.get(lbl)
+                    if blk:
+                        for s in find_successors(blk):
+                            if s == target_block.label:
+                                return True, cond_ssa, true_lbl, false_lbl
+                            if s not in check_seen:
+                                check_queue.append(s)
+
+            # Check false branch as body (inverted condition)
+            false_blk = block_map.get(false_lbl)
+            if false_blk:
+                false_term = false_blk.terminator or ''
+                back = re.match(r'br label %(\w+)', false_term)
+                if back and back.group(1) == target_block.label:
+                    # False branch is body, true is exit — condition inverted
+                    return True, cond_ssa, false_lbl, true_lbl
+
+            return False, None, None, None
+
+        def parse_struct_phi_type(phi_line):
+            """Extract struct field types from a phi of struct type.
+            Returns (is_struct, field_types_list)."""
+            # e.g. %62 = phi { float, float } [...]
+            m = re.match(r'%\d+\s*=\s*phi\s+\{([^}]+)\}', phi_line)
+            if m:
+                field_types = [t.strip() for t in m.group(1).split(',')]
+                return True, field_types
+            return False, []
+
+        def emit_loop(header_block, entry_block, cond_ssa,
+                      body_label, exit_label):
+            """Emit a WGSL loop construct for a natural loop."""
+            entry_label = entry_block.label
+            if entry_label == 'entry':
+                entry_label = str(len(self.func_args))
+
+            # Declare and initialize phi variables from entry values
+            for phi_line in header_block.phis:
+                dst, ty = phi_dst_and_type(phi_line)
+                if not dst:
+                    continue
+
+                is_struct, field_types = parse_struct_phi_type(phi_line)
+                if is_struct:
+                    # Struct phi: declare var for each field
+                    init_val = resolve_phi_value(phi_line, entry_label)
+                    for fi, ft in enumerate(field_types):
+                        wt = LLVM_TYPE_TO_WGSL.get(ft, 'f32')
+                        var_name = f'{self._var(dst)}_{fi}'
+                        if init_val == 'zeroinitializer' or init_val == 'undef' or init_val is None:
+                            stmts.append(f'var {var_name}: {wt} = {self._zero_val(wt)};')
+                        elif init_val.startswith('%') and init_val in self.struct_fields:
+                            stmts.append(f'var {var_name}: {wt} = {self.struct_fields[init_val].get(fi, self._zero_val(wt))};')
+                        else:
+                            stmts.append(f'var {var_name}: {wt} = {self._zero_val(wt)};')
+                        self.values[f'{dst}_{fi}'] = (var_name, wt)
+                    # Register the struct fields dict
+                    self.struct_fields[dst] = {fi: f'{self._var(dst)}_{fi}'
+                                               for fi in range(len(field_types))}
+                else:
+                    if 'ptr' in (ty or ''):
+                        continue  # Skip pointer phis
+                    wgsl_ty = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+                    init_val = resolve_phi_value(phi_line, entry_label)
+                    init_expr = self._operand(init_val, wgsl_ty) if init_val else self._zero_val(wgsl_ty)
+                    stmts.append(f'var {self._var(dst)}: {wgsl_ty} = {init_expr};')
+                    self.values[dst] = (self._var(dst), wgsl_ty)
+
+            # Process header body (compute condition, etc.)
+            # First translate header's body instructions to compute the condition
+            header_body_stmts = []
+            process_block_body(header_block, header_body_stmts)
+
+            # Emit WGSL loop
+            stmts.append('loop {')
+
+            # Header body inside loop (condition computation)
+            for s in header_body_stmts:
+                stmts.append(f'    {s}')
+
+            # Break condition
+            cond_expr = self._operand(cond_ssa, 'bool')
+            # Determine which direction: if cond is true → body, false → exit
+            # Then we break when !cond
+            stmts.append(f'    if !{cond_expr} {{ break; }}')
+
+            # Process loop body blocks
+            body_block = block_map.get(body_label)
+            if body_block:
+                visited.add(body_label)
+                # Handle inner if/else within the body
+                body_stmts = []
+                self._translate_loop_body(body_block, header_block.label,
+                                          block_map, visited, body_stmts)
+                for s in body_stmts:
+                    stmts.append(f'    {s}')
+
+            # Update phi variables from back-edge (continuing block)
+            # Find which block provides the back-edge values
+            back_label = self._find_back_edge_label(
+                body_label, header_block.label, block_map, visited)
+
+            for phi_line in header_block.phis:
+                dst, ty = phi_dst_and_type(phi_line)
+                if not dst:
+                    continue
+                is_struct, field_types = parse_struct_phi_type(phi_line)
+                if is_struct:
+                    back_val = resolve_phi_value(phi_line, back_label)
+                    if back_val and back_val in self.struct_fields:
+                        for fi, ft in enumerate(field_types):
+                            wt = LLVM_TYPE_TO_WGSL.get(ft, 'f32')
+                            var_name = f'{self._var(dst)}_{fi}'
+                            src_expr = self.struct_fields[back_val].get(fi, self._zero_val(wt))
+                            stmts.append(f'    {var_name} = {src_expr};')
+                else:
+                    if 'ptr' in (ty or ''):
+                        continue
+                    wgsl_ty = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+                    back_val = resolve_phi_value(phi_line, back_label)
+                    if back_val:
+                        update_expr = self._operand(back_val, wgsl_ty)
+                        stmts.append(f'    {self._var(dst)} = {update_expr};')
+
+            stmts.append('}')
+
+            # Process exit block
+            visited.add(header_block.label)
+            exit_block = block_map.get(exit_label)
+            if exit_block and exit_label not in visited:
+                visited.add(exit_label)
+                process_block_body(exit_block, stmts)
+                # Follow exit terminator
+                process_block_terminator(exit_block)
+
+        def process_block_terminator(block):
+            """Process just the terminator of a block (follow unconditional branches).
+
+            Includes loop detection and phi resolution so that sequential loops
+            (e.g., mean loop followed by variance loop) are properly structured.
+            """
+            t = block.terminator
+            if not t or t == 'ret void':
+                return
+            uncond = re.match(r'br label %(\w+)', t)
+            if uncond:
+                tgt_label = uncond.group(1)
+                tgt = block_map.get(tgt_label)
+                if tgt and tgt_label not in visited:
+                    # Check if target is a loop header
+                    is_loop, loop_cond, loop_body, loop_exit = is_loop_header(tgt, block.label)
+                    if is_loop:
+                        emit_loop(tgt, block, loop_cond, loop_body, loop_exit)
+                        return
+
+                    # Not a loop — process phis at target and continue
+                    for phi in tgt.phis:
+                        dst, ty = phi_dst_and_type(phi)
+                        if dst and 'ptr' not in ty:
+                            is_struct, field_types = parse_struct_phi_type(phi)
+                            if is_struct:
+                                init_val = resolve_phi_value(phi, block.label)
+                                for fi, ft in enumerate(field_types):
+                                    wt = LLVM_TYPE_TO_WGSL.get(ft, 'f32')
+                                    var_name = f'{self._var(dst)}_{fi}'
+                                    if init_val and init_val in self.struct_fields:
+                                        src_expr = self.struct_fields[init_val].get(fi, self._zero_val(wt))
+                                        stmts.append(f'let {var_name}: {wt} = {src_expr};')
+                                    elif init_val == 'zeroinitializer' or init_val == 'undef' or not init_val:
+                                        stmts.append(f'let {var_name}: {wt} = {self._zero_val(wt)};')
+                                    else:
+                                        stmts.append(f'let {var_name}: {wt} = {self._zero_val(wt)};')
+                                    self.values[f'{dst}_{fi}'] = (var_name, wt)
+                                self.struct_fields[dst] = {fi: f'{self._var(dst)}_{fi}'
+                                                           for fi in range(len(field_types))}
+                            else:
+                                val = resolve_phi_value(phi, block.label)
+                                if val:
+                                    wgsl_ty = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+                                    self.values[dst] = (self._var(dst), wgsl_ty)
+                                    val_expr = self._operand(val, wgsl_ty)
+                                    stmts.append(f'let {self._var(dst)}: {wgsl_ty} = {val_expr};')
+                    process_block(tgt)
+
+        def process_block(block):
+            """Process a block and follow its control flow."""
+            if block.label in visited:
+                return
+            visited.add(block.label)
+
+            # Process body
+            process_block_body(block, stmts)
+
+            # Handle terminator
+            t = block.terminator
+            if t == 'ret void' or not t:
+                return
+
+            # Unconditional branch
+            uncond = re.match(r'br label %(\w+)', t)
+            if uncond:
+                tgt_label = uncond.group(1)
+                tgt = block_map.get(tgt_label)
+                if tgt and tgt_label not in visited:
+                    # Check if target is a loop header
+                    is_loop, loop_cond, loop_body, loop_exit = is_loop_header(tgt, block.label)
+                    if is_loop:
+                        emit_loop(tgt, block, loop_cond, loop_body, loop_exit)
+                        return
+
+                    # Not a loop — process phis at target and continue
+                    for phi in tgt.phis:
+                        dst, ty = phi_dst_and_type(phi)
+                        if dst and 'ptr' not in ty:
+                            val = resolve_phi_value(phi, block.label)
+                            if val:
+                                wgsl_ty = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+                                self.values[dst] = (self._var(dst), wgsl_ty)
+                                val_expr = self._operand(val, wgsl_ty)
+                                stmts.append(f'let {self._var(dst)}: {wgsl_ty} = {val_expr};')
+                    process_block(tgt)
+                return
+
+            # Conditional branch
+            cond_match = re.match(r'br i1 (%\d+), label %(\w+), label %(\w+)', t)
+            if cond_match:
+                cond_ssa = cond_match.group(1)
+                true_label = cond_match.group(2)
+                false_label = cond_match.group(3)
+                self._translate_conditional_branch(
+                    cond_ssa, true_label, false_label,
+                    block, block_map, stmts, visited
+                )
+
+        process_block(blocks[0])
+        return stmts
+
+    def _translate_conditional_branch(
+        self, cond_ssa, true_label, false_label,
+        entry_block, block_map, stmts, visited
+    ):
+        """Translate a conditional branch with phi resolution into WGSL if/else."""
+        cond_expr = self._operand(cond_ssa, 'bool')
+
+        # Find merge point
+        # Pattern: one branch may jump directly to the other (forming a triangle/diamond)
+        true_block = block_map.get(true_label)
+        false_block = block_map.get(false_label)
+
+        if not true_block or not false_block:
+            stmts.append(f'// Unresolved branch: {cond_ssa}')
+            return
+
+        # Determine which block is the merge point
+        true_succs = set()
+        m = re.match(r'br label %(\w+)', true_block.terminator or '')
+        if m:
+            true_succs.add(m.group(1))
+        false_succs = set()
+        m = re.match(r'br label %(\w+)', false_block.terminator or '')
+        if m:
+            false_succs.add(m.group(1))
+
+        merge_label = None
+        # Pattern 1: Both branches converge to same block
+        common = true_succs & false_succs
+        if common:
+            merge_label = common.pop()
+        # Pattern 2: false block jumps to true_label (true_label is merge)
+        elif true_label in false_succs:
+            merge_label = true_label
+        # Pattern 3: true block jumps to false_label (false_label is merge)
+        elif false_label in true_succs:
+            merge_label = false_label
+
+        if not merge_label:
+            # Fallback: just process both blocks sequentially
+            stmts.append(f'// Complex control flow - linearized')
+            visited.add(true_label)
+            visited.add(false_label)
+            for line in true_block.body:
+                r = self._translate_one(line)
+                if r:
+                    stmts.append(r)
+            for line in false_block.body:
+                r = self._translate_one(line)
+                if r:
+                    stmts.append(r)
+            return
+
+        merge_block = block_map.get(merge_label)
+
+        # Determine which predecessor labels feed into the merge phi nodes
+        # entry_label is the label of the block that branches
+        entry_label = entry_block.label
+        if entry_label == 'entry':
+            # Find the actual numeric label for entry (usually %6 from the preds comments)
+            # The entry block number is the arg count for the function
+            entry_label = str(len(self.func_args))
+
+        # Collect phi nodes at the merge point
+        phi_declarations = []
+        phi_true_assigns = []
+        phi_false_assigns = []
+
+        # Pre-resolve phis in non-merge blocks so their SSA values are known
+        # (e.g., block 14's phi resolves %15 = %2 which is a func arg ptr)
+        for block in [true_block, false_block]:
+            if block.label == merge_label:
+                continue
+            for phi_line in block.phis:
+                pm = re.match(r'(%\d+)\s*=\s*phi\s+(.+)', phi_line)
+                if not pm:
+                    continue
+                phi_dst = pm.group(1)
+                rest = pm.group(2)
+                if 'ptr' in rest:
+                    pairs = re.findall(
+                        r'\[\s*(%\d+|\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|zeroinitializer),\s*%(\w+)\s*\]',
+                        phi_line
+                    )
+                    for val, lbl in pairs:
+                        if val.startswith('%'):
+                            try:
+                                arg_idx = int(val[1:])
+                                if arg_idx in self.arg_to_binding:
+                                    self.gep_info[phi_dst] = GepInfo(
+                                        buffer_arg_idx=arg_idx,
+                                        offset_expr='0',
+                                    )
+                            except ValueError:
+                                pass
+
+        if merge_block:
+            for phi_line in merge_block.phis:
+                dst, ty = None, None
+                m = re.match(r'(%\d+)\s*=\s*phi\s+(.+)', phi_line)
+                if not m:
+                    continue
+                dst = m.group(1)
+                rest = m.group(2)
+
+                # Extract type (may be "ptr addrspace(1)" or "i32")
+                is_ptr = 'ptr' in rest
+                if is_ptr:
+                    ty_str = 'ptr'
+                    wgsl_type = 'i32'  # pointers get resolved via loads
+                else:
+                    ty_match = re.match(r'(\S+)\s+\[', rest)
+                    ty_str = ty_match.group(1) if ty_match else 'i32'
+                    wgsl_type = LLVM_TYPE_TO_WGSL.get(ty_str, 'i32')
+
+                # Parse [value, %label] pairs
+                pairs = re.findall(
+                    r'\[\s*(%\d+|\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|zeroinitializer),\s*%(\w+)\s*\]',
+                    phi_line
+                )
+
+                # Determine which value comes from true path vs false path
+                true_val = None
+                false_val = None
+                for val, lbl in pairs:
+                    if lbl == entry_label:
+                        # Direct from entry = true branch (or false, depends on pattern)
+                        if merge_label == true_label:
+                            # Merge IS the true block, so entry→true means direct (true path)
+                            true_val = val
+                        else:
+                            true_val = val
+                    elif lbl == false_label:
+                        false_val = val
+                    elif lbl == true_label:
+                        true_val = val
+                    else:
+                        # Find which path this predecessor is on
+                        # If the predecessor can reach true_label, it's on true path
+                        false_val = val  # default
+
+                if is_ptr:
+                    # Phi on pointer: need to resolve buffer loads in each branch
+                    # We'll handle the subsequent load from this phi result
+                    # by setting up GEP info for each branch's value
+                    # and then doing the load inside the if/else
+
+                    # First, find the load that uses this phi result
+                    load_dst = None
+                    load_elem_type = 'i32'
+                    if merge_block:
+                        for body_line in merge_block.body:
+                            load_match = re.match(
+                                r'(%\d+)\s*=\s*load\s+(\S+),\s*ptr\s+addrspace\(1\)\s+' + re.escape(dst),
+                                body_line
+                            )
+                            if load_match:
+                                load_dst = load_match.group(1)
+                                load_elem_type = load_match.group(2).rstrip(',')
+                                break
+
+                    if load_dst:
+                        load_wgsl_type = LLVM_TYPE_TO_WGSL.get(load_elem_type, 'i32')
+                        phi_declarations.append(
+                            f'var {self._var(load_dst)}: {load_wgsl_type};'
+                        )
+                        self.values[load_dst] = (self._var(load_dst), load_wgsl_type)
+
+                        # Generate load for true branch
+                        true_load = self._resolve_ptr_load(true_val, load_wgsl_type)
+                        false_load = self._resolve_ptr_load(false_val, load_wgsl_type)
+
+                        phi_true_assigns.append(
+                            f'{self._var(load_dst)} = {true_load};'
+                        )
+                        phi_false_assigns.append(
+                            f'{self._var(load_dst)} = {false_load};'
+                        )
+                else:
+                    # Scalar phi: declare var, assign in each branch
+                    phi_declarations.append(
+                        f'var {self._var(dst)}: {wgsl_type};'
+                    )
+                    self.values[dst] = (self._var(dst), wgsl_type)
+
+                    if true_val:
+                        phi_true_assigns.append(
+                            f'{self._var(dst)} = {self._operand(true_val, wgsl_type)};'
+                        )
+                    if false_val:
+                        phi_false_assigns.append(
+                            f'{self._var(dst)} = {self._operand(false_val, wgsl_type)};'
+                        )
+
+        # Emit var declarations for phi results
+        for decl in phi_declarations:
+            stmts.append(decl)
+
+        # Build true/false branch statement lists
+        true_stmts = []
+        false_stmts = []
+
+        # Determine actual true vs false body based on merge pattern
+        if merge_label == true_label:
+            # True block IS the merge; false block has its own body
+            # Entry→true (merge): assigns from entry_label
+            # Entry→false→true (merge): assigns from false_label
+            true_stmts.extend(phi_true_assigns)
+            false_stmts.extend(phi_false_assigns)
+            # Process false block body
+            for line in false_block.body:
+                r = self._translate_one(line)
+                if r:
+                    false_stmts.append(r)
+        elif merge_label == false_label:
+            # False block IS the merge; true block has its own body
+            true_stmts.extend(phi_true_assigns)
+            # Process true block body
+            for line in true_block.body:
+                r = self._translate_one(line)
+                if r:
+                    true_stmts.append(r)
+            false_stmts.extend(phi_false_assigns)
+        else:
+            # Classic diamond: both branches converge to a separate merge
+            true_stmts.extend(phi_true_assigns)
+            for line in true_block.body:
+                r = self._translate_one(line)
+                if r:
+                    true_stmts.append(r)
+            false_stmts.extend(phi_false_assigns)
+            for line in false_block.body:
+                r = self._translate_one(line)
+                if r:
+                    false_stmts.append(r)
+
+        # Emit if/else
+        stmts.append(f'if {cond_expr} {{')
+        for s in true_stmts:
+            stmts.append(f'    {s}')
+        if false_stmts:
+            stmts.append(f'}} else {{')
+            for s in false_stmts:
+                stmts.append(f'    {s}')
+        stmts.append('}')
+
+        # Mark branches as visited (but not merge if it coincides)
+        if true_label != merge_label:
+            visited.add(true_label)
+        if false_label != merge_label:
+            visited.add(false_label)
+
+        # Process merge block body (skip phi nodes and load from phi ptr)
+        if merge_block and merge_label not in visited:
+            visited.add(merge_label)
+            for body_line in merge_block.body:
+                # Skip loads from phi pointer results (already handled above)
+                if any(re.search(re.escape(dst_ssa) + r'\b', body_line)
+                       for phi_line in (merge_block.phis or [])
+                       for dst_ssa in [re.match(r'(%\d+)', phi_line).group(1)]
+                       if re.match(r'(%\d+)', phi_line)
+                       and 'ptr' in phi_line):
+                    continue
+                r = self._translate_one(body_line)
+                if r:
+                    stmts.append(r)
+
+            # Follow merge block's terminator
+            t = merge_block.terminator
+            if t and t != 'ret void':
+                uncond = re.match(r'br label %(\w+)', t)
+                if uncond:
+                    next_label = uncond.group(1)
+                    next_block = block_map.get(next_label)
+                    if next_block and next_label not in visited:
+                        visited.add(next_label)
+                        # Recursively process remaining blocks
+                        for body_line in next_block.body:
+                            r = self._translate_one(body_line)
+                            if r:
+                                stmts.append(r)
+                        # Follow chain of unconditional branches
+                        nt = next_block.terminator
+                        while nt and nt != 'ret void':
+                            um = re.match(r'br label %(\w+)', nt)
+                            if um:
+                                nl = um.group(1)
+                                nb = block_map.get(nl)
+                                if nb and nl not in visited:
+                                    visited.add(nl)
+                                    for body_line in nb.body:
+                                        r = self._translate_one(body_line)
+                                        if r:
+                                            stmts.append(r)
+                                    nt = nb.terminator
+                                else:
+                                    break
+                            else:
+                                break
+
+    def _translate_loop_body(self, body_block, header_label, block_map, visited, body_stmts):
+        """Translate the body of a loop, handling inner if/else and chains.
+
+        Processes body_block and follows its control flow until we hit
+        the back-edge (branch back to header_label). Appends to body_stmts.
+        """
+        # Translate body instructions
+        for line in body_block.body:
+            result = self._translate_one(line)
+            if result:
+                body_stmts.append(result)
+
+        # Follow terminator (except back-edge)
+        t = body_block.terminator
+        if not t or t == 'ret void':
+            return
+
+        # Unconditional branch
+        uncond = re.match(r'br label %(\w+)', t)
+        if uncond:
+            tgt = uncond.group(1)
+            if tgt == header_label:
+                return  # Back-edge — stop
+            tgt_block = block_map.get(tgt)
+            if tgt_block and tgt not in visited:
+                visited.add(tgt)
+                self._translate_loop_body(tgt_block, header_label,
+                                          block_map, visited, body_stmts)
+            return
+
+        # Conditional branch inside loop body (inner if/else)
+        cond = re.match(r'br i1 (%\d+), label %(\w+), label %(\w+)', t)
+        if cond:
+            cond_ssa = cond.group(1)
+            true_label = cond.group(2)
+            false_label = cond.group(3)
+            self._translate_conditional_branch(
+                cond_ssa, true_label, false_label,
+                body_block, block_map, body_stmts, visited
+            )
+
+    def _find_back_edge_label(self, body_label, header_label, block_map, visited):
+        """Find the label of the block providing the back-edge to the loop header.
+
+        Walk from body_label following unconditional branches until we find
+        one that branches to header_label. Return its label.
+        """
+        seen = set()
+        queue = [body_label]
+        while queue:
+            lbl = queue.pop(0)
+            if lbl in seen:
+                continue
+            seen.add(lbl)
+            blk = block_map.get(lbl)
+            if not blk:
+                continue
+            t = blk.terminator
+            if not t:
+                continue
+            # Check unconditional branch to header
+            uncond = re.match(r'br label %(\w+)', t)
+            if uncond and uncond.group(1) == header_label:
+                return lbl
+            # Follow branches
+            cond = re.match(r'br i1 %\d+, label %(\w+), label %(\w+)', t)
+            if cond:
+                queue.append(cond.group(1))
+                queue.append(cond.group(2))
+            elif uncond:
+                queue.append(uncond.group(1))
+        return body_label  # Fallback
+
+    def _resolve_ptr_load(self, ptr_val: str, wgsl_type: str) -> str:
+        """Resolve a pointer value to a buffer load expression."""
+        if ptr_val is None:
+            return self._zero_val(wgsl_type)
+
+        # Check if it's a function argument
+        if ptr_val.startswith('%'):
+            try:
+                arg_idx = int(ptr_val[1:])
+                if arg_idx in self.arg_to_binding:
+                    binding_idx = self.arg_to_binding[arg_idx]
+                    return f'buf{binding_idx}[0u]'
+            except ValueError:
+                pass
+
+            # Check if it's a GEP result
+            if ptr_val in self.gep_info:
+                gep = self.gep_info[ptr_val]
+                binding_idx = self.arg_to_binding.get(gep.buffer_arg_idx, 0)
+                return f'buf{binding_idx}[u32({gep.offset_expr})]'
+
+            # Check if it's a known value (like a phi-resolved pointer)
+            if ptr_val in self.values:
+                return self.values[ptr_val][0]
+
+        return self._zero_val(wgsl_type)
+
+    def _translate_one(self, line: str) -> Optional[str]:
+        """Translate a single LLVM IR instruction to WGSL. Returns None for no-op."""
+
+        # --- ret void ---
+        if line.strip() == 'ret void':
+            return None  # End of function, no explicit return needed
+
+        # --- Assignment instructions: %dst = ... ---
+        assign_match = re.match(r'(%\d+)\s*=\s*(.*)', line)
+        if assign_match:
+            dst = assign_match.group(1)
+            rhs = assign_match.group(2).strip()
+            return self._translate_assignment(dst, rhs)
+
+        # --- store to shared memory (addrspace(3)) ---
+        smem_store_match = re.match(
+            r'store\s+(\S+)\s+(%\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|true|false),\s*ptr\s+addrspace\(3\)\s+(%\d+)',
+            line
+        )
+        if smem_store_match:
+            ty = smem_store_match.group(1)
+            val = smem_store_match.group(2)
+            ptr = smem_store_match.group(3)
+            return self._translate_smem_store(val, ptr, ty)
+
+        # --- store instruction ---
+        store_match = re.match(
+            r'store\s+(\S+)\s+(%\d+|[\d.e+-]+|0x[\da-fA-F]+|undef|true|false),\s*ptr\s+addrspace\(1\)\s+(%\d+)',
+            line
+        )
+        if store_match:
+            ty = store_match.group(1)
+            val = store_match.group(2)
+            ptr = store_match.group(3)
+            return self._translate_store(val, ptr, ty)
+
+        # --- void calls (barriers, etc.) ---
+        void_call_match = re.match(r'call\s+void\s+@(\w+)\(', line)
+        if void_call_match:
+            func_name = void_call_match.group(1)
+            if func_name == '__spirv_ControlBarrier':
+                return 'workgroupBarrier();'
+            return None  # Ignore other void calls
+
+        # Ignore other instructions (labels, metadata, etc.)
+        return None
+
+    def _translate_assignment(self, dst: str, rhs: str) -> Optional[str]:
+        """Translate an assignment instruction."""
+
+        # --- SPIR-V builtin calls ---
+        builtin_match = re.match(
+            r'call\s+i32\s+@__spirv_BuiltIn(\w+)\(i32\s+(\d+)\)', rhs
+        )
+        if builtin_match:
+            builtin = builtin_match.group(1)
+            axis = int(builtin_match.group(2))
+            component = ['x', 'y', 'z'][axis]
+
+            if builtin == 'WorkgroupId':
+                expr = f'i32(_wg_id.{component})'
+                self.uniform_values.add(dst)  # same across all threads
+            elif builtin == 'LocalInvocationId':
+                expr = f'i32(_lid.{component})'
+                # NOT uniform — differs per thread
+            elif builtin == 'NumWorkgroups':
+                expr = f'i32(_num_wg.{component})'
+                self.uniform_values.add(dst)  # same across all threads
+            else:
+                expr = f'0 /* unknown builtin: {builtin} */'
+
+            self.values[dst] = (self._var(dst), 'i32')
+            return f'let {self._var(dst)}: i32 = {expr};'
+
+        # --- SubgroupShuffleXor call ---
+        # Native path: subgroupShuffleXor() (requires Subgroups feature)
+        # Fallback: emulate via workgroup shared memory
+        shuffle_match = re.match(
+            r'call\s+(\S+)\s+@__spirv_SubgroupShuffleXor\(i32\s+\d+,\s*(\S+)\s+(%\d+),\s*i32\s+(%\d+|\d+)\)',
+            rhs
+        )
+        if shuffle_match:
+            ret_type = shuffle_match.group(1)
+            val = shuffle_match.group(3)
+            mask = shuffle_match.group(4)
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(ret_type, 'i32')
+            val_expr = self._operand(val, wgsl_type)
+            mask_expr = self._operand(mask, 'u32')
+            if not mask_expr.startswith('u32('):
+                mask_expr = f'u32({mask_expr})'
+            self.values[dst] = (self._var(dst), wgsl_type)
+
+            if self.use_native_subgroups:
+                # Native WGSL subgroup shuffle
+                self.needs_subgroups = True
+                return (f'let {self._var(dst)}: {wgsl_type} = '
+                        f'subgroupShuffleXor({val_expr}, {mask_expr});')
+            else:
+                # Shared-memory emulation
+                self.needs_shuffle_scratch = True
+                return (f'_shfl[_lid.x] = {val_expr};\n'
+                        f'    workgroupBarrier();\n'
+                        f'    let {self._var(dst)}: {wgsl_type} = _shfl[_lid.x ^ {mask_expr}];\n'
+                        f'    workgroupBarrier();')
+
+        # --- atomicrmw instruction ---
+        # Pattern: atomicrmw OP ptr addrspace(1) %ptr, TYPE %val ordering, align N
+        atomic_match = re.match(
+            r'atomicrmw\s+(\w+)\s+ptr\s+addrspace\(1\)\s+(%\d+),\s*(\w+)\s+(%\d+|[\d.e+-]+|0x[\da-fA-F]+)\s',
+            rhs
+        )
+        if atomic_match:
+            return self._translate_atomicrmw(
+                dst, atomic_match.group(1), atomic_match.group(2),
+                atomic_match.group(3), atomic_match.group(4))
+
+        # --- insertvalue (struct aggregate) ---
+        # Pattern: insertvalue { T1, T2, ... } %src_or_undef, T %val, IDX
+        insertval_match = re.match(
+            r'insertvalue\s+\{[^}]+\}\s+(%\d+|undef),\s*\w+\s+(%\d+|[\d.e+-]+|0x[\da-fA-F]+),\s*(\d+)',
+            rhs
+        )
+        if insertval_match:
+            src = insertval_match.group(1)
+            val = insertval_match.group(2)
+            idx = int(insertval_match.group(3))
+            # Parse struct field types from the type signature
+            type_match = re.match(r'insertvalue\s+\{([^}]+)\}', rhs)
+            field_types_str = type_match.group(1) if type_match else 'float'
+            field_types = [t.strip() for t in field_types_str.split(',')]
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(field_types[idx] if idx < len(field_types) else 'float', 'f32')
+
+            # Copy fields from source (if not undef)
+            fields = {}
+            if src != 'undef' and src in self.struct_fields:
+                fields = dict(self.struct_fields[src])
+
+            val_expr = self._operand(val, wgsl_type)
+            fields[idx] = val_expr
+
+            self.struct_fields[dst] = fields
+            # Also register individual field values for extractvalue
+            for fi, fexpr in fields.items():
+                ft = LLVM_TYPE_TO_WGSL.get(field_types[fi] if fi < len(field_types) else 'float', 'f32')
+                self.values[f'{dst}_{fi}'] = (fexpr, ft)
+            return None  # No statement — struct is virtual
+
+        # --- extractvalue (struct aggregate) ---
+        # Pattern: extractvalue { T1, T2, ... } %src, IDX
+        extractval_match = re.match(
+            r'extractvalue\s+\{([^}]+)\}\s+(%\d+),\s*(\d+)',
+            rhs
+        )
+        if extractval_match:
+            field_types_str = extractval_match.group(1)
+            src = extractval_match.group(2)
+            idx = int(extractval_match.group(3))
+            field_types = [t.strip() for t in field_types_str.split(',')]
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(field_types[idx] if idx < len(field_types) else 'float', 'f32')
+
+            if src in self.struct_fields and idx in self.struct_fields[src]:
+                expr = self.struct_fields[src][idx]
+            else:
+                # Fallback: look for the field variable
+                field_key = f'{src}_{idx}'
+                if field_key in self.values:
+                    expr = self.values[field_key][0]
+                else:
+                    expr = self._zero_val(wgsl_type)
+
+            self.values[dst] = (expr, wgsl_type)
+            return None
+
+        # --- insertelement <N x T> (general, including <1 x T>) ---
+        insert_match = re.match(
+            r'insertelement\s+<(\d+)\s+x\s+(\w+)>\s+(%\d+|undef),\s*\w+\s+(%\d+|\d+|[\d.e+-]+|0x[\da-fA-F]+),\s*i32\s+(\d+)',
+            rhs
+        )
+        if insert_match:
+            vec_size = int(insert_match.group(1))
+            elem_type = insert_match.group(2)
+            src_vec = insert_match.group(3)
+            val = insert_match.group(4)
+            idx = int(insert_match.group(5))
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(elem_type, 'f32')
+            val_expr = self._operand(val, wgsl_type)
+
+            if vec_size == 1:
+                # <1 x T>: pass-through
+                self.values[dst] = (val_expr, wgsl_type)
+                return None
+
+            # <N x T>: track elements
+            elements = {}
+            if src_vec != 'undef' and src_vec in self.vector_elements:
+                elements = dict(self.vector_elements[src_vec])
+            elements[idx] = (val_expr, wgsl_type)
+            self.vector_elements[dst] = elements
+            return None
+
+        # --- extractelement <N x T> (general, including <1 x T>) ---
+        extract_match = re.match(
+            r'extractelement\s+<(\d+)\s+x\s+(\w+)>\s+(%\d+),\s*i32\s+(\d+)',
+            rhs
+        )
+        if extract_match:
+            vec_size = int(extract_match.group(1))
+            elem_type = extract_match.group(2)
+            vec_val = extract_match.group(3)
+            idx = int(extract_match.group(4))
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(elem_type, 'f32')
+
+            if vec_size == 1:
+                # <1 x T>: pass-through
+                vec_expr = self._operand(vec_val, wgsl_type)
+                self.values[dst] = (vec_expr, wgsl_type)
+                return None
+
+            # <N x T>: look up tracked element
+            if vec_val in self.vector_elements and idx in self.vector_elements[vec_val]:
+                expr, etype = self.vector_elements[vec_val][idx]
+                self.values[dst] = (expr, etype)
+            else:
+                self.values[dst] = (self._zero_val(wgsl_type), wgsl_type)
+            return None
+
+        # --- Other call instructions (generic) ---
+        call_match = re.match(r'call\s+(\S+)\s+@([\w.]+)\((.*)\)', rhs)
+        if call_match:
+            ret_type = call_match.group(1)
+            func_name = call_match.group(2)
+            args_str = call_match.group(3)
+
+            # Try to handle known LLVM intrinsics
+            intrinsic_result = self._translate_llvm_intrinsic(
+                dst, ret_type, func_name, args_str)
+            if intrinsic_result is not None:
+                return intrinsic_result
+
+            # Skip unknown SPIR-V/LLVM intrinsics with zero default
+            wgsl_type = LLVM_TYPE_TO_WGSL.get(ret_type, 'i32')
+            zv = self._zero_val(wgsl_type)
+            self.values[dst] = (self._var(dst), wgsl_type)
+            return f'let {self._var(dst)}: {wgsl_type} = {zv}; /* TODO: {func_name} */'
+
+
+        # --- Cast instructions: fptosi, sitofp, zext, sext, trunc, etc. ---
+        cast_match = re.match(
+            r'(fptosi|sitofp|fptoui|uitofp|zext|sext|trunc|fpext|fptrunc|bitcast)'
+            r'\s+(\S+)\s+(.+?)\s+to\s+(\S+)',
+            rhs
+        )
+        if cast_match:
+            cast_op = cast_match.group(1)
+            src_type = cast_match.group(2)
+            src_val = cast_match.group(3).strip()
+            dst_type = cast_match.group(4)
+            return self._translate_cast(dst, cast_op, src_type, src_val, dst_type)
+
+        # --- GEP into shared memory (addrspace(3)) ---
+        smem_gep_match = re.match(
+            r'getelementptr\s+(?:inbounds\s+)?i8,\s*ptr\s+addrspace\(3\)\s+@global_smem,\s*i32\s+(.+)',
+            rhs
+        )
+        if smem_gep_match:
+            offset = smem_gep_match.group(1).strip()
+            offset_expr = self._operand(offset, 'i32')
+            self.smem_gep_info[dst] = offset_expr
+            self.needs_smem = True
+            return None
+
+        # --- Nested GEP constant expression: base is a GEP into @global_smem with constant offset ---
+        # Pattern: getelementptr i8, ptr addrspace(3) getelementptr (i8, ptr addrspace(3) @global_smem, i64 BASE), i32 OFFSET
+        nested_gep_match = re.match(
+            r'getelementptr\s+(?:inbounds\s+)?i8,\s*ptr\s+addrspace\(3\)\s+'
+            r'getelementptr\s*\(i8,\s*ptr\s+addrspace\(3\)\s+@global_smem,\s*i64\s+(\d+)\),\s*i32\s+(.+)',
+            rhs
+        )
+        if nested_gep_match:
+            base_offset = int(nested_gep_match.group(1))
+            dyn_offset = nested_gep_match.group(2).strip()
+            dyn_expr = self._operand(dyn_offset, 'i32')
+            self.smem_gep_info[dst] = f'({base_offset} + {dyn_expr})'
+            self.needs_smem = True
+            return None
+
+        # --- GEP with typed element into shared memory (e.g., float or i32) ---
+        # Pattern: getelementptr inbounds float, ptr addrspace(3) %ptr, i32 N
+        smem_typed_gep_match = re.match(
+            r'getelementptr\s+(?:inbounds\s+)?(\w+),\s*ptr\s+addrspace\(3\)\s+(%\d+),\s*i32\s+(.+)',
+            rhs
+        )
+        if smem_typed_gep_match:
+            elem_type = smem_typed_gep_match.group(1)
+            base_ptr = smem_typed_gep_match.group(2)
+            idx = smem_typed_gep_match.group(3).strip()
+            elem_bytes = LLVM_TYPE_TO_BYTES.get(elem_type, 4)
+            idx_expr = self._operand(idx, 'i32')
+
+            if base_ptr in self.smem_gep_info:
+                base_expr = self.smem_gep_info[base_ptr]
+                if elem_bytes == 1:
+                    self.smem_gep_info[dst] = f'({base_expr} + {idx_expr})'
+                else:
+                    self.smem_gep_info[dst] = f'({base_expr} + ({idx_expr}) * {elem_bytes})'
+                self.needs_smem = True
+                return None
+
+        # --- Chained GEP into shared memory with i8 base ---
+        # Pattern: getelementptr i8, ptr addrspace(3) %known_smem_ptr, i32 OFFSET
+        smem_chain_gep_match = re.match(
+            r'getelementptr\s+(?:inbounds\s+)?i8,\s*ptr\s+addrspace\(3\)\s+(%\d+),\s*i32\s+(.+)',
+            rhs
+        )
+        if smem_chain_gep_match:
+            base_ptr = smem_chain_gep_match.group(1)
+            offset = smem_chain_gep_match.group(2).strip()
+            offset_expr = self._operand(offset, 'i32')
+            if base_ptr in self.smem_gep_info:
+                base_expr = self.smem_gep_info[base_ptr]
+                self.smem_gep_info[dst] = f'({base_expr} + {offset_expr})'
+                self.needs_smem = True
+                return None
+
+        # --- load from shared memory (addrspace(3)) ---
+        smem_load_match = re.match(
+            r'load\s+(\S+),\s*ptr\s+addrspace\(3\)\s+(%\d+)',
+            rhs
+        )
+        if smem_load_match:
+            elem_type = smem_load_match.group(1).rstrip(',')
+            ptr = smem_load_match.group(2)
+            return self._translate_smem_load(dst, ptr, elem_type)
+
+        # --- GEP ---
+        gep_match = re.match(
+            r'getelementptr\s+(?:inbounds\s+)?(\S+),\s*ptr\s+addrspace\(1\)\s+(%\d+),\s*i32\s+(.+)',
+            rhs
+        )
+        if gep_match:
+            elem_type = gep_match.group(1).rstrip(',')
+            base = gep_match.group(2)
+            offset = gep_match.group(3).strip()
+            return self._translate_gep(dst, base, offset, elem_type)
+
+        # --- load ---
+        load_match = re.match(
+            r'load\s+(\S+),\s*ptr\s+addrspace\(1\)\s+(%\d+)',
+            rhs
+        )
+        if load_match:
+            elem_type = load_match.group(1).rstrip(',')
+            ptr = load_match.group(2)
+            return self._translate_load(dst, ptr, elem_type)
+
+        # --- select ---
+        select_match = re.match(
+            r'select\s+i1\s+(%\d+),\s*(\S+)\s+(%\d+|[\d.e+-]+|undef),\s*\S+\s+(%\d+|[\d.e+-]+|undef)',
+            rhs
+        )
+        if select_match:
+            cond = select_match.group(1)
+            ty = select_match.group(2).rstrip(',')
+            true_val = select_match.group(3)
+            false_val = select_match.group(4)
+            return self._translate_select(dst, cond, true_val, false_val, ty)
+
+        # --- Binary arithmetic/bitwise/comparison ops ---
+        # Pattern: op [flags] type operand1, operand2
+        binop_match = re.match(
+            r'(\w+)\s*(?:(?:nsw|nuw|exact|disjoint)\s+)*(\S+)\s+(.+),\s*(.+)',
+            rhs
+        )
+        if binop_match:
+            op = binop_match.group(1)
+            ty = binop_match.group(2)
+            op1 = binop_match.group(3).strip()
+            op2 = binop_match.group(4).strip()
+            return self._translate_binop(dst, op, ty, op1, op2)
+
+        # Unknown instruction
+        return f'// UNKNOWN: {dst} = {rhs}'
+
+    # -------------------------------------------------------------------
+    # GEP handling
+    # -------------------------------------------------------------------
+
+    def _translate_gep(self, dst: str, base: str, offset: str,
+                       elem_type: str) -> Optional[str]:
+        """Translate a GEP instruction — doesn't emit code, just tracks provenance."""
+        offset_expr = self._operand(offset, 'i32')
+        offset_uniform = self._is_operand_uniform(offset)
+
+        # Determine which buffer the base points to
+        try:
+            base_arg_idx = int(base[1:])  # %0, %1, etc.
+            if base_arg_idx in self.arg_to_binding:
+                self.gep_info[dst] = GepInfo(
+                    buffer_arg_idx=base_arg_idx,
+                    offset_expr=offset_expr,
+                    is_uniform=offset_uniform,
+                )
+                return None  # No WGSL statement for GEP
+        except ValueError:
+            pass
+
+        # If base is itself a GEP result (chained GEPs)
+        if base in self.gep_info:
+            parent = self.gep_info[base]
+            # Combine parent offset with new offset
+            combined_offset = f'({parent.offset_expr} + {offset_expr})'
+            self.gep_info[dst] = GepInfo(
+                buffer_arg_idx=parent.buffer_arg_idx,
+                offset_expr=combined_offset,
+                is_uniform=parent.is_uniform and offset_uniform,
+            )
+            return None
+
+        return f'// GEP with unknown base: {base}'
+
+    def _is_atomic_buffer(self, arg_idx: int) -> bool:
+        """Check if a buffer argument has atomic operations."""
+        return arg_idx in self.atomic_buffers
+
+    def _is_operand_uniform(self, operand: str) -> bool:
+        """Check if an operand (SSA name or literal) is thread-uniform.
+
+        A value is uniform if it is the same across all threads in a workgroup:
+        - Numeric literals / constants
+        - WorkgroupId, NumWorkgroups builtins
+        - Scalar function arguments (params)
+        - Values derived purely from uniform operands
+        """
+        if operand in self.uniform_values:
+            return True
+        # Numeric literals are always uniform
+        if operand.lstrip('-').replace('.', '', 1).replace('e', '', 1).replace('+', '', 1).isdigit():
+            return True
+        if operand.startswith('0x'):
+            return True
+        if operand in ('true', 'false', 'undef', 'zeroinitializer'):
+            return True
+        return False
+
+    def _translate_load(self, dst: str, ptr: str, elem_type: str) -> str:
+        """Translate a load instruction."""
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(elem_type, 'f32')
+
+        if ptr in self.gep_info:
+            gep = self.gep_info[ptr]
+            binding_idx = self.arg_to_binding.get(gep.buffer_arg_idx, 0)
+            buf_name = f'buf{binding_idx}'
+            self.values[dst] = (self._var(dst), wgsl_type)
+            if self._is_atomic_buffer(gep.buffer_arg_idx):
+                # Atomic buffer: use atomicLoad; element type is atomic<i32>
+                load_expr = f'atomicLoad(&{buf_name}[u32({gep.offset_expr})])'
+                if wgsl_type == 'f32':
+                    load_expr = f'bitcast<f32>({load_expr})'
+                return f'let {self._var(dst)}: {wgsl_type} = {load_expr};'
+            return f'let {self._var(dst)}: {wgsl_type} = {buf_name}[u32({gep.offset_expr})];'
+
+        # Direct load from a function argument (no GEP → load at index 0)
+        if ptr.startswith('%'):
+            try:
+                arg_idx = int(ptr[1:])
+                if arg_idx in self.arg_to_binding:
+                    binding_idx = self.arg_to_binding[arg_idx]
+                    buf_name = f'buf{binding_idx}'
+                    self.values[dst] = (self._var(dst), wgsl_type)
+                    if self._is_atomic_buffer(arg_idx):
+                        load_expr = f'atomicLoad(&{buf_name}[0u])'
+                        if wgsl_type == 'f32':
+                            load_expr = f'bitcast<f32>({load_expr})'
+                        return f'let {self._var(dst)}: {wgsl_type} = {load_expr};'
+                    return f'let {self._var(dst)}: {wgsl_type} = {buf_name}[0u];'
+            except ValueError:
+                pass
+
+        self.values[dst] = (self._var(dst), wgsl_type)
+        return f'let {self._var(dst)}: {wgsl_type} = {wgsl_type}(0); // load from unknown ptr {ptr}'
+
+    def _translate_smem_load(self, dst: str, ptr: str, elem_type: str) -> str:
+        """Translate a load from shared memory (addrspace(3))."""
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(elem_type, 'i32')
+        if ptr in self.smem_gep_info:
+            byte_offset = self.smem_gep_info[ptr]
+            if wgsl_type == 'f16':
+                # f16 is 2 bytes; shared memory is array<i32> (4 bytes per slot)
+                # Pack two f16 values per i32 slot
+                idx = f'u32({byte_offset}) >> 2u'
+                shift = f'(u32({byte_offset}) & 2u) << 3u'  # 0 or 16
+                expr = f'f16(bitcast<vec2<f16>>(u32(_smem[{idx}]))[u32({byte_offset}) >> 1u & 1u])'
+                self.values[dst] = (self._var(dst), wgsl_type)
+                return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+            idx = f'u32({byte_offset}) >> 2u'
+            if wgsl_type == 'f32':
+                expr = f'bitcast<f32>(_smem[{idx}])'
+            else:
+                expr = f'_smem[{idx}]'
+            self.values[dst] = (self._var(dst), wgsl_type)
+            return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+        self.values[dst] = (self._var(dst), wgsl_type)
+        return f'let {self._var(dst)}: {wgsl_type} = {self._zero_val(wgsl_type)}; // smem load unknown ptr'
+
+    def _translate_smem_store(self, val: str, ptr: str, ty: str) -> str:
+        """Translate a store to shared memory (addrspace(3))."""
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+        val_expr = self._operand(val, wgsl_type)
+        if ptr in self.smem_gep_info:
+            byte_offset = self.smem_gep_info[ptr]
+            if wgsl_type == 'f16':
+                # Store f16 into i32 shared memory: convert to i32 via vec2<f16>
+                # Use the lower 16 bits of the i32 slot (simple approach: one f16 per i32 slot)
+                idx = f'u32({byte_offset}) >> 2u'
+                return f'_smem[{idx}] = bitcast<i32>(vec2<f16>({val_expr}, f16(0)));'
+            idx = f'u32({byte_offset}) >> 2u'
+            if wgsl_type == 'f32':
+                return f'_smem[{idx}] = bitcast<i32>({val_expr});'
+            else:
+                return f'_smem[{idx}] = {val_expr};'
+        return f'// smem store to unknown ptr {ptr}'
+
+    def _translate_store(self, val: str, ptr: str, ty: str = 'float') -> str:
+        """Translate a store instruction with bounds checking."""
+        # Check if this is a masked store (value from select ... undef)
+        mask_cond = self.masked_values.get(val)
+
+        # Determine the buffer element type for proper casting
+        buf_elem = 'f32'  # default
+        binding_idx = 0
+        if ptr in self.gep_info:
+            gep = self.gep_info[ptr]
+            binding_idx = self.arg_to_binding.get(gep.buffer_arg_idx, 0)
+            if binding_idx < len(self.buffer_bindings):
+                buf_elem = self.buffer_bindings[binding_idx].elem_type
+
+        val_expr = self._operand(val, buf_elem)
+
+        # Convert bool to numeric if storing to numeric buffer
+        if val in self.values and self.values[val][1] == 'bool':
+            if buf_elem == 'f32':
+                val_expr = f'select(f32(0), f32(1), {self._operand(val, "bool")})'
+            else:
+                val_expr = f'select(0, 1, {self._operand(val, "bool")})'
+
+        if ptr in self.gep_info:
+            gep = self.gep_info[ptr]
+            binding_idx = self.arg_to_binding.get(gep.buffer_arg_idx, 0)
+            buf_name = f'buf{binding_idx}'
+            idx_expr = gep.offset_expr
+
+            # Atomic buffer: use atomicStore
+            if self._is_atomic_buffer(gep.buffer_arg_idx):
+                int_val = val_expr
+                if buf_elem == 'f32':
+                    int_val = f'bitcast<i32>({val_expr})'
+                elif buf_elem != 'i32':
+                    int_val = f'i32({val_expr})'
+                if mask_cond:
+                    cond_expr = self._operand(mask_cond, 'bool')
+                    return (f'if {cond_expr} '
+                            f'{{ atomicStore(&{buf_name}[u32({idx_expr})], {int_val}); }}')
+                return (f'if u32({idx_expr}) < arrayLength(&{buf_name}) '
+                        f'{{ atomicStore(&{buf_name}[u32({idx_expr})], {int_val}); }}')
+
+            # For masked stores, use condition instead of bounds check
+            if mask_cond:
+                cond_expr = self._operand(mask_cond, 'bool')
+                # Get the true value from the select (not the select result with zero fallback)
+                # The true value is stored in the select var, and cond controls it
+                return (f'if {cond_expr} '
+                        f'{{ {buf_name}[u32({idx_expr})] = {val_expr}; }}')
+
+            # Uniform offset: all threads compute the same index, so guard
+            # with _lid.x == 0u to avoid D3D12 UAV concurrent-write issues
+            if gep.is_uniform:
+                return (f'if _lid.x == 0u '
+                        f'{{ {buf_name}[u32({idx_expr})] = {val_expr}; }}')
+
+            return (f'if u32({idx_expr}) < arrayLength(&{buf_name}) '
+                    f'{{ {buf_name}[u32({idx_expr})] = {val_expr}; }}')
+
+        # Direct store to a function argument (scalar store at element 0)
+        # Guard with _lid.x == 0: all threads write to index 0 (same value),
+        # but on some GPU backends (D3D12) concurrent writes to the same
+        # storage element after workgroupBarrier() can be lost.
+        if ptr.startswith('%'):
+            try:
+                arg_idx = int(ptr[1:])
+                if arg_idx in self.arg_to_binding:
+                    binding_idx = self.arg_to_binding[arg_idx]
+                    buf_name = f'buf{binding_idx}'
+                    if binding_idx < len(self.buffer_bindings):
+                        buf_elem = self.buffer_bindings[binding_idx].elem_type
+                        val_expr = self._operand(val, buf_elem)
+                        # Convert bool to numeric if needed
+                        if val in self.values and self.values[val][1] == 'bool':
+                            if buf_elem == 'f32':
+                                val_expr = f'select(f32(0), f32(1), {self._operand(val, "bool")})'
+                            else:
+                                val_expr = f'select(0, 1, {self._operand(val, "bool")})'
+                    if mask_cond:
+                        cond_expr = self._operand(mask_cond, 'bool')
+                        return f'if {cond_expr} {{ {buf_name}[0u] = {val_expr}; }}'
+                    return f'if _lid.x == 0u {{ {buf_name}[0u] = {val_expr}; }}'
+            except ValueError:
+                pass
+
+        return f'// store to unknown ptr {ptr}'
+
+    # -------------------------------------------------------------------
+    # Atomic RMW instruction handling
+    # -------------------------------------------------------------------
+
+    def _translate_atomicrmw(self, dst: str, op: str, ptr: str,
+                             val_type: str, val: str) -> str:
+        """Translate an atomicrmw instruction to WGSL atomic builtins.
+
+        LLVM: %old = atomicrmw OP ptr addrspace(1) %ptr, TYPE %val ...
+        WGSL: atomicAdd(&buf_atomic[idx], val), etc.
+
+        WGSL atomic builtins (i32/u32 only):
+          atomicAdd, atomicSub, atomicMax, atomicMin,
+          atomicAnd, atomicOr, atomicXor, atomicExchange,
+          atomicCompareExchangeWeak, atomicLoad, atomicStore
+        """
+        is_float = val_type in ('float', 'half', 'double')
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(val_type, 'i32')
+        val_expr = self._operand(val, wgsl_type)
+
+        # Resolve the buffer and index from GEP info
+        if ptr not in self.gep_info:
+            self.values[dst] = (self._var(dst), wgsl_type)
+            return f'let {self._var(dst)}: {wgsl_type} = {self._zero_val(wgsl_type)}; // atomicrmw unknown ptr'
+
+        gep = self.gep_info[ptr]
+        binding_idx = self.arg_to_binding.get(gep.buffer_arg_idx, 0)
+        buf_name = f'buf{binding_idx}'
+        idx_expr = gep.offset_expr
+
+        # WGSL atomicrmw op mapping (integer ops → direct WGSL builtins)
+        ATOMIC_OP_MAP = {
+            'add': 'atomicAdd',
+            'sub': 'atomicSub',
+            'max': 'atomicMax',
+            'min': 'atomicMin',
+            'umax': 'atomicMax',  # unsigned max — use u32 cast
+            'umin': 'atomicMin',  # unsigned min — use u32 cast
+            'and': 'atomicAnd',
+            'or': 'atomicOr',
+            'xor': 'atomicXor',
+            'xchg': 'atomicExchange',
+        }
+
+        if is_float and op == 'fadd':
+            # Float atomic add: emulate with CAS loop
+            # bitcast f32 ↔ i32 and use atomicCompareExchangeWeak
+            self.values[dst] = (self._var(dst), wgsl_type)
+            old_var = self._var(dst) + '_old'
+            new_var = self._var(dst) + '_new'
+            cas_var = self._var(dst) + '_cas'
+            return (
+                f'var {old_var}: i32 = atomicLoad(&{buf_name}[u32({idx_expr})]);\n'
+                f'    loop {{\n'
+                f'        let {new_var}: i32 = bitcast<i32>(bitcast<f32>({old_var}) + {val_expr});\n'
+                f'        let {cas_var} = atomicCompareExchangeWeak(&{buf_name}[u32({idx_expr})], {old_var}, {new_var});\n'
+                f'        if {cas_var}.exchanged {{ break; }}\n'
+                f'        {old_var} = {cas_var}.old_value;\n'
+                f'    }}\n'
+                f'    let {self._var(dst)}: {wgsl_type} = bitcast<f32>({old_var});'
+            )
+        elif is_float and op in ('max', 'min', 'umax', 'umin'):
+            # Float atomic max/min: emulate with CAS loop using integer comparison
+            # Triton bitcasts floats to ints for these; use the integer path
+            cmp_op = 'max' if op in ('max', 'umax') else 'min'
+            wgsl_fn = f'atomicMax' if cmp_op == 'max' else f'atomicMin'
+            self.values[dst] = (self._var(dst), 'i32')
+            return (f'let {self._var(dst)}: i32 = '
+                    f'{wgsl_fn}(&{buf_name}[u32({idx_expr})], {val_expr});')
+        elif op in ATOMIC_OP_MAP:
+            # Direct integer atomic operation
+            wgsl_fn = ATOMIC_OP_MAP[op]
+            self.values[dst] = (self._var(dst), 'i32')
+            int_val = val_expr
+            # Ensure the value is i32 for the atomic builtin
+            if wgsl_type != 'i32':
+                int_val = f'i32({val_expr})'
+            return (f'let {self._var(dst)}: i32 = '
+                    f'{wgsl_fn}(&{buf_name}[u32({idx_expr})], {int_val});')
+        else:
+            # Unknown atomic op — fallback
+            self.values[dst] = (self._var(dst), 'i32')
+            return f'let {self._var(dst)}: i32 = 0; // unsupported atomicrmw {op}'
+
+    # -------------------------------------------------------------------
+    # Cast instruction handling
+    # -------------------------------------------------------------------
+
+    def _translate_cast(self, dst: str, cast_op: str, src_type: str,
+                        src_val: str, dst_type: str) -> str:
+        """Translate LLVM cast instructions (fptosi, sitofp, zext, etc.)."""
+        src_wgsl = LLVM_TYPE_TO_WGSL.get(src_type, 'i32')
+        dst_wgsl = LLVM_TYPE_TO_WGSL.get(dst_type, 'i32')
+        a = self._operand(src_val, src_wgsl)
+
+        if cast_op == 'sitofp':
+            float_ty = dst_wgsl if dst_wgsl in ('f32', 'f16') else 'f32'
+            expr = f'{float_ty}({a})'
+            wgsl_type = float_ty
+        elif cast_op == 'fptosi':
+            expr = f'i32({a})'
+            wgsl_type = 'i32'
+        elif cast_op == 'uitofp':
+            float_ty = dst_wgsl if dst_wgsl in ('f32', 'f16') else 'f32'
+            expr = f'{float_ty}(u32({a}))'
+            wgsl_type = float_ty
+        elif cast_op == 'fptoui':
+            expr = f'u32({a})'
+            wgsl_type = 'u32'
+        elif cast_op == 'zext':
+            # zext i1 → i32: convert bool to int
+            if src_wgsl == 'bool' or src_type == 'i1':
+                expr = f'select(0, 1, {self._operand(src_val, "bool")})'
+            else:
+                expr = f'i32(u32({a}))'
+            wgsl_type = dst_wgsl if dst_wgsl != 'bool' else 'i32'
+        elif cast_op == 'sext':
+            expr = f'i32({a})'
+            wgsl_type = dst_wgsl if dst_wgsl != 'bool' else 'i32'
+        elif cast_op == 'trunc':
+            expr = f'{dst_wgsl}({a})'
+            wgsl_type = dst_wgsl if dst_wgsl != 'bool' else 'i32'
+        elif cast_op in ('fpext', 'fptrunc'):
+            expr = f'{dst_wgsl}({a})'
+            wgsl_type = dst_wgsl
+        elif cast_op == 'bitcast':
+            if src_wgsl == 'f32' and dst_wgsl == 'i32':
+                expr = f'bitcast<i32>({a})'
+            elif src_wgsl == 'i32' and dst_wgsl == 'f32':
+                expr = f'bitcast<f32>({a})'
+            else:
+                expr = a
+            wgsl_type = dst_wgsl
+        else:
+            expr = f'0 /* unknown cast: {cast_op} */'
+            wgsl_type = dst_wgsl
+
+        self.values[dst] = (self._var(dst), wgsl_type)
+        # Propagate uniformity through casts
+        if self._is_operand_uniform(src_val):
+            self.uniform_values.add(dst)
+        return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+
+    # -------------------------------------------------------------------
+    # LLVM intrinsic handling
+    # -------------------------------------------------------------------
+
+    def _translate_llvm_intrinsic(self, dst: str, ret_type: str,
+                                  func_name: str, args_str: str) -> Optional[str]:
+        """Translate known LLVM intrinsic calls to WGSL builtins.
+
+        Returns the translated WGSL statement or None if the intrinsic
+        is not recognized.
+        """
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(ret_type, 'f32')
+
+        # Parse arguments: "float %val" or "float %a, float %b"
+        arg_vals = []
+        if args_str.strip():
+            arg_parts = [a.strip() for a in args_str.split(',')]
+            for part in arg_parts:
+                tokens = part.strip().split()
+                if len(tokens) >= 2:
+                    arg_vals.append(self._operand(tokens[-1], wgsl_type))
+                elif len(tokens) == 1:
+                    arg_vals.append(self._operand(tokens[0], wgsl_type))
+
+        # Strip type suffix(es): llvm.fabs.f32 → llvm.fabs
+        base_name = func_name
+        # Try progressively stripping dot-separated suffix
+        for _ in range(3):
+            # Check unary
+            for prefix, wgsl_fn in LLVM_UNARY_INTRINSICS.items():
+                if base_name == prefix or base_name.startswith(prefix + '.'):
+                    if arg_vals:
+                        expr = f'{wgsl_fn}({arg_vals[0]})'
+                        self.values[dst] = (self._var(dst), wgsl_type)
+                        return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+
+            # Check binary
+            for prefix, wgsl_fn in LLVM_BINARY_INTRINSICS.items():
+                if base_name == prefix or base_name.startswith(prefix + '.'):
+                    if len(arg_vals) >= 2:
+                        expr = f'{wgsl_fn}({arg_vals[0]}, {arg_vals[1]})'
+                        self.values[dst] = (self._var(dst), wgsl_type)
+                        return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+
+            # Check ternary (fma, fmuladd)
+            for prefix, wgsl_fn in LLVM_TERNARY_INTRINSICS.items():
+                if base_name == prefix or base_name.startswith(prefix + '.'):
+                    if len(arg_vals) >= 3:
+                        expr = f'{wgsl_fn}({arg_vals[0]}, {arg_vals[1]}, {arg_vals[2]})'
+                        self.values[dst] = (self._var(dst), wgsl_type)
+                        return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+
+            # Strip one trailing suffix segment
+            last_dot = base_name.rfind('.')
+            if last_dot > 0:
+                base_name = base_name[:last_dot]
+            else:
+                break
+
+        return None  # Not a recognized intrinsic
+
+    def _translate_select(self, dst: str, cond: str, true_val: str,
+                          false_val: str, ty: str) -> str:
+        """Translate a select instruction."""
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(ty, 'f32')
+        cond_expr = self._operand(cond, 'bool')
+        true_expr = self._operand(true_val, wgsl_type)
+        false_expr = self._operand(false_val, wgsl_type)
+
+        # Track masked select: select i1 %cond, T %val, T undef
+        # This pattern means "val if cond, else don't care" — used for masked stores
+        if false_val.strip() == 'undef':
+            self.masked_values[dst] = cond
+
+        # WGSL select(false_val, true_val, cond) — reversed from LLVM
+        self.values[dst] = (self._var(dst), wgsl_type)
+        # Propagate uniformity through select
+        if (self._is_operand_uniform(cond) and
+            self._is_operand_uniform(true_val) and
+            self._is_operand_uniform(false_val)):
+            self.uniform_values.add(dst)
+        return f'let {self._var(dst)}: {wgsl_type} = select({false_expr}, {true_expr}, {cond_expr});'
+
+    # -------------------------------------------------------------------
+    # Binary operation handling
+    # -------------------------------------------------------------------
+
+    def _translate_binop(self, dst: str, op: str, ty: str,
+                         op1: str, op2: str) -> Optional[str]:
+        """Translate a binary operation."""
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(ty, 'i32')
+
+        # Comparison operations: icmp, fcmp
+        if op == 'icmp' or op == 'fcmp':
+            return self._translate_cmp(dst, op, ty, op1, op2)
+
+        # The type for the operands
+        a = self._operand(op1, wgsl_type)
+        b = self._operand(op2, wgsl_type)
+
+        # Integer arithmetic
+        if op == 'add':
+            expr = f'{a} + {b}'
+        elif op == 'sub':
+            expr = f'{a} - {b}'
+        elif op == 'mul':
+            expr = f'{a} * {b}'
+        elif op == 'sdiv':
+            expr = f'{a} / {b}'
+        elif op == 'srem':
+            expr = f'{a} % {b}'
+        # Unsigned arithmetic (need u32 operations)
+        elif op == 'udiv':
+            expr = f'i32(u32({a}) / u32({b}))'
+        elif op == 'urem':
+            expr = f'i32(u32({a}) % u32({b}))'
+        # Bitwise
+        elif op == 'and':
+            expr = f'{a} & {b}'
+        elif op == 'or':
+            expr = f'{a} | {b}'
+        elif op == 'xor':
+            expr = f'{a} ^ {b}'
+        # Shifts (shift amount must be u32 in WGSL)
+        elif op == 'shl':
+            expr = f'{a} << u32({b})'
+        elif op == 'lshr':
+            expr = f'i32(u32({a}) >> u32({b}))'
+        elif op == 'ashr':
+            expr = f'{a} >> u32({b})'
+        # Float arithmetic
+        elif op == 'fadd':
+            expr = f'{a} + {b}'
+        elif op == 'fsub':
+            expr = f'{a} - {b}'
+        elif op == 'fmul':
+            expr = f'{a} * {b}'
+        elif op == 'fdiv':
+            expr = f'{a} / {b}'
+        elif op == 'frem':
+            expr = f'{a} % {b}'
+        # Float unary (fneg is sometimes encoded as fsub 0, x)
+        elif op == 'fneg':
+            expr = f'-{a}'
+        # Type conversions
+        elif op == 'sitofp':
+            expr = f'f32({a})'
+            wgsl_type = 'f32'
+        elif op == 'fptosi':
+            expr = f'i32({a})'
+            wgsl_type = 'i32'
+        elif op == 'uitofp':
+            expr = f'f32(u32({a}))'
+            wgsl_type = 'f32'
+        elif op == 'fptoui':
+            expr = f'u32({a})'
+            wgsl_type = 'u32'
+        elif op == 'fpext':
+            expr = f'f32({a})'
+            wgsl_type = 'f32'
+        elif op == 'fptrunc':
+            expr = f'f16({a})'
+            wgsl_type = 'f16'
+        elif op == 'sext':
+            expr = f'i32({a})'
+            wgsl_type = 'i32'
+        elif op == 'zext':
+            expr = f'i32(u32({a}))'
+            wgsl_type = 'i32'
+        elif op == 'trunc':
+            expr = f'i32({a})'
+            wgsl_type = 'i32'
+        elif op == 'bitcast':
+            expr = a
+        else:
+            expr = f'0 /* unknown op: {op} */'
+
+        self.values[dst] = (self._var(dst), wgsl_type)
+        # Propagate uniformity: if both operands are uniform, result is uniform
+        if self._is_operand_uniform(op1) and self._is_operand_uniform(op2):
+            self.uniform_values.add(dst)
+        return f'let {self._var(dst)}: {wgsl_type} = {expr};'
+
+    def _translate_cmp(self, dst: str, op: str, pred_and_ty: str,
+                       op1: str, op2: str) -> str:
+        """Translate comparison instruction."""
+        # For icmp: pred_and_ty is the predicate (e.g., 'slt'), op1 is type+val
+        # Actually, our regex captured: op='icmp', ty=predicate, op1=type, op2=val1,val2
+        # But the format is: icmp slt i32 %a, %b
+        # After our regex: op='icmp', ty='slt', op1='i32', op2='%a, %b'
+        # Wait, let me re-examine...
+
+        # The regex matched: (\w+)\s*(flags)*(\S+)\s+(.+),\s*(.+)
+        # For "icmp slt i32 %26, %3":
+        #   op = 'icmp'
+        #   ty = 'slt' (the predicate, captured as "type")
+        #   op1 = 'i32 %26' (the actual type + first operand)
+        #   op2 = '%3' (second operand)
+
+        pred = pred_and_ty  # This is actually the predicate (slt, sge, etc.)
+
+        # Parse op1 to extract the actual type and first operand
+        parts = op1.strip().split()
+        if len(parts) >= 2:
+            actual_type = parts[0]
+            first_operand = parts[1]
+        else:
+            actual_type = 'i32'
+            first_operand = op1
+
+        wgsl_type = LLVM_TYPE_TO_WGSL.get(actual_type, 'i32')
+        a = self._operand(first_operand, wgsl_type)
+        b = self._operand(op2.strip(), wgsl_type)
+
+        # Signed comparisons
+        cmp_ops = {
+            'eq': '==', 'ne': '!=',
+            'slt': '<', 'sle': '<=', 'sgt': '>', 'sge': '>=',
+            'ult': '<', 'ule': '<=', 'ugt': '>', 'uge': '>=',
+            # Float predicates
+            'oeq': '==', 'one': '!=', 'ogt': '>', 'oge': '>=',
+            'olt': '<', 'ole': '<=',
+            'ueq': '==', 'une': '!=',
+        }
+
+        cmp_op = cmp_ops.get(pred, '==')
+
+        # For unsigned comparisons, convert to u32
+        if pred in ('ult', 'ule', 'ugt', 'uge') and wgsl_type == 'i32':
+            a = f'u32({a})'
+            b = f'u32({b})'
+
+        self.values[dst] = (self._var(dst), 'bool')
+        # Propagate uniformity through comparisons
+        if self._is_operand_uniform(first_operand) and self._is_operand_uniform(op2.strip()):
+            self.uniform_values.add(dst)
+        return f'let {self._var(dst)}: bool = {a} {cmp_op} {b};'
+
+    # -------------------------------------------------------------------
+    # Helper: Convert LLVM operand to WGSL expression
+    # -------------------------------------------------------------------
+
+    def _operand(self, op: str, expected_type: str = 'i32') -> str:
+        """Convert an LLVM operand to a WGSL expression."""
+        op = op.strip()
+
+        # undef / zeroinitializer → zero value
+        if op == 'undef' or op == 'zeroinitializer':
+            return self._zero_val(expected_type)
+
+        # SSA value reference
+        if op.startswith('%'):
+            # Check if it's a function argument
+            try:
+                arg_idx = int(op[1:])
+                if arg_idx < len(self.func_args):
+                    # Is this a scalar arg?
+                    if arg_idx in self.arg_to_param:
+                        param_name = self.arg_to_param[arg_idx]
+                        return f'params.{param_name}'
+                    # Is this a pointer arg? (shouldn't be used directly as a value)
+                    if arg_idx in self.arg_to_binding:
+                        return f'0 /* ptr arg {arg_idx} */'
+            except ValueError:
+                pass
+
+            # Previously computed SSA value
+            if op in self.values:
+                return self.values[op][0]
+            return self._var(op)
+
+        # Integer literal
+        if re.match(r'^-?\d+$', op):
+            val = int(op)
+            if expected_type in ('f32', 'f16'):
+                return f'{expected_type}({val})'
+            if expected_type == 'bool':
+                return 'true' if val != 0 else 'false'
+            return str(val)
+
+        # Float literal
+        if re.match(r'^-?[\d.]+(?:e[+-]?\d+)?$', op):
+            val = float(op)
+            if expected_type in ('f32', 'f16'):
+                return f'{expected_type}({val})'
+            return str(val)
+
+        # Hex literal — convert LLVM hex double to float value
+        if op.startswith('0x'):
+            try:
+                # LLVM represents float constants as the hex encoding
+                # of a double-precision value
+                int_val = int(op, 16)
+                double_val = struct.unpack('d', struct.pack('Q', int_val))[0]
+                if expected_type in ('f32', 'f16'):
+                    return f'{expected_type}({double_val})'
+                return str(double_val)
+            except (ValueError, struct.error):
+                return op
+
+        return f'0 /* unknown operand: {op} */'
+
+    def _var(self, ssa_name: str) -> str:
+        """Convert SSA name like %7 to WGSL variable name like v7."""
+        if ssa_name.startswith('%'):
+            return f'v{ssa_name[1:]}'
+        return f'v_{ssa_name}'
+
+    def _zero_val(self, wgsl_type: str) -> str:
+        """Return the zero value for a WGSL type."""
+        if wgsl_type == 'f32':
+            return 'f32(0)'
+        if wgsl_type == 'f16':
+            return 'f16(0)'
+        if wgsl_type == 'bool':
+            return 'false'
+        if wgsl_type == 'u32':
+            return 'u32(0)'
+        return '0'
+
+    # -------------------------------------------------------------------
+    # Step 8: Emit WGSL code
+    # -------------------------------------------------------------------
+
+    def _emit_wgsl(self, stmts: List[str]) -> str:
+        """Generate the complete WGSL shader code."""
+        lines = []
+
+        # Enable extensions if needed
+        if self.needs_f16:
+            lines.append('enable f16;')
+        if self.needs_subgroups:
+            lines.append('enable subgroups;')
+        if self.needs_f16 or self.needs_subgroups:
+            lines.append('')
+
+        lines.append(f'// Auto-generated by Triton WebGPU Backend')
+        lines.append(f'// Kernel: {self.kernel_name}')
+        lines.append(f'// Workgroup size: {self.workgroup_size}'
+                     f' ({self.num_warps} warps x {self.warp_size} threads)')
+        lines.append('')
+
+        # Shuffle scratch buffer for SubgroupShuffleXor emulation
+        if self.needs_shuffle_scratch:
+            lines.append(f'var<workgroup> _shfl: array<i32, {self.workgroup_size}>;')
+            lines.append('')
+
+        # Shared memory for cross-warp communication (addrspace(3))
+        if self.needs_smem:
+            smem_i32_count = max(self.num_warps, (self.smem_bytes + 3) // 4)
+            lines.append(f'var<workgroup> _smem: array<i32, {smem_i32_count}>;')
+            lines.append('')
+
+        # Buffer declarations
+        # Build reverse map: binding_idx → func_arg_idx for atomic detection
+        binding_to_arg = {}
+        for arg_idx, bind_idx in self.arg_to_binding.items():
+            binding_to_arg[bind_idx] = arg_idx
+
+        for bb in self.buffer_bindings:
+            access = 'read_write' if bb.access == 'read_write' else 'read'
+            arg_idx = binding_to_arg.get(bb.binding, -1)
+            is_atomic = arg_idx in self.atomic_buffers
+            if is_atomic:
+                # Atomic buffers use atomic<i32> element type
+                lines.append(f'@group(0) @binding({bb.binding}) '
+                            f'var<storage, {access}> buf{bb.binding}: '
+                            f'array<atomic<i32>>;  // {bb.name} (atomic)')
+            else:
+                lines.append(f'@group(0) @binding({bb.binding}) '
+                            f'var<storage, {access}> buf{bb.binding}: '
+                            f'array<{bb.elem_type}>;  // {bb.name}')
+        lines.append('')
+
+        # Params struct (scalar arguments)
+        if self.param_fields:
+            lines.append('struct Params {')
+            for pf in self.param_fields:
+                lines.append(f'    {pf.name}: {pf.wgsl_type},')
+            lines.append('};')
+            param_binding = len(self.buffer_bindings)
+            lines.append(f'@group(0) @binding({param_binding}) '
+                        f'var<storage, read> params: Params;')
+            lines.append('')
+
+        # Compute entry point
+        lines.append(f'@compute @workgroup_size({self.workgroup_size})')
+        lines.append('fn main(')
+        lines.append('    @builtin(workgroup_id) _wg_id: vec3<u32>,')
+        lines.append('    @builtin(local_invocation_id) _lid: vec3<u32>,')
+        lines.append('    @builtin(num_workgroups) _num_wg: vec3<u32>,')
+        lines.append(') {')
+
+        # Body
+        for stmt in stmts:
+            if stmt:
+                lines.append(f'    {stmt}')
+
+        lines.append('}')
+        lines.append('')
+
+        return '\n'.join(lines)
+
+
+# ---------------------------------------------------------------------------
+# Public API
+# ---------------------------------------------------------------------------
+
+def translate_llvm_to_wgsl(llir: str, signature: dict,
+                           num_warps: int = 4,
+                           warp_size: int = 32,
+                           use_native_subgroups: bool = False) -> TranslationResult:
+    """
+    Translate LLVM IR from Triton's WebGPU backend to WGSL.
+
+    Args:
+        llir: LLVM IR string
+        signature: Triton parameter signature dict (excluding constexprs)
+        num_warps: Warps per workgroup
+        warp_size: Threads per warp
+        use_native_subgroups: Use native subgroupShuffleXor if adapter supports it
+
+    Returns:
+        TranslationResult with WGSL code, bindings, and metadata
+    """
+    translator = LLVMToWGSL(llir, signature, num_warps, warp_size,
+                             use_native_subgroups=use_native_subgroups)
+    return translator.translate()
diff --git a/third_party/webgpu/backend/wgpu_runner.py b/third_party/webgpu/backend/wgpu_runner.py
new file mode 100644
index 000000000..db98182e8
--- /dev/null
+++ b/third_party/webgpu/backend/wgpu_runner.py
@@ -0,0 +1,300 @@
+"""
+WebGPU Runtime Kernel Runner
+==============================
+
+Executes WGSL compute shaders on the GPU via wgpu-py (wgpu-native/Vulkan).
+
+Usage:
+    runner = WebGPURunner()
+    result = runner.run_kernel(
+        wgsl_code=wgsl,
+        buffer_bindings=[...],
+        param_fields=[...],
+        workgroup_size=128,
+        grid=(num_workgroups,),
+        buffers={'x_ptr': x_np, 'y_ptr': y_np, 'output_ptr': output_np},
+        scalars={'n_elements': n},
+    )
+    output = result['output_ptr']  # numpy array
+"""
+
+import struct
+import numpy as np
+
+try:
+    import wgpu
+    import wgpu.utils
+    HAS_WGPU = True
+except ImportError:
+    HAS_WGPU = False
+
+from .llvm_to_wgsl import BufferBinding, ParamField
+
+
+WGSL_TYPE_TO_NUMPY = {
+    'f32': np.float32,
+    'f16': np.float16,
+    'i32': np.int32,
+    'u32': np.uint32,
+}
+
+WGSL_TYPE_TO_STRUCT_FMT = {
+    'f32': '<f',
+    'i32': '<i',
+    'u32': '<I',
+}
+
+
+class WebGPURunner:
+    """Execute WGSL compute shaders on the GPU via wgpu-py."""
+
+    def __init__(self, adapter=None, device=None):
+        if not HAS_WGPU:
+            raise RuntimeError("wgpu-py is not installed. Run: pip install wgpu")
+
+        if device is not None:
+            self._adapter = adapter
+            self._device = device
+        else:
+            self._adapter = wgpu.gpu.request_adapter_sync(
+                power_preference="high-performance"
+            )
+            self._device = self._adapter.request_device_sync(
+                required_limits=self._get_limits()
+            )
+
+    def _get_limits(self) -> dict:
+        """Request generous device limits for compute."""
+        return {
+            "max-bind-groups": 4,
+            "max-storage-buffers-per-shader-stage": 16,
+            "max-storage-buffer-binding-size": 1 << 30,  # 1 GiB
+            "max-buffer-size": 1 << 30,
+            "max-compute-workgroups-per-dimension": 65535,
+            "max-compute-invocations-per-workgroup": 256,
+            "max-compute-workgroup-size-x": 256,
+        }
+
+    @property
+    def adapter_info(self) -> str:
+        """Return GPU adapter description."""
+        info = self._adapter.info
+        return f"{info.get('device', 'unknown')} ({info.get('backend_type', '?')})"
+
+    def run_kernel(
+        self,
+        wgsl_code: str,
+        buffer_bindings: list,
+        param_fields: list,
+        workgroup_size: int,
+        grid: tuple,
+        buffers: dict,
+        scalars: dict = None,
+    ) -> dict:
+        """
+        Execute a WGSL compute shader.
+
+        Args:
+            wgsl_code: Complete WGSL shader source
+            buffer_bindings: List of BufferBinding (from translator)
+            param_fields: List of ParamField (from translator)
+            workgroup_size: Threads per workgroup
+            grid: Tuple of (num_workgroups_x, [y, [z]])
+            buffers: Dict mapping buffer name → numpy array (input/output data)
+            scalars: Dict mapping scalar param name → int/float value
+
+        Returns:
+            Dict mapping output buffer names → numpy arrays with results
+        """
+        scalars = scalars or {}
+        device = self._device
+
+        # 1. Create the shader module
+        shader = device.create_shader_module(code=wgsl_code)
+
+        # 2. Create GPU buffers for each binding
+        gpu_buffers = {}
+        binding_entries = []
+
+        for bb in buffer_bindings:
+            if bb.name in buffers:
+                # User-provided buffer
+                np_arr = np.ascontiguousarray(buffers[bb.name])
+                buf_size = np_arr.nbytes
+                usage = wgpu.BufferUsage.STORAGE | wgpu.BufferUsage.COPY_SRC
+                if bb.access == 'read':
+                    usage |= wgpu.BufferUsage.COPY_DST
+                else:
+                    usage |= wgpu.BufferUsage.COPY_DST
+
+                gpu_buf = device.create_buffer(size=buf_size, usage=usage)
+                device.queue.write_buffer(gpu_buf, 0, np_arr.tobytes())
+            else:
+                # Internal/unused buffer — create a dummy 16-byte buffer
+                buf_size = 16
+                usage = wgpu.BufferUsage.STORAGE | wgpu.BufferUsage.COPY_SRC
+                gpu_buf = device.create_buffer(size=buf_size, usage=usage)
+
+            gpu_buffers[bb.name] = gpu_buf
+
+            binding_entries.append({
+                "binding": bb.binding,
+                "resource": {
+                    "buffer": gpu_buf,
+                    "offset": 0,
+                    "size": gpu_buf.size,
+                },
+            })
+
+        # 3. Create params buffer (scalar arguments)
+        if param_fields:
+            params_data = bytearray()
+            for pf in param_fields:
+                val = scalars.get(pf.name, 0)
+                fmt = WGSL_TYPE_TO_STRUCT_FMT.get(pf.wgsl_type, '<i')
+                params_data.extend(struct.pack(fmt, val))
+
+            # Pad to minimum buffer size (16 bytes)
+            while len(params_data) < 16:
+                params_data.extend(b'\x00')
+
+            params_binding = len(buffer_bindings)
+            params_buf = device.create_buffer(
+                size=len(params_data),
+                usage=wgpu.BufferUsage.STORAGE | wgpu.BufferUsage.COPY_DST,
+            )
+            device.queue.write_buffer(params_buf, 0, bytes(params_data))
+
+            binding_entries.append({
+                "binding": params_binding,
+                "resource": {
+                    "buffer": params_buf,
+                    "offset": 0,
+                    "size": params_buf.size,
+                },
+            })
+
+        # 4. Create bind group layout and bind group
+        bind_group_layout_entries = []
+        for i, entry in enumerate(binding_entries):
+            is_params = (i == len(buffer_bindings) and param_fields)
+            if is_params:
+                buf_type = "read-only-storage"
+            else:
+                bb = buffer_bindings[i] if i < len(buffer_bindings) else None
+                if bb and bb.access == 'read':
+                    buf_type = "read-only-storage"
+                else:
+                    buf_type = "storage"
+
+            bind_group_layout_entries.append({
+                "binding": entry["binding"],
+                "visibility": wgpu.ShaderStage.COMPUTE,
+                "buffer": {"type": buf_type},
+            })
+
+        bind_group_layout = device.create_bind_group_layout(
+            entries=bind_group_layout_entries
+        )
+
+        bind_group = device.create_bind_group(
+            layout=bind_group_layout,
+            entries=binding_entries,
+        )
+
+        # 5. Create compute pipeline
+        pipeline_layout = device.create_pipeline_layout(
+            bind_group_layouts=[bind_group_layout]
+        )
+
+        pipeline = device.create_compute_pipeline(
+            layout=pipeline_layout,
+            compute={"module": shader, "entry_point": "main"},
+        )
+
+        # 6. Encode and submit commands
+        command_encoder = device.create_command_encoder()
+        compute_pass = command_encoder.begin_compute_pass()
+        compute_pass.set_pipeline(pipeline)
+        compute_pass.set_bind_group(0, bind_group)
+
+        # Grid dimensions
+        gx = grid[0] if len(grid) > 0 else 1
+        gy = grid[1] if len(grid) > 1 else 1
+        gz = grid[2] if len(grid) > 2 else 1
+        compute_pass.dispatch_workgroups(gx, gy, gz)
+        compute_pass.end()
+
+        device.queue.submit([command_encoder.finish()])
+
+        # 7. Read back output buffers
+        results = {}
+        for bb in buffer_bindings:
+            if bb.name in buffers and bb.access == 'read_write':
+                gpu_buf = gpu_buffers[bb.name]
+                raw = device.queue.read_buffer(gpu_buf)
+                np_dtype = WGSL_TYPE_TO_NUMPY.get(bb.elem_type, np.float32)
+                results[bb.name] = np.frombuffer(raw, dtype=np_dtype).copy()
+
+        return results
+
+
+def run_triton_kernel_on_webgpu(
+    compiled_kernel,
+    grid: tuple,
+    **kwargs,
+) -> dict:
+    """
+    High-level API: Execute a compiled Triton WebGPU kernel on the GPU.
+
+    Args:
+        compiled_kernel: Result of triton.compile() for WebGPU target
+        grid: Tuple of workgroup counts (x,) or (x, y) or (x, y, z)
+        **kwargs: Named arguments matching the kernel signature.
+                  Pointer args should be numpy arrays.
+                  Scalar args should be int/float.
+
+    Returns:
+        Dict mapping output buffer names → numpy arrays with GPU results
+    """
+    from .llvm_to_wgsl import translate_llvm_to_wgsl
+
+    # Get the LLVM IR from compilation
+    llir = compiled_kernel.asm.get('llir', '')
+    if not llir:
+        raise ValueError("Compiled kernel has no LLVM IR. Was it compiled for WebGPU?")
+
+    # Get metadata
+    metadata = compiled_kernel.metadata
+    sig = {}
+    # Build signature from metadata
+    # The signature info may be in different places depending on Triton version
+    if hasattr(compiled_kernel, 'signature'):
+        sig = compiled_kernel.signature
+
+    num_warps = metadata.get('num_warps', 4)
+    warp_size = 32
+
+    # Translate to WGSL
+    result = translate_llvm_to_wgsl(llir, sig, num_warps, warp_size)
+
+    # Separate buffer vs scalar args
+    buffers = {}
+    scalars = {}
+    for name, val in kwargs.items():
+        if isinstance(val, np.ndarray):
+            buffers[name] = val
+        else:
+            scalars[name] = val
+
+    # Run
+    runner = WebGPURunner()
+    return runner.run_kernel(
+        wgsl_code=result.wgsl,
+        buffer_bindings=result.buffer_bindings,
+        param_fields=result.param_fields,
+        workgroup_size=result.workgroup_size,
+        grid=grid,
+        buffers=buffers,
+        scalars=scalars,
+    )
diff --git a/third_party/webgpu/lib/TritonWebGPUToLLVM/CMakeLists.txt b/third_party/webgpu/lib/TritonWebGPUToLLVM/CMakeLists.txt
new file mode 100644
index 000000000..50ad35990
--- /dev/null
+++ b/third_party/webgpu/lib/TritonWebGPUToLLVM/CMakeLists.txt
@@ -0,0 +1,12 @@
+add_triton_library(TritonWebGPUToLLVM
+    TargetInfo.cpp
+    TritonGPUToLLVM.cpp
+
+    LINK_LIBS PUBLIC
+    TritonAnalysis
+    TritonGPUToLLVM
+    MLIRIR
+    MLIRPass
+    MLIRGPUDialect
+    MLIRUBToLLVM
+)
diff --git a/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.cpp b/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.cpp
new file mode 100644
index 000000000..6de910cf7
--- /dev/null
+++ b/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.cpp
@@ -0,0 +1,345 @@
+// TargetInfo.cpp -- WebGPU/SPIR-V target info implementation
+//
+// Implements the TargetInfoBase interface for the WebGPU backend.
+// Operations that require SPIR-V-specific intrinsics are lowered to
+// external function calls (e.g., __spirv_ControlBarrier) that the
+// SPIR-V translator will recognize and convert to SPIR-V ops.
+
+#include "TargetInfo.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/GPU/IR/GPUDialect.h"
+#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
+#include "mlir/Dialect/LLVMIR/LLVMTypes.h"
+#include "triton/Conversion/TritonGPUToLLVM/Utility.h"
+#include "triton/Dialect/TritonGPU/IR/Dialect.h"
+
+using namespace mlir;
+using namespace mlir::LLVM;
+
+namespace mlir::triton::WebGPU {
+
+// ---------------------------------------------------------------------------
+// Helper: get or create an LLVM function declaration in the module
+// ---------------------------------------------------------------------------
+LLVM::LLVMFuncOp TargetInfo::getOrCreateFunction(
+    RewriterBase &rewriter, StringRef name,
+    LLVM::LLVMFunctionType fnType) const {
+  auto moduleOp =
+      rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
+  auto fn = moduleOp.lookupSymbol<LLVM::LLVMFuncOp>(name);
+  if (fn)
+    return fn;
+
+  OpBuilder::InsertionGuard guard(rewriter);
+  rewriter.setInsertionPointToStart(moduleOp.getBody());
+  fn = LLVM::LLVMFuncOp::create(rewriter, UnknownLoc::get(rewriter.getContext()),
+                                 name, fnType);
+  fn.setLinkage(LLVM::Linkage::External);
+  return fn;
+}
+
+// ---------------------------------------------------------------------------
+// Helper: emit a subgroup shuffle via SPIR-V function call
+// ---------------------------------------------------------------------------
+Value TargetInfo::emitShuffleCall(RewriterBase &rewriter, Location loc,
+                                  Value val, Value offset,
+                                  StringRef funcName) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  auto *ctx = rewriter.getContext();
+  Type valTy = val.getType();
+  unsigned bits = valTy.getIntOrFloatBitWidth();
+
+  // For 64-bit: split into two 32-bit values and shuffle independently
+  if (bits == 64) {
+    Type vecTy = vec_ty(f32_ty, 2);
+    Value vec = b.bitcast(val, vecTy);
+    Value val0 = b.extract_element(f32_ty, vec, b.i32_val(0));
+    Value val1 = b.extract_element(f32_ty, vec, b.i32_val(1));
+    val0 = emitShuffleCall(rewriter, loc, val0, offset, funcName);
+    val1 = emitShuffleCall(rewriter, loc, val1, offset, funcName);
+    vec = b.undef(vecTy);
+    vec = b.insert_element(vecTy, vec, val0, b.i32_val(0));
+    vec = b.insert_element(vecTy, vec, val1, b.i32_val(1));
+    return b.bitcast(vec, valTy);
+  }
+
+  // Promote to i32 for shuffle
+  Type origTy = valTy;
+  if (valTy != i32_ty) {
+    val = b.bitcast(val, int_ty(bits));
+    if (bits < 32)
+      val = b.zext(i32_ty, val);
+  }
+
+  // Create the SPIR-V subgroup shuffle function call
+  // Signature: i32 funcName(i32 scope, i32 val, i32 offset)
+  auto i32Type = IntegerType::get(ctx, 32);
+  auto fnTy = LLVM::LLVMFunctionType::get(i32Type, {i32Type, i32Type, i32Type});
+  auto fn = getOrCreateFunction(rewriter, funcName, fnTy);
+
+  // Subgroup scope = 3 in SPIR-V
+  SmallVector<Value> shuffleArgs = {b.i32_val(3), val, offset};
+  Value result = b.call(fn, shuffleArgs).getResult();
+
+  // Demote back to original type
+  if (origTy != i32_ty) {
+    if (bits < 32)
+      result = b.trunc(int_ty(bits), result);
+    result = b.bitcast(result, origTy);
+  }
+  return result;
+}
+
+// ---------------------------------------------------------------------------
+// TargetInfoBase overrides
+// ---------------------------------------------------------------------------
+
+bool TargetInfo::supportMaximumMinimum() const {
+  // No hardware max/min with NaN propagation
+  return false;
+}
+
+Value TargetInfo::getClusterCTAId(RewriterBase &rewriter,
+                                   Location loc) const {
+  // WebGPU doesn't support multi-CTA clusters
+  return LLVM::createConstantI32(loc, rewriter, 0);
+}
+
+Value TargetInfo::ballot(RewriterBase &rewriter, Location loc, Type type,
+                          Value cmp) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  auto *ctx = rewriter.getContext();
+  auto i32Ty = IntegerType::get(ctx, 32);
+  auto i1Ty = IntegerType::get(ctx, 1);
+
+  // __spirv_GroupNonUniformBallot(scope, predicate) -> <4 x i32>
+  // For simplicity, return all-ones when cmp is true, all-zeros when false
+  // This is a simplified implementation
+  auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty, i1Ty});
+  auto fn = getOrCreateFunction(rewriter, "__spirv_GroupNonUniformBallot", fnTy);
+  SmallVector<Value> ballotArgs = {b.i32_val(3), cmp};
+  Value result = b.call(fn, ballotArgs).getResult();
+
+  // Extend to the requested ballot type width
+  unsigned targetBits = type.getIntOrFloatBitWidth();
+  if (targetBits > 32)
+    result = b.zext(type, result);
+  else if (targetBits < 32)
+    result = b.trunc(type, result);
+  return result;
+}
+
+void TargetInfo::barrier(Location loc, RewriterBase &rewriter,
+                          triton::gpu::AddrSpace targets) const {
+  // Use the standard TritonGPU BarrierOp, which will be lowered in the
+  // conversion pass to a SPIR-V barrier function call
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  b.barrier(targets);
+}
+
+void TargetInfo::clusterBarrier(Location loc,
+                                 RewriterBase &rewriter) const {
+  // WebGPU doesn't have cluster barriers, use workgroup barrier
+  barrier(loc, rewriter, triton::gpu::AddrSpace::Local);
+}
+
+void TargetInfo::warpSync(Location loc, RewriterBase &rewriter) const {
+  // WebGPU doesn't have warp-level sync, use workgroup barrier
+  barrier(loc, rewriter, triton::gpu::AddrSpace::Local);
+}
+
+void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
+                               std::optional<Value> ctaId, Value val,
+                               Value pred) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  assert(!ctaId.has_value() &&
+         "WebGPU does not support cross-CTA shared memory");
+
+  // Predicated store: if (pred) *ptr = val
+  if (!isa<VectorType>(val.getType())) {
+    // Scalar store
+    b.store(val, ptr);
+    return;
+  }
+
+  // Vector store
+  auto vecTy = cast<VectorType>(val.getType());
+  unsigned vec = vecTy.getNumElements();
+  if (vec == 1) {
+    Value elem = b.extract_element(vecTy.getElementType(), val, b.i32_val(0));
+    b.store(elem, ptr);
+    return;
+  }
+
+  // Store each element individually
+  for (unsigned i = 0; i < vec; i++) {
+    Value elem = b.extract_element(vecTy.getElementType(), val, b.i32_val(i));
+    auto elemPtr = b.gep(ptr.getType(), vecTy.getElementType(), ptr,
+                         b.i32_val(i), LLVM::GEPNoWrapFlags::inbounds);
+    b.store(elem, elemPtr);
+  }
+}
+
+Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
+                               std::optional<Value> ctaId, Type loadTy,
+                               Value pred, Operation *localLoadOp) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  assert(!ctaId.has_value() &&
+         "WebGPU does not support cross-CTA shared memory");
+
+  if (!isa<VectorType>(loadTy)) {
+    return b.load(loadTy, ptr);
+  }
+
+  auto vecTy = cast<VectorType>(loadTy);
+  unsigned vec = vecTy.getNumElements();
+  Type elemTy = vecTy.getElementType();
+
+  if (vec == 1) {
+    Value elem = b.load(elemTy, ptr);
+    Value vec_val = b.undef(vecTy);
+    return b.insert_element(vecTy, vec_val, elem, b.i32_val(0));
+  }
+
+  // Load each element and pack into vector
+  Value result = b.undef(vecTy);
+  for (unsigned i = 0; i < vec; i++) {
+    auto elemPtr = b.gep(ptr.getType(), elemTy, ptr, b.i32_val(i),
+                         LLVM::GEPNoWrapFlags::inbounds);
+    Value elem = b.load(elemTy, elemPtr);
+    result = b.insert_element(vecTy, result, elem, b.i32_val(i));
+  }
+  return result;
+}
+
+Value TargetInfo::shuffleXor(RewriterBase &rewriter, Location loc, Value val,
+                              int i) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Type valTy = val.getType();
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    val = b.ptrtoint(i64_ty, val);
+  Value result = emitShuffleCall(rewriter, loc, val, b.i32_val(i),
+                                  "__spirv_SubgroupShuffleXor");
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    result = b.inttoptr(valTy, result);
+  return result;
+}
+
+Value TargetInfo::shuffleUp(RewriterBase &rewriter, Location loc, Value val,
+                             int i) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Type valTy = val.getType();
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    val = b.ptrtoint(i64_ty, val);
+  Value result = emitShuffleCall(rewriter, loc, val, b.i32_val(i),
+                                  "__spirv_SubgroupShuffleUp");
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    result = b.inttoptr(valTy, result);
+  return result;
+}
+
+Value TargetInfo::shuffleIdx(RewriterBase &rewriter, Location loc, Value val,
+                              int i) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Type valTy = val.getType();
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    val = b.ptrtoint(i64_ty, val);
+  Value result = emitShuffleCall(rewriter, loc, val, b.i32_val(i),
+                                  "__spirv_SubgroupShuffle");
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    result = b.inttoptr(valTy, result);
+  return result;
+}
+
+Value TargetInfo::shuffleIdx(RewriterBase &rewriter, Location loc, Value val,
+                              Value i) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Type valTy = val.getType();
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    val = b.ptrtoint(i64_ty, val);
+  Value result = emitShuffleCall(rewriter, loc, val, i,
+                                  "__spirv_SubgroupShuffle");
+  if (isa<LLVM::LLVMPointerType>(valTy))
+    result = b.inttoptr(valTy, result);
+  return result;
+}
+
+Value TargetInfo::permute(RewriterBase &rewriter, Location loc, Value a,
+                           Value b_, Value selector) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  // Emulate byte permute via shifts and masks
+  // This is a simplified version that handles the common case
+  auto *ctx = rewriter.getContext();
+  auto i32Ty = IntegerType::get(ctx, 32);
+  auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty, i32Ty, i32Ty});
+  auto fn = getOrCreateFunction(rewriter, "__spirv_BytePermute", fnTy);
+  SmallVector<Value> permuteArgs = {a, b_, selector};
+  return b.call(fn, permuteArgs).getResult();
+}
+
+Value TargetInfo::programId(RewriterBase &rewriter, Location loc,
+                             ModuleOp moduleOp, ProgramIDDim axis) const {
+  // Use GPU dialect's BlockIdOp then convert to i32
+  mlir::gpu::Dimension dim;
+  switch (axis) {
+  case ProgramIDDim::X:
+    dim = mlir::gpu::Dimension::x;
+    break;
+  case ProgramIDDim::Y:
+    dim = mlir::gpu::Dimension::y;
+    break;
+  case ProgramIDDim::Z:
+    dim = mlir::gpu::Dimension::z;
+    break;
+  }
+  Value blockId = mlir::gpu::BlockIdOp::create(rewriter, loc, dim);
+  // Convert index to i32
+  return arith::IndexCastOp::create(rewriter, loc, rewriter.getI32Type(),
+                                    blockId);
+}
+
+bool TargetInfo::warpReduce(RewriterBase &rewriter, Location loc,
+                             SmallVector<Value> &acc, triton::ReduceOp op,
+                             unsigned reduceLaneIdMask) const {
+  // No hardware warp reduce; fall back to generic tree reduction
+  return false;
+}
+
+std::string TargetInfo::getMulhiFuncName(Type resultElementTy) const {
+  return resultElementTy.isInteger(32) ? "__spirv_umulhi32" : "__spirv_umulhi64";
+}
+
+void TargetInfo::printf(RewriterBase &rewriter, Value formatStrStart,
+                         int formatStrByteCount, ValueRange args,
+                         ArrayRef<bool> isSigned) const {
+  // WebGPU/SPIR-V does not support device-side printf
+  // This is a no-op
+}
+
+void TargetInfo::printf(RewriterBase &rewriter, StringRef msg, ValueRange args,
+                         ArrayRef<bool> isSigned) const {
+  // No-op for WebGPU
+}
+
+void TargetInfo::assertFail(RewriterBase &rewriter, Location loc,
+                             StringRef message, StringRef file, StringRef func,
+                             int line) const {
+  // No-op for WebGPU (no device-side assert support)
+  // Could potentially write to a debug buffer in the future
+}
+
+int TargetInfo::getSharedAddressSpace() const {
+  // LLVM address space 3 = SPIR-V Workgroup memory
+  return 3;
+}
+
+int TargetInfo::getAddressSpace(Attribute addressSpace) const {
+  if (isa<triton::gpu::SharedMemorySpaceAttr>(addressSpace))
+    return 3; // Workgroup
+  llvm::report_fatal_error("Unsupported address space for WebGPU backend");
+  return 0;
+}
+
+bool TargetInfo::supportVectorizedAtomics() const { return false; }
+
+} // namespace mlir::triton::WebGPU
diff --git a/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.h b/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.h
new file mode 100644
index 000000000..ec5e7cc20
--- /dev/null
+++ b/third_party/webgpu/lib/TritonWebGPUToLLVM/TargetInfo.h
@@ -0,0 +1,90 @@
+#ifndef TRITON_CONVERSION_TRITONWEBGPU_TO_LLVM_TARGETINFO_H
+#define TRITON_CONVERSION_TRITONWEBGPU_TO_LLVM_TARGETINFO_H
+
+#include "triton/Conversion/TritonGPUToLLVM/TargetInfoBase.h"
+#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
+
+// Forward declarations
+namespace mlir::LLVM {
+class LLVMFuncOp;
+class LLVMFunctionType;
+} // namespace mlir::LLVM
+
+namespace mlir::triton::WebGPU {
+
+class TargetInfo : public mlir::triton::TargetInfoBase {
+public:
+  explicit TargetInfo(int warpSize = 32) : warpSize(warpSize) {}
+
+  bool supportMaximumMinimum() const override;
+
+  Value getClusterCTAId(RewriterBase &rewriter, Location loc) const override;
+
+  Value ballot(RewriterBase &rewriter, Location loc, Type type,
+               Value cmp) const override;
+
+  void barrier(Location loc, RewriterBase &rewriter,
+               triton::gpu::AddrSpace targets) const override;
+  void clusterBarrier(Location loc, RewriterBase &rewriter) const override;
+  void warpSync(Location loc, RewriterBase &rewriter) const override;
+
+  void storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
+                    std::optional<Value> ctaId, Value val,
+                    Value pred) const override;
+  Value loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
+                    std::optional<Value> ctaId, Type elemTy, Value pred,
+                    Operation *localLoadOp = nullptr) const override;
+
+  Value shuffleXor(RewriterBase &rewriter, Location loc, Value val,
+                   int i) const override;
+  Value shuffleUp(RewriterBase &rewriter, Location loc, Value val,
+                  int i) const override;
+  Value shuffleIdx(RewriterBase &rewriter, Location loc, Value val,
+                   int i) const override;
+  Value shuffleIdx(RewriterBase &rewriter, Location loc, Value val,
+                   Value i) const override;
+
+  Value permute(RewriterBase &rewriter, Location loc, Value a, Value b,
+                Value selector) const override;
+
+  Value programId(RewriterBase &rewriter, Location loc, ModuleOp moduleOp,
+                  ProgramIDDim axis) const override;
+
+  bool warpReduce(RewriterBase &rewriter, Location loc, SmallVector<Value> &acc,
+                  triton::ReduceOp op,
+                  unsigned reduceLaneIdMask) const override;
+
+  std::string getMulhiFuncName(Type resultElementTy) const override;
+
+  void printf(RewriterBase &rewriter, Value formatStrStart,
+              int formatStrByteCount, ValueRange args,
+              ArrayRef<bool> isSigned = {}) const override;
+
+  void printf(RewriterBase &rewriter, StringRef msg, ValueRange args,
+              ArrayRef<bool> isSigned = {}) const override;
+
+  void assertFail(RewriterBase &rewriter, Location loc, StringRef message,
+                  StringRef file, StringRef func, int line) const override;
+
+  int getSharedAddressSpace() const override;
+  int getAddressSpace(Attribute addressSpace) const override;
+  bool supportVectorizedAtomics() const override;
+
+  int getWarpSize() const { return warpSize; }
+
+private:
+  int warpSize;
+
+  // Helper: create or get an LLVM function declaration
+  LLVM::LLVMFuncOp getOrCreateFunction(RewriterBase &rewriter,
+                                        StringRef name,
+                                        LLVM::LLVMFunctionType fnType) const;
+
+  // Helper: emit a SPIR-V subgroup shuffle via function call
+  Value emitShuffleCall(RewriterBase &rewriter, Location loc, Value val,
+                        Value offset, StringRef funcName) const;
+};
+
+} // namespace mlir::triton::WebGPU
+
+#endif // TRITON_CONVERSION_TRITONWEBGPU_TO_LLVM_TARGETINFO_H
diff --git a/third_party/webgpu/lib/TritonWebGPUToLLVM/TritonGPUToLLVM.cpp b/third_party/webgpu/lib/TritonWebGPUToLLVM/TritonGPUToLLVM.cpp
new file mode 100644
index 000000000..8a720f60c
--- /dev/null
+++ b/third_party/webgpu/lib/TritonWebGPUToLLVM/TritonGPUToLLVM.cpp
@@ -0,0 +1,820 @@
+// TritonGPUToLLVM.cpp -- WebGPU/SPIR-V backend conversion pass
+//
+// Converts TritonGPU IR to LLVM IR targeting SPIR-V.
+// Reuses generic conversion patterns from lib/Conversion/TritonGPUToLLVM/
+// and adds WebGPU-specific patterns for load/store, barriers, and GPU ops.
+
+#include "TargetInfo.h"
+#include "triton/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVMBase.h"
+#include "mlir/Conversion/ArithToLLVM/ArithToLLVM.h"
+#include "mlir/Conversion/ControlFlowToLLVM/ControlFlowToLLVM.h"
+#include "mlir/Conversion/MathToLLVM/MathToLLVM.h"
+#include "mlir/Conversion/UBToLLVM/UBToLLVM.h"
+#include "mlir/Dialect/Arith/Transforms/Passes.h"
+#include "mlir/Dialect/ControlFlow/IR/ControlFlow.h"
+#include "mlir/Dialect/GPU/IR/GPUDialect.h"
+#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
+#include "mlir/Dialect/Math/IR/Math.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+
+#include "mlir/Pass/Pass.h"
+#include "triton/Analysis/Allocation.h"
+#include "triton/Analysis/AxisInfo.h"
+#include "triton/Analysis/Membar.h"
+#include "triton/Conversion/TritonGPUToLLVM/PatternTritonGPUOpToLLVM.h"
+#include "triton/Conversion/TritonGPUToLLVM/TypeConverter.h"
+#include "triton/Conversion/TritonGPUToLLVM/Utility.h"
+#include "triton/Dialect/Triton/IR/Dialect.h"
+#include "triton/Dialect/TritonGPU/IR/Dialect.h"
+#include "triton/Dialect/TritonNvidiaGPU/IR/Dialect.h"
+
+using namespace mlir;
+using namespace mlir::triton;
+namespace ttg = mlir::triton::gpu;
+namespace mgpu = ::mlir::gpu;  // Disambiguate from triton::gpu
+
+// ============================================================================
+// Helper function declarations
+// ============================================================================
+
+static LLVM::LLVMFuncOp
+getOrCreateFuncDecl(RewriterBase &rewriter, ModuleOp mod, StringRef name,
+                    LLVM::LLVMFunctionType fnType) {
+  auto fn = mod.lookupSymbol<LLVM::LLVMFuncOp>(name);
+  if (fn)
+    return fn;
+  OpBuilder::InsertionGuard guard(rewriter);
+  rewriter.setInsertionPointToStart(mod.getBody());
+  fn = LLVM::LLVMFuncOp::create(
+      rewriter, UnknownLoc::get(rewriter.getContext()), name, fnType);
+  fn.setLinkage(LLVM::Linkage::External);
+  return fn;
+}
+
+// ============================================================================
+// WebGPU-specific load/store patterns
+// ============================================================================
+
+namespace {
+
+// Convert triton::LoadOp to standard LLVM loads
+struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp> {
+  using ConvertOpToLLVMPattern<triton::LoadOp>::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+
+    // Get the result type
+    auto resultTy = dyn_cast<RankedTensorType>(op.getType());
+    if (!resultTy) {
+      // Scalar load
+      Value ptr = adaptor.getPtr();
+      Value result = b.load(typeConverter->convertType(op.getType()), ptr);
+      if (op.getMask()) {
+        Value mask = adaptor.getMask();
+        Value other = adaptor.getOther();
+        if (!other)
+          other = b.undef(result.getType());
+        result = b.select(mask, result, other);
+      }
+      rewriter.replaceOp(op, result);
+      return success();
+    }
+
+    Type elemTy = typeConverter->convertType(resultTy.getElementType());
+    unsigned numElems = ttg::getTotalElemsPerThread(resultTy);
+
+    SmallVector<Value> ptrElems =
+        unpackLLElements(loc, adaptor.getPtr(), rewriter);
+    SmallVector<Value> maskElems;
+    SmallVector<Value> otherElems;
+
+    if (op.getMask()) {
+      maskElems = unpackLLElements(loc, adaptor.getMask(), rewriter);
+    }
+    if (op.getOther()) {
+      otherElems = unpackLLElements(loc, adaptor.getOther(), rewriter);
+    }
+
+    SmallVector<Value> resultElems;
+    for (unsigned i = 0; i < numElems; i++) {
+      Value ptr = ptrElems[i];
+      Value loaded = b.load(elemTy, ptr);
+
+      if (!maskElems.empty()) {
+        Value mask = maskElems[i];
+        Value other = otherElems.empty() ? b.undef(elemTy) : otherElems[i];
+        loaded = b.select(mask, loaded, other);
+      }
+      resultElems.push_back(loaded);
+    }
+
+    Value result = packLLElements(loc, getTypeConverter(), resultElems, rewriter,
+                                  resultTy);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+// Convert triton::StoreOp to standard LLVM stores
+struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp> {
+  using ConvertOpToLLVMPattern<triton::StoreOp>::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+
+    auto valueTy = dyn_cast<RankedTensorType>(op.getValue().getType());
+    if (!valueTy) {
+      // Scalar store
+      Value ptr = adaptor.getPtr();
+      Value val = adaptor.getValue();
+      b.store(val, ptr);
+      rewriter.eraseOp(op);
+      return success();
+    }
+
+    unsigned numElems = ttg::getTotalElemsPerThread(valueTy);
+
+    SmallVector<Value> ptrElems =
+        unpackLLElements(loc, adaptor.getPtr(), rewriter);
+    SmallVector<Value> valElems =
+        unpackLLElements(loc, adaptor.getValue(), rewriter);
+    SmallVector<Value> maskElems;
+
+    if (op.getMask()) {
+      maskElems = unpackLLElements(loc, adaptor.getMask(), rewriter);
+    }
+
+    for (unsigned i = 0; i < numElems; i++) {
+      Value ptr = ptrElems[i];
+      Value val = valElems[i];
+
+      if (!maskElems.empty()) {
+        // Predicated store: use LLVM conditional branch
+        // For simplicity, we use unconditional store + mask check
+        // This assumes the pointer is valid even when mask is false
+        // TODO: Use llvm.masked.store for proper masking
+        Value mask = maskElems[i];
+        Value zero = b.undef(val.getType());
+        // Only store when mask is true by selecting value
+        // Still stores but writes undef to avoid side effects
+        // A proper implementation would skip the store entirely
+        val = b.select(mask, val, zero);
+      }
+      b.store(val, ptr);
+    }
+
+    rewriter.eraseOp(op);
+    return success();
+  }
+};
+
+// Convert triton::AtomicRMWOp to LLVM atomicrmw
+struct AtomicRMWOpConversion
+    : public ConvertOpToLLVMPattern<triton::AtomicRMWOp> {
+  using ConvertOpToLLVMPattern<triton::AtomicRMWOp>::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+
+    // Map Triton's atomic RMW kind to LLVM's
+    auto mapAtomicOp = [](triton::RMWOp rmwOp) -> LLVM::AtomicBinOp {
+      switch (rmwOp) {
+      case triton::RMWOp::AND:
+        return LLVM::AtomicBinOp::_and;
+      case triton::RMWOp::OR:
+        return LLVM::AtomicBinOp::_or;
+      case triton::RMWOp::XOR:
+        return LLVM::AtomicBinOp::_xor;
+      case triton::RMWOp::ADD:
+        return LLVM::AtomicBinOp::add;
+      case triton::RMWOp::FADD:
+        return LLVM::AtomicBinOp::fadd;
+      case triton::RMWOp::MAX:
+        return LLVM::AtomicBinOp::max;
+      case triton::RMWOp::MIN:
+        return LLVM::AtomicBinOp::min;
+      case triton::RMWOp::UMAX:
+        return LLVM::AtomicBinOp::umax;
+      case triton::RMWOp::UMIN:
+        return LLVM::AtomicBinOp::umin;
+      case triton::RMWOp::XCHG:
+        return LLVM::AtomicBinOp::xchg;
+      }
+      llvm_unreachable("Unsupported atomic RMW op");
+    };
+
+    auto resultTy = dyn_cast<RankedTensorType>(op.getType());
+    if (!resultTy) {
+      // Scalar atomic
+      Value ptr = adaptor.getPtr();
+      Value val = adaptor.getVal();
+      auto result = LLVM::AtomicRMWOp::create(
+          rewriter, loc, mapAtomicOp(op.getAtomicRmwOp()), ptr, val,
+          LLVM::AtomicOrdering::monotonic);
+      rewriter.replaceOp(op, result.getResult());
+      return success();
+    }
+
+    unsigned numElems = ttg::getTotalElemsPerThread(resultTy);
+    SmallVector<Value> ptrElems =
+        unpackLLElements(loc, adaptor.getPtr(), rewriter);
+    SmallVector<Value> valElems =
+        unpackLLElements(loc, adaptor.getVal(), rewriter);
+    SmallVector<Value> maskElems;
+    if (op.getMask())
+      maskElems = unpackLLElements(loc, adaptor.getMask(), rewriter);
+
+    Type elemTy =
+        typeConverter->convertType(resultTy.getElementType());
+    SmallVector<Value> resultElems;
+
+    for (unsigned i = 0; i < numElems; i++) {
+      Value ptr = ptrElems[i];
+      Value val = valElems[i];
+      auto atomicOp = LLVM::AtomicRMWOp::create(
+          rewriter, loc, mapAtomicOp(op.getAtomicRmwOp()), ptr, val,
+          LLVM::AtomicOrdering::monotonic);
+      Value result = atomicOp.getResult();
+      if (!maskElems.empty()) {
+        result = b.select(maskElems[i], result, b.undef(elemTy));
+      }
+      resultElems.push_back(result);
+    }
+
+    Value result = packLLElements(loc, getTypeConverter(), resultElems, rewriter,
+                                  resultTy);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+// Convert triton::AtomicCASOp to LLVM cmpxchg
+struct AtomicCASOPConversion
+    : public ConvertOpToLLVMPattern<triton::AtomicCASOp> {
+  using ConvertOpToLLVMPattern<triton::AtomicCASOp>::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+
+    auto resultTy = dyn_cast<RankedTensorType>(op.getType());
+    if (!resultTy) {
+      Value ptr = adaptor.getPtr();
+      Value cmp = adaptor.getCmp();
+      Value val = adaptor.getVal();
+      auto cmpxchg = LLVM::AtomicCmpXchgOp::create(
+          rewriter, loc, ptr, cmp, val, LLVM::AtomicOrdering::monotonic,
+          LLVM::AtomicOrdering::monotonic);
+      // extract_val result type should be the element type (cmp's type), not
+      // the struct type {T, i1} returned by cmpxchg.
+      Value result = b.extract_val(cmp.getType(), cmpxchg, 0);
+      rewriter.replaceOp(op, result);
+      return success();
+    }
+
+    unsigned numElems = ttg::getTotalElemsPerThread(resultTy);
+    SmallVector<Value> ptrElems =
+        unpackLLElements(loc, adaptor.getPtr(), rewriter);
+    SmallVector<Value> cmpElems =
+        unpackLLElements(loc, adaptor.getCmp(), rewriter);
+    SmallVector<Value> valElems =
+        unpackLLElements(loc, adaptor.getVal(), rewriter);
+
+    Type elemTy =
+        typeConverter->convertType(resultTy.getElementType());
+    SmallVector<Value> resultElems;
+
+    for (unsigned i = 0; i < numElems; i++) {
+      auto cmpxchg = LLVM::AtomicCmpXchgOp::create(
+          rewriter, loc, ptrElems[i], cmpElems[i], valElems[i],
+          LLVM::AtomicOrdering::monotonic, LLVM::AtomicOrdering::monotonic);
+      resultElems.push_back(b.extract_val(elemTy, cmpxchg, 0));
+    }
+
+    Value result = packLLElements(loc, getTypeConverter(), resultElems, rewriter,
+                                  resultTy);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+// Convert triton::gpu::WarpIdOp to threadId / warpSize
+struct WarpIdOpConversion
+    : public ConvertOpToLLVMPattern<triton::gpu::WarpIdOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::gpu::WarpIdOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+
+    if (triton::gpu::lookupNumWarps(op) == 1) {
+      rewriter.replaceOp(op, b.i32_val(0));
+      return success();
+    }
+
+    // Get threadId.x as i32
+    Value tid = mgpu::ThreadIdOp::create(rewriter, loc,
+                                          mgpu::Dimension::x);
+    tid = arith::IndexCastOp::create(rewriter, loc, i32_ty, tid);
+
+    if (std::optional<int> startId =
+            getWarpGroupStartThreadId(rewriter.getInsertionBlock()))
+      tid = LLVM::SubOp::create(rewriter, loc, tid, b.i32_val(*startId));
+
+    int threadsPerWarp = triton::gpu::lookupThreadsPerWarp(rewriter);
+    Value warpId = b.udiv(tid, b.i32_val(threadsPerWarp));
+    rewriter.replaceOp(op, warpId);
+    return success();
+  }
+};
+
+// Convert triton::DotOp using FMA (no tensor cores)
+struct DotOpConversion : public ConvertOpToLLVMPattern<triton::DotOp> {
+  using ConvertOpToLLVMPattern<triton::DotOp>::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    return convertFMADot(op, adaptor, getTypeConverter(), rewriter);
+  }
+};
+
+// Convert triton::GetNumProgramsOp to gpu::GridDimOp
+// (backend-specific: not in the generic populateSPMDOpToLLVMPattern)
+struct GetNumProgramsOpConversion
+    : public ConvertOpToLLVMPattern<triton::GetNumProgramsOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Location loc = op.getLoc();
+    mgpu::Dimension dim;
+    switch (op.getAxis()) {
+    case ProgramIDDim::X:
+      dim = mgpu::Dimension::x;
+      break;
+    case ProgramIDDim::Y:
+      dim = mgpu::Dimension::y;
+      break;
+    case ProgramIDDim::Z:
+      dim = mgpu::Dimension::z;
+      break;
+    }
+    Value gridDim = mgpu::GridDimOp::create(rewriter, loc, dim);
+    Value result = arith::IndexCastOp::create(
+        rewriter, loc, rewriter.getI32Type(), gridDim);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+// Convert mgpu::BarrierOp to a SPIR-V-compatible barrier function call
+struct GPUBarrierOpConversion
+    : public ConvertOpToLLVMPattern<mgpu::BarrierOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(mgpu::BarrierOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto *ctx = rewriter.getContext();
+    auto moduleOp = op->getParentOfType<ModuleOp>();
+
+    // __spirv_ControlBarrier(execution_scope, memory_scope, memory_semantics)
+    auto i32Ty = IntegerType::get(ctx, 32);
+    auto voidTy = LLVM::LLVMVoidType::get(ctx);
+    auto fnTy = LLVM::LLVMFunctionType::get(voidTy, {i32Ty, i32Ty, i32Ty});
+    auto fn = getOrCreateFuncDecl(rewriter, moduleOp,
+                                   "__spirv_ControlBarrier", fnTy);
+
+    // Scope::Workgroup = 2
+    // MemorySemantics::AcquireRelease | MemorySemantics::WorkgroupMemory
+    // = 0x8 | 0x100 = 0x108
+    SmallVector<Value> barrierArgs = {b.i32_val(2), b.i32_val(2), b.i32_val(0x108)};
+    b.call(fn, barrierArgs);
+    rewriter.eraseOp(op);
+    return success();
+  }
+};
+
+// Convert mgpu::BlockIdOp to SPIR-V workgroup ID function call
+struct GPUBlockIdOpConversion
+    : public ConvertOpToLLVMPattern<mgpu::BlockIdOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(mgpu::BlockIdOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto *ctx = rewriter.getContext();
+    auto moduleOp = op->getParentOfType<ModuleOp>();
+
+    auto i32Ty = IntegerType::get(ctx, 32);
+    auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty});
+
+    auto fn = getOrCreateFuncDecl(rewriter, moduleOp,
+                                   "__spirv_BuiltInWorkgroupId", fnTy);
+
+    int dimIdx = 0;
+    switch (op.getDimension()) {
+    case mgpu::Dimension::x:
+      dimIdx = 0;
+      break;
+    case mgpu::Dimension::y:
+      dimIdx = 1;
+      break;
+    case mgpu::Dimension::z:
+      dimIdx = 2;
+      break;
+    }
+
+    SmallVector<Value> dimArgs = {b.i32_val(dimIdx)};
+    Value result = b.call(fn, dimArgs).getResult();
+    // BlockIdOp returns index type; convert to index
+    Value indexResult = arith::IndexCastOp::create(rewriter, loc,
+                                                    rewriter.getIndexType(),
+                                                    result);
+    rewriter.replaceOp(op, indexResult);
+    return success();
+  }
+};
+
+// Convert mgpu::ThreadIdOp to SPIR-V local invocation ID
+struct GPUThreadIdOpConversion
+    : public ConvertOpToLLVMPattern<mgpu::ThreadIdOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(mgpu::ThreadIdOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto *ctx = rewriter.getContext();
+    auto moduleOp = op->getParentOfType<ModuleOp>();
+
+    auto i32Ty = IntegerType::get(ctx, 32);
+    auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty});
+
+    auto fn = getOrCreateFuncDecl(rewriter, moduleOp,
+                                   "__spirv_BuiltInLocalInvocationId", fnTy);
+
+    int dimIdx = 0;
+    switch (op.getDimension()) {
+    case mgpu::Dimension::x:
+      dimIdx = 0;
+      break;
+    case mgpu::Dimension::y:
+      dimIdx = 1;
+      break;
+    case mgpu::Dimension::z:
+      dimIdx = 2;
+      break;
+    }
+
+    SmallVector<Value> dimArgs = {b.i32_val(dimIdx)};
+    Value result = b.call(fn, dimArgs).getResult();
+    Value indexResult = arith::IndexCastOp::create(rewriter, loc,
+                                                    rewriter.getIndexType(),
+                                                    result);
+    rewriter.replaceOp(op, indexResult);
+    return success();
+  }
+};
+
+// Convert mgpu::GridDimOp
+struct GPUGridDimOpConversion
+    : public ConvertOpToLLVMPattern<mgpu::GridDimOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(mgpu::GridDimOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto *ctx = rewriter.getContext();
+    auto moduleOp = op->getParentOfType<ModuleOp>();
+
+    auto i32Ty = IntegerType::get(ctx, 32);
+    auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty});
+
+    auto fn = getOrCreateFuncDecl(rewriter, moduleOp,
+                                   "__spirv_BuiltInNumWorkgroups", fnTy);
+
+    int dimIdx = 0;
+    switch (op.getDimension()) {
+    case mgpu::Dimension::x:
+      dimIdx = 0;
+      break;
+    case mgpu::Dimension::y:
+      dimIdx = 1;
+      break;
+    case mgpu::Dimension::z:
+      dimIdx = 2;
+      break;
+    }
+
+    SmallVector<Value> dimArgs = {b.i32_val(dimIdx)};
+    Value result = b.call(fn, dimArgs).getResult();
+    Value indexResult = arith::IndexCastOp::create(rewriter, loc,
+                                                    rewriter.getIndexType(),
+                                                    result);
+    rewriter.replaceOp(op, indexResult);
+    return success();
+  }
+};
+
+// Convert mgpu::BlockDimOp
+struct GPUBlockDimOpConversion
+    : public ConvertOpToLLVMPattern<mgpu::BlockDimOp> {
+  using ConvertOpToLLVMPattern::ConvertOpToLLVMPattern;
+
+  LogicalResult
+  matchAndRewrite(mgpu::BlockDimOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto *ctx = rewriter.getContext();
+    auto moduleOp = op->getParentOfType<ModuleOp>();
+
+    auto i32Ty = IntegerType::get(ctx, 32);
+    auto fnTy = LLVM::LLVMFunctionType::get(i32Ty, {i32Ty});
+
+    auto fn = getOrCreateFuncDecl(rewriter, moduleOp,
+                                   "__spirv_BuiltInWorkgroupSize", fnTy);
+
+    int dimIdx = 0;
+    switch (op.getDimension()) {
+    case mgpu::Dimension::x:
+      dimIdx = 0;
+      break;
+    case mgpu::Dimension::y:
+      dimIdx = 1;
+      break;
+    case mgpu::Dimension::z:
+      dimIdx = 2;
+      break;
+    }
+
+    SmallVector<Value> dimArgs = {b.i32_val(dimIdx)};
+    Value result = b.call(fn, dimArgs).getResult();
+    Value indexResult = arith::IndexCastOp::create(rewriter, loc,
+                                                    rewriter.getIndexType(),
+                                                    result);
+    rewriter.replaceOp(op, indexResult);
+    return success();
+  }
+};
+
+// ============================================================================
+// Conversion target for WebGPU: LLVM is legal, Triton/GPU is illegal
+// ============================================================================
+
+class TritonLLVMFunctionConversionTarget : public ConversionTarget {
+public:
+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx)
+      : ConversionTarget(ctx) {
+    addLegalDialect<LLVM::LLVMDialect>();
+    addLegalOp<UnrealizedConversionCastOp>();
+  }
+};
+
+class TritonLLVMConversionTarget : public ConversionTarget {
+public:
+  explicit TritonLLVMConversionTarget(MLIRContext &ctx)
+      : ConversionTarget(ctx) {
+    addLegalDialect<LLVM::LLVMDialect>();
+    addLegalDialect<cf::ControlFlowDialect>();
+    addIllegalDialect<triton::TritonDialect>();
+    addIllegalDialect<triton::gpu::TritonGPUDialect>();
+    addIllegalDialect<triton::nvidia_gpu::TritonNvidiaGPUDialect>();
+    addIllegalDialect<mgpu::GPUDialect>();
+    addLegalOp<UnrealizedConversionCastOp>();
+    // Warp specialization is lowered later.
+    addLegalOp<triton::gpu::WarpSpecializeOp>();
+    addLegalOp<triton::gpu::WarpYieldOp>();
+    addLegalOp<triton::gpu::WarpSpecializePartitionsOp>();
+    addLegalOp<triton::gpu::WarpReturnOp>();
+    addDynamicallyLegalOp<triton::gpu::GlobalScratchAllocOp>(
+        [](triton::gpu::GlobalScratchAllocOp op) {
+          return op.getBackend() != "default";
+        });
+  }
+};
+
+// ============================================================================
+// Main conversion pass
+// ============================================================================
+
+struct ConvertTritonWebGPUToLLVM
+    : public OperationPass<ModuleOp> {
+  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(ConvertTritonWebGPUToLLVM)
+
+  ConvertTritonWebGPUToLLVM()
+      : OperationPass<ModuleOp>(TypeID::get<ConvertTritonWebGPUToLLVM>()) {}
+
+  ConvertTritonWebGPUToLLVM(const ConvertTritonWebGPUToLLVM &other)
+      : OperationPass<ModuleOp>(other) {}
+
+  std::unique_ptr<Pass> clonePass() const override {
+    return std::make_unique<ConvertTritonWebGPUToLLVM>(*this);
+  }
+
+  StringRef getArgument() const override {
+    return "convert-triton-webgpu-to-llvm";
+  }
+  StringRef getDescription() const override {
+    return "Convert TritonGPU IR to LLVM IR for WebGPU/SPIR-V target";
+  }
+  StringRef getName() const override { return "ConvertTritonWebGPUToLLVM"; }
+
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry.insert<LLVM::LLVMDialect, mlir::arith::ArithDialect,
+                    mlir::math::MathDialect, mgpu::GPUDialect,
+                    mlir::scf::SCFDialect, triton::TritonDialect,
+                    ttg::TritonGPUDialect>();
+  }
+
+  void runOnOperation() override {
+    MLIRContext *context = &getContext();
+    ModuleOp mod = getOperation();
+    WebGPU::TargetInfo targetInfo;
+
+    // Allocate shared memory and set barrier
+    ModuleAllocation allocation(mod,
+                                triton::defaultAllocationAnalysisScratchSizeFn);
+    ModuleMembarAnalysis membarPass(&allocation);
+    membarPass.run();
+
+    mlir::LowerToLLVMOptions option(context);
+    option.overrideIndexBitwidth(32);
+    TritonGPUToLLVMTypeConverter typeConverter(context, option, targetInfo);
+
+    // Lower functions first
+    {
+      TritonLLVMFunctionConversionTarget funcTarget(*context);
+      RewritePatternSet funcPatterns(context);
+      mlir::triton::populateFuncOpConversionPattern(
+          typeConverter, funcPatterns, targetInfo, patternBenefitDefault);
+      if (failed(applyPartialConversion(mod, funcTarget,
+                                        std::move(funcPatterns))))
+        return signalPassFailure();
+    }
+
+    // Initialize shared memory
+    initSharedMemory(typeConverter);
+
+    ModuleAxisInfoAnalysis axisInfoAnalysis(mod);
+
+    RewritePatternSet patterns(context);
+    int benefit = patternBenefitPrioritizeOverLLVMConversions;
+    int webgpuBenefit = benefit + 1;
+
+    // --- WebGPU-specific patterns (higher priority) ---
+    patterns.add<LoadOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<StoreOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<AtomicRMWOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<AtomicCASOPConversion>(typeConverter, webgpuBenefit);
+    patterns.add<DotOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<GetNumProgramsOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<WarpIdOpConversion>(typeConverter, webgpuBenefit);
+
+    // GPU dialect conversions (barrier, block/thread IDs)
+    patterns.add<GPUBarrierOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<GPUBlockIdOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<GPUThreadIdOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<GPUGridDimOpConversion>(typeConverter, webgpuBenefit);
+    patterns.add<GPUBlockDimOpConversion>(typeConverter, webgpuBenefit);
+
+    // --- Generic patterns (lower priority) ---
+    mlir::triton::populateConvertLayoutOpToLLVMPatterns(
+        typeConverter, targetInfo, patterns, benefit);
+    mlir::triton::populateReduceOpToLLVMPatterns(typeConverter, patterns,
+                                                 targetInfo, benefit);
+    mlir::triton::populateScanOpToLLVMPatterns(typeConverter, patterns,
+                                               targetInfo, benefit);
+    mlir::triton::populateGatherOpToLLVMPatterns(typeConverter, patterns,
+                                                 targetInfo, benefit);
+    mlir::triton::populateHistogramOpToLLVMPatterns(typeConverter, patterns,
+                                                    targetInfo, benefit);
+    mlir::triton::populateMemoryOpToLLVMPatterns(typeConverter, targetInfo,
+                                                 patterns, benefit);
+    mlir::triton::populateElementwiseOpToLLVMPatterns(
+        typeConverter, patterns, axisInfoAnalysis, targetInfo, benefit);
+
+    // Float arith ops are NOT in the common populateElementwiseOpToLLVMPatterns;
+    // each backend must register them via ElementwiseOpConversion so that
+    // struct-packed tensor operands are unpacked to scalars before creating
+    // the LLVM op. Without these, the stock ArithToLLVM patterns match and
+    // produce illegal llvm.fadd/fmul/etc. with struct-typed operands.
+    {
+      using namespace mlir::triton::gpu;
+#define POPULATE_FLOAT_OP(SRC_OP, DST_OP)                                      \
+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(                       \
+      typeConverter, axisInfoAnalysis, benefit)
+      POPULATE_FLOAT_OP(arith::AddFOp, LLVM::FAddOp);
+      POPULATE_FLOAT_OP(arith::SubFOp, LLVM::FSubOp);
+      POPULATE_FLOAT_OP(arith::MulFOp, LLVM::FMulOp);
+      POPULATE_FLOAT_OP(arith::DivFOp, LLVM::FDivOp);
+      POPULATE_FLOAT_OP(arith::ExtFOp, LLVM::FPExtOp);
+      POPULATE_FLOAT_OP(arith::TruncFOp, LLVM::FPTruncOp);
+      POPULATE_FLOAT_OP(arith::FPToSIOp, LLVM::FPToSIOp);
+      POPULATE_FLOAT_OP(arith::SIToFPOp, LLVM::SIToFPOp);
+#undef POPULATE_FLOAT_OP
+    }
+
+    mlir::triton::populateMinMaxFOpToLLVMPattern(typeConverter, patterns,
+                                                 axisInfoAnalysis, false,
+                                                 benefit);
+    mlir::triton::populateClampFOpToLLVMPattern(
+        typeConverter, patterns, axisInfoAnalysis, targetInfo,
+        patternBenefitClampOptimizedPattern);
+    mlir::triton::populateMakeRangeOpToLLVMPattern(typeConverter, targetInfo,
+                                                   patterns, benefit);
+    mlir::triton::populateViewOpToLLVMPatterns(typeConverter, patterns,
+                                               benefit);
+    mlir::triton::populateAssertOpToLLVMPattern(typeConverter, patterns,
+                                                targetInfo, benefit);
+    mlir::triton::populateControlFlowOpToLLVMPattern(typeConverter, patterns,
+                                                     targetInfo, benefit);
+    mlir::triton::populateSPMDOpToLLVMPattern(typeConverter, patterns,
+                                              targetInfo, benefit);
+    mlir::triton::populatePrintOpToLLVMPattern(typeConverter, patterns,
+                                               targetInfo, benefit);
+    mlir::triton::populateInstrumentationToLLVMPatterns(typeConverter,
+                                                        patterns);
+
+    // Standard MLIR conversions
+    mlir::arith::populateCeilFloorDivExpandOpsPatterns(patterns);
+    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);
+    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);
+    mlir::ub::populateUBToLLVMConversionPatterns(typeConverter, patterns);
+
+    // Apply conversion
+    TritonLLVMConversionTarget convTarget(*context);
+    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))
+      return signalPassFailure();
+
+    // Lower CF ops separately
+    {
+      TritonLLVMFunctionConversionTarget cfTarget(*context);
+      cfTarget.markUnknownOpDynamicallyLegal([&](Operation *op) {
+        return op->getDialect() !=
+               context->getLoadedDialect<cf::ControlFlowDialect>();
+      });
+      RewritePatternSet cfPatterns(context);
+      mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,
+                                                            cfPatterns);
+      if (failed(applyPartialConversion(mod, cfTarget, std::move(cfPatterns))))
+        return signalPassFailure();
+    }
+
+    fixUpLoopAnnotation(mod);
+    makeAllWarpGroupsIsolatedFromAbove(mod);
+  }
+
+private:
+  void initSharedMemory(LLVMTypeConverter &typeConverter) {
+    ModuleOp mod = getOperation();
+    OpBuilder b(mod.getBodyRegion());
+    auto loc = mod.getLoc();
+    auto elemTy = typeConverter.convertType(b.getIntegerType(8));
+    // Dynamic shared allocation: array size 0 with external linkage
+    auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);
+    LLVM::GlobalOp::create(
+        b, loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,
+        "global_smem", /*value=*/Attribute(), /*alignment=*/16,
+        // Address space 3 = SPIR-V Workgroup memory
+        static_cast<unsigned>(3));
+  }
+};
+
+} // anonymous namespace
+
+// ============================================================================
+// Public API
+// ============================================================================
+
+namespace mlir::triton::WebGPU {
+
+std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonWebGPUToLLVMPass() {
+  return std::make_unique<ConvertTritonWebGPUToLLVM>();
+}
+
+} // namespace mlir::triton::WebGPU
diff --git a/third_party/webgpu/triton_webgpu.cc b/third_party/webgpu/triton_webgpu.cc
new file mode 100644
index 000000000..dc2e745b8
--- /dev/null
+++ b/third_party/webgpu/triton_webgpu.cc
@@ -0,0 +1,158 @@
+// triton_webgpu.cc -- Pybind11 module for WebGPU backend
+//
+// Provides:
+//   webgpu.load_dialects(ctx)       -- no-op for now (no custom dialects)
+//   webgpu.set_spv_target_triple(mod) -- set SPIR-V target triple on LLVM module
+//   webgpu.translate_llvmir_to_spirv(llvmIR) -> (bytes, name)
+//   webgpu.passes.ttgpuir.add_to_llvmir(pm) -- TritonGPU→LLVM conversion pass
+//
+// The SPIR-V translation reuses the same approach as Intel's XPU backend:
+//   1. Parse LLVM IR string
+//   2. Set spir64 target triple + data layout
+//   3. Use LLVM's SPIR-V serialization (via MLIR libs)
+
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IRReader/IRReader.h"
+#include "llvm/Support/SourceMgr.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+
+#include <pybind11/pybind11.h>
+#include <pybind11/stl.h>
+
+#include <set>
+#include <string>
+
+namespace py = pybind11;
+using ret = py::return_value_policy;
+
+// Forward declaration for the WebGPU TritonGPU→LLVM conversion pass
+namespace mlir::triton::WebGPU {
+std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
+createConvertTritonWebGPUToLLVMPass();
+} // namespace mlir::triton::WebGPU
+
+// Find kernel functions in the LLVM module.
+// For SPIR-V, kernels use CallingConv::SPIR_KERNEL.
+// For standard LLVM IR (from Triton), they are just regular functions with
+// external linkage whose names don't start with '_' or 'llvm.'.
+static std::string findKernelName(llvm::Module &M) {
+  // First, try SPIR_KERNEL calling convention
+  for (auto &F : M.functions()) {
+    if (F.getCallingConv() == llvm::CallingConv::SPIR_KERNEL) {
+      return F.getName().str();
+    }
+  }
+  // Fallback: find the first non-intrinsic, non-declaration function
+  for (auto &F : M.functions()) {
+    if (!F.isDeclaration() && !F.isIntrinsic() &&
+        F.getLinkage() == llvm::GlobalValue::ExternalLinkage) {
+      return F.getName().str();
+    }
+  }
+  return "";
+}
+
+// Set the SPIR-V target triple and data layout on an LLVM module.
+static void setSpvTargetTriple(llvm::Module *mod) {
+  std::string triple = "spir64-unknown-unknown";
+  std::string layout =
+      "e-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:"
+      "256-v256:256-v512:512-v1024:1024-n8:16:32:64";
+  mod->setTargetTriple(llvm::Triple(triple));
+  mod->setDataLayout(layout);
+}
+
+// Translate LLVM IR to SPIR-V binary.
+// This produces a SPIR-V module from the LLVM IR by:
+//   1. Parsing the LLVM IR
+//   2. Setting SPIR-V target triple
+//   3. Converting calling conventions to SPIR_KERNEL
+//   4. Serializing to SPIR-V bitcode
+//
+// NOTE: Full SPIR-V translation requires either:
+//   a) LLVM's experimental SPIR-V backend (not in bundled LLVM), or
+//   b) The llvm-spirv translator, or
+//   c) MLIR's GPU->SPIR-V pipeline
+// For now, we output LLVM bitcode with SPIR-V triple as an intermediate
+// format. The actual SPIR-V translation will be handled by an external
+// tool (e.g., llvm-spirv) or by Dawn's Tint shader compiler.
+static std::tuple<py::object, std::string>
+translateToSpirvBitcode(const std::string &llvmIR) {
+  std::string name;
+  std::string spirvBitcode;
+  {
+    py::gil_scoped_release allow_threads;
+
+    llvm::LLVMContext context;
+    std::unique_ptr<llvm::MemoryBuffer> buffer =
+        llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());
+    llvm::SMDiagnostic error;
+    std::unique_ptr<llvm::Module> module =
+        llvm::parseIR(buffer->getMemBufferRef(), error, context);
+
+    if (!module) {
+      llvm::report_fatal_error("failed to parse IR: " + error.getMessage() +
+                               " lineno: " +
+                               std::to_string(error.getLineNo()));
+    }
+
+    // Set SPIR-V target
+    setSpvTargetTriple(module.get());
+
+    // Convert kernel calling conventions to SPIR_KERNEL
+    for (auto &F : module->functions()) {
+      if (!F.isDeclaration() && !F.isIntrinsic() &&
+          F.getLinkage() == llvm::GlobalValue::ExternalLinkage) {
+        F.setCallingConv(llvm::CallingConv::SPIR_KERNEL);
+      }
+    }
+
+    // Find kernel name
+    name = findKernelName(*module);
+
+    // Serialize module to LLVM bitcode (with SPIR-V triple)
+    llvm::SmallVector<char, 0> buffer_vec;
+    llvm::raw_svector_ostream os(buffer_vec);
+    llvm::WriteBitcodeToFile(*module, os);
+    spirvBitcode.assign(buffer_vec.begin(), buffer_vec.end());
+  }
+  return std::make_tuple(py::bytes(spirvBitcode), name);
+}
+
+void init_triton_webgpu(py::module &&m) {
+  // Load dialects (no custom WebGPU dialects for now)
+  m.def("load_dialects", [](mlir::MLIRContext &context) {
+    // WebGPU backend doesn't define custom MLIR dialects yet.
+    // It reuses the standard Triton/TritonGPU dialects.
+    context.loadAllAvailableDialects();
+  });
+
+  // Set SPIR-V target triple on an LLVM module
+  m.def("set_spv_target_triple", [](llvm::Module *mod) {
+    setSpvTargetTriple(mod);
+  });
+
+  // Translate LLVM IR to SPIR-V bitcode
+  m.def(
+      "translate_to_spirv",
+      [](const std::string &llvmIR)
+          -> std::tuple<py::object, std::string> {
+        return translateToSpirvBitcode(llvmIR);
+      },
+      ret::take_ownership);
+
+  // Submodule for passes
+  auto passes = m.def_submodule("passes");
+  auto ttgpuir = passes.def_submodule("ttgpuir");
+
+  // Register the TritonGPU → LLVM conversion pass for WebGPU/SPIR-V
+  ttgpuir.def("add_to_llvmir", [](mlir::PassManager &pm) {
+    pm.addPass(mlir::triton::WebGPU::createConvertTritonWebGPUToLLVMPass());
+  });
+}
